\input{../../common/livre_begin.tex}%
\firstpassagedo{\input{reco_titre.tex}}
\input{../../common/livre_table_begin.tex}%

\sloppy

%----------------------------------------------------------------------------------------------------------------
\chapter{Reconnaissance statistique}
%----------------------------------------------------------------------------------------------------------------

\indexfrr{reconnaissance}{statistique}
\indexfr{reconnaissance}
\indexfr{graphème}
\indexfrr{séquence}{graphème}

Les traitements d'images aboutissent à une séquence de graphèmes équivalents à des lettres ou des morceaux de lettres. La reconnaissance statistique doit déterminer le mot le plus probable correspondant à cette séquence. Les modèles abordés dans cette partie seront cantonnés aux HMM et IOHMM d'après les conclusions de la première partie (paragraphe~\ref{biblio_conclusion}). Les chaînes de Markov cachées permettent de modéliser simplement une séquence discrète, ces modèles sont associés ici à un classifieur continu plus apte à dissocier les différentes formes des graphèmes. Ces modèles hybrides ont été passés en revue dans le paragraphe~\ref{biblio_mmc_classifieur}. 



%----------------------------------------------------------------------------------------------------------------
\section{Préambule}
%----------------------------------------------------------------------------------------------------------------




\subsection{De l'image à la reconnaissance}

Ce chapitre dédié à la reconnaissance s'appuie sur les résultats obtenus par le chapitre~\ref{image_chapitre_label} dédié aux traitements d'images qui détaille la segmentation de l'image d'un mot en une séquence de graphèmes, ce premier résultat est illustré par la figure~\ref{image_graphem_resultat}, page~\pageref{image_graphem_resultat}. Chaque graphème est censé représenter une lettre ou un morceau de lettre de sorte que la segmentation en graphèmes est une sous-segmentation de la segmentation en lettres. Ce résultat tant espéré n'est néanmoins pas souvent vérifié comme l'atteste la figure~\ref{image_grapheme_erreur}, page~\pageref{image_grapheme_erreur}. Certains couples de lettres récalcitrants apparaissent régulièrement liés comme les doubles "tt", ou les couples faisant intervenir la lettre "o" comme "\oe" ou "oi". Cette mauvaise segmentation intervient dans environ un cas sur dix, ceci implique qu'un mot de plus de dix lettres a toutes les chances de contenir une erreur de segmentation. Bien que non négligeables, ces erreurs ne seront prises en compte qu'au paragraphe~\ref{hmm_bi_lettre}.


\indexfr{particules aériennes}

Cette segmentation en graphèmes est en fait composée de deux séquences, la première regroupe effectivement des lettres ou des morceaux de lettres, la seconde, de longueur différente ou égale, regroupe des petites images comme les accents ou les points appelés ici les particules aériennes. Ce paragraphe a pour but de présenter ce qui constitue les données sur lesquelles s'appuient les méthodes développées dans ce chapitre et d'exposer succintement l'enchaînement de celles-ci jusqu'au résultat.

\indexfrr{graphème}{séquence}
\indexfrr{liaison}{séquence}
\indexfrr{séquence}{entrée}
\indexfrr{séquence}{graphème}
\indexfrr{séquence}{liaison}
\indexfrr{séquence}{sortie}

La modélisation choisie fait appel aux modèles IOHMM évoqués au paragraphe~\ref{biblio_iohmm_par_ref}, page~\pageref{biblio_iohmm_par_ref} qui prend en entrée deux séquences d'observations. La première appelée séquence d'entrées, sera extraite des liaisons entre graphèmes, de leurs positions relatives. Cette séquence contient autant d'observations qu'il y a de graphèmes et décrit les relations spatiales qui les unissent. La seconde séquence appelée séquence de sortie est extraite directement de la forme des graphèmes ainsi que de la présence des accents. Les particules aériennes sont en fait considérées comme des prolongements incertains des graphèmes. Chaque graphème est décrit indépendamment des autres et à cette description s'ajoute une moyenne des particules aériennes avoisinantes. La première étape est donc de construire ces deux séquences de liaisons $\vecteur{L_1}{L_T}$ et de graphèmes $\vecteur{O_1}{O_T}$ illustrées par la figure~\ref{reco_deux_sequence_liaison_grapheme} et développées dans les paragraphes~\ref{reco_description_grapheme} et~\ref{reco_selection_caracteristique}. Ces paragraphes étudient différentes descriptions possibles et leur sélection.


		\begin{figure}[ht]
    $$\begin{tabular}{|c|} \hline
    \filefig{../reco/fig_sequence}
    \\ \hline \end{tabular}$$
    \caption{	Illustration des séquences de liaisons $\vecteur{L_1}{L_T}$ et d'observations $\vecteur{O_1}{O_T}$
    					à partir de la séquence de graphèmes $\vecteur{G_1}{G_T}$ et la séquence d'accents ici réduite
    					au seul accent $A_1$. Les flèches simples indiquent l'enchaînement des éléments à l'intérieur 
    					d'une séquence,
    					les flèches doubles indiquent qu'un élément intègre le calcul d'un autre. Les accents peuvent être
    					ainsi reliés par une double flèche aux graphèmes ou directement aux observations.}
    \label{reco_deux_sequence_liaison_grapheme}
		\end{figure}

\indexfr{IOHMM}

Une fois ces deux séquences construites, il suffit de leur appliquer les modèles IOHMM afin de procéder à la reconnaissance proprement dite, ce qui sera l'objet des paragraphes~\ref{reco_modele_presentation_1} à~\ref{reco_selection_architecture}. Ceux-ci s'intéressent aux propriétés des modèles ainsi qu'à leur élaboration. Mais tout d'abord, le paragraphe~\ref{reco_description_grapheme} étudiera le passage des graphèmes aux caractéristiques.









\subsection{Base d'apprentissage et base de test}
\indexfrr{base}{apprentissage}
\indexfrr{base}{test}

Le meilleur moyen de jauger les modèles appris par la suite est de vérifier si l'erreur obtenue sur une base ayant servi à l'apprentissage (ou \emph{base d'apprentissage}) est conservée sur une autre base (ou \emph{base de test}) que le modèle découvre pour la première fois.

Soit $B=\accolade{\pa{X_i,Y_i} \sachant 1 \infegal i \infegal N}$ l'ensemble des observations disponibles. Cet ensemble est aléatoirement scindé en deux sous-ensembles $B_a$ et $B_t$ de telle sorte que :

    $$
    \begin{array}{l}
    B_a \neq \emptyset \text{ et } B_t \neq \emptyset \\
    B_a \cup B_t = B \text{ et } B_a \cap B_t = \emptyset \\
    \frac{\card{B_a}}{\card{B_t}} = p \in \crochetopen{0,1} 
    	\text {, en règle générale, } p \in \crochet{\frac{1}{2},\frac{3}{4}}
    \end{array}
    $$

Ce découpage est valide si tous les exemples de la base $B$ obéissent à la même loi, les deux bases $B_a$ et $B_t$ sont alors dites \emph{homogènes}. \indexfrr{base}{homogène} Les modèles seront donc appris sur la base d'apprentissage $B_a$ et "testés" sur la base de test $B_t$. Le test consiste à vérifier que l'erreur sur $B_t$ est sensiblement égale à celle sur $B_a$, auquel cas on dit que le modèle généralise bien. \indexfr{généralisation} Le modèle trouvé n'est pour autant le bon modèle mais il est robuste~: les résultats obtenus sur une base ne servant pas à l'estimation des coefficients des modèles sont similaires à ceux obtenus sur une base ayant servi à cette estimation. La courbe figure~\ref{figure_modele_optimal} illustre une définition du modèle optimal comme étant celui qui minimise l'erreur sur la base de test qui ne contient que des "nouvelles" images. 

		\begin{figure}[ht]
    $$\frame{$\begin{array}[c|c]{c}\includegraphics[height=4cm, width=6cm]
    {\filext{../dessin2/errapptest}}\end{array}$}$$
    \caption{	Modèle optimal pour la base de test~: les modèles robustes sont situés dans la zone pour
    					laquelle erreur d'apprentissage et erreur de test sont du même ordre de grandeur.}
    \label{figure_modele_optimal}
		\end{figure}

De plus, ce découpage permet de définir une zone valide pour la sélection de modèles située en deça du mimimum obtenu pour la base de test. En dehors de cette zone, les performances obtenues ne peuvent être considérées comme fiables puisque les résultats obtenus sur une nouvelle base d'exemples pourtant de même loi que la base d'apprentissage sont largement supérieurs à ceux obtenus sur une base ayant servi à l'estimation des coefficients des modèles.









%---------------------------------------------------------------------------------------------------------------
\section{Description des graphèmes}
%---------------------------------------------------------------------------------------------------------------
\label{reco_description_grapheme}


\indexfr{graphème}
\indexfr{caractéristiques}
\indexfrr{séquence}{caractéristiques}
\indexfrr{séquence}{observations}

Ce paragraphe reprend plus en détail les caractéristiques présentées au paragraphe~\ref{biblio_caracteristique}. Celles-ci ont pour objectif de traduire une image de lettre ou morceau de lettre dont les dimensions peuvent être variables en un vecteur de dimension fixe et indépendant de la taille du graphème. Ainsi, une séquence de graphèmes sera convertie en une séquence d'observations ou vecteurs de taille fixe. Plusieurs angles d'approche sont présentés, comme des projections des graphèmes sur les axes des abscisses et des ordonnées, une représentation matricielle de l'image, le calcul de moments, diverses caractéristiques extraites du contour... dont les performances seront comparées au paragraphe~\ref{reco_selection_caracteristique}. 





\subsection{Matrice}
\label{reco_graphem_matrice}
\indexfrr{graphème}{mise à l'échelle}

Cette description consiste à décrire des images de tailles différentes par une matrice de réels de dimension fixe. La figure~\ref{reco_feature_set_1} illustre ce principe pour une lettre "a" et un découpage en huit lignes et six colonnes. Pour un découpage quelconque en $l$ lignes et $c$ colonnes, afin d'éviter les erreurs d'arrondi, les dimensions $\pa{X,Y}$ du graphème sont multipliées par $l$ en hauteur et $c$ en largeur. Les nouvelles dimensions du graphème sont $\pa{Xc,Yl}$. Si on note $N\pa{x_1,y_1,x_2,y_2}$ le nombre de pixels noirs contenus dans l'ensemble de pixels $\acc{ \pa{x,y} \sac x \in \cro{x_1,x_2} \text{ et } y \in \cro{y_1,y_2} }$ de l'image agrandie, alors la matrice $M = \pa{ m_{ij} } _ { \begin{subarray}{c} 1 \infegal i \infegal l \\ 1 \infegal j \infegal c \end{subarray} } $ des caractéristiques est définie par~:

		\begin{eqnarray}
		\forall \pa{i,j}, \, m_{ij} &=&  \dfrac{ N \pa{ X\pa{i-1} + 1, \, Y\pa{j-1} + 1, \, iX, \, jY} }
																					 { 
																					 	 \underset{i,j}{\max} \; 
																					 	 N \pa{ X\pa{i-1} + 1, \, Y\pa{j-1} + 1, \, iX, \, jY}
																					 }
		\end{eqnarray}



				\begin{figure}[ht]
				\[
				\begin{tabular}{|c|cc|}\hline
				$\begin{array}{c}
    		\includegraphics[height=1.5cm, width=3cm]{\filext{../reco/image/graph_gerard}}
    		\end{array}$
				&
				$\begin{array}{c}
    		\includegraphics[height=4cm, width=3cm]{\filext{../reco/image/graph_a}}
    		\end{array}$
    		&
				\begin{tabular}{|c|c|c|c|c|c|} \hline
				 -    &  	37\%  &  	35\%  &  	 -    &  	 -    &  	 -    \\ \hline
				20\%  &  	88\%  &  	90\%  &  	69\%  &  	1\%   &  	 -    \\ \hline  
				64\%  &  	32\%  &  	12\%  &  	100\% &  	32\%  &  	 -    \\ \hline  
				77\%  &  	7\%   &  	1\%   &  	83\%  &  	81\%  &  	1\%   \\ \hline  
				50\%  &  	78\%  &  	5\%   &  	85\%  &  	78\%  &  	41\%  \\ \hline  
				35\%  &  	99\%  &  	96\%  &  	55\%  &  	42\%  &  	68\%  \\ \hline  
				70\%  &  	52\%  &  	79\%  &  	20\%  &  	35\%  &  	39\%  \\ \hline  
				6\%   &  	 -    &  	12\%  &  	 -    &  	 -    &  	 -    \\ \hline  
				\end{tabular}
				\\ \hline
				\end{tabular}
				\]
	    	\caption{	Représentation d'un graphème sous forme de matrice 8x6~: cette matrice divise l'image en carrés et
	    						chacune de ces cases contient le nombre de pixels noirs inclus dans ce carré 
	    						rapporté au maximum trouvé
	    						sur l'ensemble des cases.
	    						La première image est celle du mot dont est tirée la lettre "a".}
	    	\label{reco_feature_set_1}
	    	\end{figure}


\begin{xremark}{minuscule ou majuscule}
Si cette matrice suffit à décrire la forme, elle ne permet pas de distinguer un "o" minuscule d'un "O" majuscule. Comme toutes les caractéristiques décrites dans ce chapitre, cette distinction n'est possible que si la lettre n'apparaît pas seule.
\end{xremark}


		
		
\indexfrr{matrice}{caractéristiques}
\indexfrr{caractéristiques}{matrice}

Il reste à déterminer les dimensions les plus appropriées de la matrice de caractéristiques. Elle ne doit pas être trop petite afin d'être suffisamment discriminante ni trop grande pour éviter une trop grande complexité des modèles de reconnaissance. Ce choix sera débattu au paragraphe~\ref{reco_selection_caracteristique}. Bien que cette représentation soit indépendante des problèmes d'échelle, elle est sensible à l'épaisseur des traits ou différentes orientations de caractères (écriture penchée).

\indexfr{inclinaison}
\indexfr{épaisseur du tracé}
\indexfr{squelette}

On peut supposer que les problèmes d'écriture penchée sont en partie résolus lors d'une étape de correction de l'inclinaison comme celle décrite au paragraphe~\ref{image_redressement_sobel}. L'épaisseur des traits reste tout de même un problème comme l'illustre la figure~\ref{reco_lettre_a_epaisseur} qui pourrait être en partie résolu par l'application de cette matrice sur le squelette des graphèmes ou un squelette d'épaisseur fixée à quelques pixels. Les prétraitements d'images permettent souvent de réduire la variabilité des caractéristiques et ainsi réduire la taille des bases d'apprentissage pour des performances équivalentes.


				\begin{figure}[ht]
				\[
				\begin{tabular}{|c|c|}\hline
    		\includegraphics[height=4cm, width=2cm]{\filext{../reco/image/lettrea1}} &
    		\includegraphics[height=4cm, width=2cm]{\filext{../reco/image/lettrea2}} \\ \hline
    		\end{tabular}
    		\]
    		\caption{Deux lettres "A" dont les épaisseurs sont différentes et les inclinaisons différentes.}
    		\label{reco_lettre_a_epaisseur}
    		\end{figure}

    		
\indexfrr{caractéristiques}{topologique}

L'article \citeindex{Oliveira2002} effectue un test entre différents types de caractéristiques dont celles-ci appelées caractéristiques "topologiques" et également celles présentées au paragraphe suivant~\ref{reco_graphem_histo} qui se révèlent meilleures dans le cadre de l'article. 











\subsection{Profils ou histogrammes}
\label{reco_graphem_histo}
\indexfrr{graphème}{profil}
\indexfrr{graphème}{projection}
\indexfr{projection}
\indexfr{transition}
\indexfrr{graphème}{transition}


Les profils d'un graphème sont proches d'une projection de celui-ci horizontalement ou verticalement. Sur chaque ligne ou chaque colonne, on peut compter le nombre de pixels noirs qui s'y trouvent ou le nombre de transitions d'un pixel blanc vers un pixel noir équivalent au nombre de traits coupant la ligne. Toutefois, là encore, il s'agit de décrire un graphème selon un vecteur de taille fixe. C'est pourquoi l'image est découpée en bandes horizontales et verticales et pour chacune d'elles, sont calculées la moyenne du nombre de pixels noirs par ligne (ou colonne) et la moyenne du nombre de transitions par ligne (ou colonne). Un exemple de ce type de description est donné par la table~\ref{reco_caract_profil}.



			\begin{table}[ht]
			\[
			\begin{tabular}{|c|l|}\hline
			caractéristiques & signification \\ \hline
			1..5  	& épaisseur moyenne sur 5 bandes horizontales	/ $5 \, H$ \\ \hline
			6..10   & épaisseur moyenne sur 5 bandes verticales / $5 \, L$ \\ \hline
			11..15  & nombre moyen de transitions sur 5 bandes horizontales / $2$ \\ \hline
			16..20  & nombre moyen de transitions sur 5 bandes verticales / $2$ \\ \hline
			\end{tabular}
			\]
			\caption{ Représentation d'un graphème à l'aide de profils pour une image découpée en cinq bandes
								horizontales et cinq bandes verticales. $H$ et $L$ représentent respectivement la hauteur
								et la largeur de l'image.}
			\label{reco_caract_profil}
			\end{table}
			

A cet ensemble de caractéristiques, peuvent également être inclus des découpages selon d'autres directions comme les deux diagonales. De plus, il reste encore à choisir la finesse du découpage, c'est-à-dire le nombre de bandes horizontales et verticales le mieux adapté à la reconnaissance. Les questions sont identiques à celles soulevées lors du précédent jeu de caractéristiques (\ref{reco_graphem_matrice}) et seront débattues au paragraphe~\ref{reco_selection_caracteristique}.

\indexfrr{graphème}{boucle}

D'autres caractéristiques sont souvent ajoutées à ces profils comme la probabilité que le graphème contienne une boucle, sa surface, sa position. Elles sont succintement décrites dans~\citeindex{Waard1995}. La figure~\ref{reco_image_profil_graphem} illustre une partie de ces caractéristiques.


				\begin{figure}[ht]
				\[\begin{tabular}{|c|c|c|}\hline
    		\includegraphics[height=4cm, width=4cm]{\filext{../reco/image/profa1}}  &
    		\includegraphics[height=4cm, width=4cm]{\filext{../reco/image/profa2}}  &
    		\includegraphics[height=4cm, width=4cm]{\filext{../reco/image/profi3}} 
    		\\ \hline \end{tabular}\]
    		\caption{	Illustration des projections horizontales et verticales du nombre de pixels noirs regroupées
    							sur cinq bandes dont la description est donnée par la table~\ref{reco_caract_profil}.
    							Les deux premiers profils sont sensiblement égaux tandis que le dernier provenant d'une lettre
    							différente ne possède qu'un seul grand pic.}
    		\label{reco_image_profil_graphem}
    		\end{figure}



\begin{xremark}{pondération des dimensions}
Contrairement au jeu de caractéristiques présenté au paragraphe~\ref{reco_graphem_matrice}, les informations contenues dans chacune des dimensions de ce vecteur sont hétérogènes. Toutes n'ont pas la même signification et par conséquent, elles évoluent à des échelles différentes. Par exemple, le nombre de transitions est souvent inclus dans l'intervalle $\cro{0,2}$ alors que l'épaisseur moyenne rapportée à l'image évolue plus souvent dans l'intervalle $\cro{0,\frac{1}{2}}$. Il est nécessaire de normaliser chaque dimension afin de les rendre homogènes. Les coefficients renormalisateurs peuvent être également appris\seeannex{classification_graphem_carac_dist}{apprentissage d'une distance}.
\label{reco_remarque_ponderation}\indexfr{hétérogénéité}\indexfr{homogénéité}
\end{xremark}









\subsection{Description d'un graphème à partir d'une carte de Kohonen}
\label{reco_point_caracteristique_kohonen}
\indexfrr{caractéristiques}{Kohonen}
\indexfr{Kohonen}
\indexfr{quadrillage}


L'idée est toujours ici de décrire les graphèmes grâce à un vecteur de dimension fixe en plaçant un nombre fixe de points caractéristiques éparpillés dans l'image du graphème. Les cartes de Kohonen permettant de placer ces points (voir \citeindex{Kohonen1997}, \citeindex{Datta1997}) selon une topologie fixée (linéaire ou quadrillage). La topologie utilisée ici sera un quadrillage contenant quelques dizaines de points. Chaque pixel du graphème attire vers lui un point du quadrillage de Kohonen qui répercute cette attirance sur ses voisins. Plus en détail, on considère un ensemble de points $\pa{ P_{ij}}_{ \begin{subarray} \, 1 \infegal i \infegal I \\ 1 \infegal j \infegal J \end{subarray} }$ définissant un treillis de Kohonen uniformément réparti sur une image de caractère comme le montre la figure~\ref{reco_kohonen_e}$a$. Les dimensions de l'image sont notées $\pa{X,Y}$ et la suite $\pa{P_{ij}}$ est initialisée comme suit~:

				\begin{eqnarray}
				\forall \pa{i,j}, \; P_{ij} = \pa{ \frac{iX}{I}, \frac{jY}{J} }'
				\end{eqnarray}


				\begin{figure}[ht]
				\[
				\begin{tabular}{|c|c|}\hline
    		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_e1}} &
    		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_e2}} \\ 
    		$a$ & $b$ \\ \hline
    		\end{tabular}
    		\]
    		\caption{	Lettre "E" sur laquelle est superposé un treillis de Kohonen (figure~$a$). 
    							L'objectif est d'obtenir la convergence des points du treillis vers la configuration
    							de la figure~$b$ dont on espère qu'elle est propre à la lettre "E".
    					  }
    		\label{reco_kohonen_e}
    		\end{figure}

Les voisins du point $P_{ij}$ sont l'ensemble~:

				$$
				V_{ij} = 	\acc{P_{i-1,j}, \; P_{i,j-1}, \; P_{i+1,j}, \; P_{i,j+1} } 
								  \bigcap
								  \acc{ P_{ij} \sac 1 \infegal i \infegal I \text{ et } 1 \infegal j \infegal J }
				$$
									
L'algorithme assurant l'évolution du treillis est le suivant~:


		\begin{xalgorithm}{caractéristiques de Kohonen}
		\indexfrr{caractéristiques}{Kohonen}
		\label{reco_algo_carac_kohonen}
		
		\begin{xalgostep}{initialisation}
			$\forall \pa{i,j} \in \ensemble{1}{I} \times \ensemble{1}{J}, \; P_{ij} = \pa{ \frac{iX}{I}, \frac{jY}{J} }'$ \\
			$t \leftarrow 0$ \\
			$\delta  \leftarrow \sqrt{ \pa{\frac{X}{I}}^2 + \pa{\frac{Y}{J}}^2 }$
		\end{xalgostep}
		
		\begin{xalgostep}{point caractéristique le plus proche et mise à jour} \label{reco_algo_carac_kohonen_conv}
			$\alpha \leftarrow \frac{0,2}{1 + \frac{t}{XY} }$ \\
			On tire aléatoirement un pixel $p$ de l'image, si ce pixel $p$ est noir, alors~:\\
			$\pa{i^*,j^*} \leftarrow \underset{i,j} {\arg \min } \; d\pa{P_{ij}, p}$  \\
			$P_{i^*,j^*} 	\leftarrow P_{i^*,j^*} + \alpha \pa{ p - P_{i^*,j^*} }$
		\end{xalgostep}
		
		\begin{xalgostep}{mise à jour des voisins} 
			$\epsilon \leftarrow \exp \pa { \frac{1}{\delta}  \norme{P_{i^*,j^*} - p} - 1} $\\
			\begin{xforeach}{P}{V_c\pa{i^*,j^*}} 
				$\beta 	\leftarrow \alpha \, \epsilon \, \exp \pa{ - \frac{1}{\delta} \norme{P  - p} } $ \\
				$P 			\leftarrow P + \beta \pa{ p - P}$
			\end{xforeach}
		\end{xalgostep}
		
		\begin{xalgostep}{terminaison}
			$t \leftarrow t+1$ \\
			Tant que l'algorithme n'a pas convergé, retour à l'étape~\ref{reco_algo_carac_kohonen_conv}.
		\end{xalgostep}
		
		
		\end{xalgorithm}

		\begin{figure}[ht]
		$$
		\begin{tabular}{|c|c|c|c|} \hline
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_mi}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_mf}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_oi}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_of}} \\ \hline
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_li}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_lf}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_ai}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_af}} \\ \hline
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_bi}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_bf}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_vi}} &
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_vf}} \\ \hline
		\end{tabular}
		$$
		\caption{Treillis de Kohonen obtenus pour différentes lettres majuscules. }
		\label{reco_treilli_kohonen_result}
		\end{figure}
						

La figure~\ref{reco_treilli_kohonen_result} illustre le résultat de l'algorithme pour différentes lettres "M", "O", "L", "A", "B", "V". Le vecteur $V \in \R^{2IJ}$ de caractéristiques utilisé pour décrire la forme des lettres provient des coordonnées des points du treillis de Kohonen renormalisées afin que les caractéristiques appartiennent à l'ensemble $\cro{0,1}$. Chaque point du treillis est noté $P_{ij} = \pa{p^x_{ij}, p^y_{ij}}$, on note $V = \vecteur{v_1}{v_{2IJ}}$ le vecteur de caractéristiques, les $v_i$ sont définis comme suit~:

			\begin{eqnarray}
			\forall k \in \ensemble{1}{2IJ}, \; v_k = \left \{ \begin{array}{ll}
										\frac{1}{I} \; 
											p^x_{\cro{\frac{k+1}{2I}}, \frac{k+1}{2} \equiv I} & \text{si } k \text{ est impair} \\ \\
										\frac{1}{J} \; 
											p^y_{\cro{\frac{k}{2I}}, \frac{k}{2} \equiv I} & \text{si } k \text{ est pair} 
										\end{array} \right.
			\end{eqnarray}


\begin{xremark}{ancre}
La lettre "O" présente des symétries et il est possible que lors de la convergence, le treillis pivote sur lui-même. Afin d'éviter cela, les quatre coins du treillis peuvent être fixés de manière à orienter le treillis (voir figure~\ref{reco_treilli_kohonen_result_orientation}). Cette option n'a pas été testée.
\end{xremark}


		\begin{figure}[ht]
		$$
		\begin{tabular}{|c|} \hline
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/koho_oo}} \\ \hline
		\end{tabular}
		$$
		\caption{Résultat après convergence du treillis de Kohonen obtenu pour la lettre "O" en maintenant fixes 
						 les quatre extremités du treillis.}
		\label{reco_treilli_kohonen_result_orientation}
		\end{figure}
		













\subsection{Description d'un graphème à partir d'un contour}
\label{reco_point_caracteristique_contour}
\indexfrr{caractéristiques}{contour}
\indexfr{Freeman}
\indexfr{chaîne de Markov}
\indexfrr{graphème}{contour}

Il est possible de décrire un graphème par son contour mais le résultat obtenu est alors de longueur variable. Le contour est décrit par l'intermédiaire du code de Freeman, en connexité quatre ou huit. Cette séquence peut alors être modélisée par une chaîne de Markov. La figure~\ref{reco_des_contour_markov} illustre cette description pour deux exemples de lettres "E" et "O". Elle montre que les extremas sont plus marqués pour la lettre "E" que pour la lettre "O". Les barres horizontales sont traduites par les fortes transitions $\rightarrow\rightarrow$ et $\leftarrow\leftarrow$. 

				\begin{figure}[ht]
				\[
				\begin{tabular}{|c|c|c|c|}  \hline
    		\includegraphics[height=1cm, width=1cm]{\filext{../reco/image/des_e}} &
    					\begin{tabular}{c|c|c|c|c}  		
    															& $\rightarrow$		& $\uparrow$		& $\leftarrow$		&	$\downarrow$		\\ \hline
									$\rightarrow$   & 0,72	& 0,17	& 0,01	& 0,09  \\ \hline
									$\uparrow$  		& 0,27	& 0,51	& 0,20	& 0,02	\\ \hline
									$\leftarrow$		& 0,01	& 0,13	& 0,70	& 0,16	\\ \hline
									$\downarrow$		& 0,15	& 0,00	& 0,30	& 0,55	\\ 
    					\end{tabular}
    					&
    		\includegraphics[height=1cm, width=1cm]{\filext{../reco/image/des_o}} &
    					\begin{tabular}{c|c|c|c|c}  		
    															& $\rightarrow$		& $\uparrow$		& $\leftarrow$		&	$\downarrow$		\\ \hline
									$\rightarrow$   & 0,25	& 0,33	& 0,00 & 0,41  \\ \hline
									$\uparrow$  		& 0,22	& 0,50	& 0,28 & 0,00  \\ \hline	
									$\leftarrow$		& 0,00	& 0,40	& 0,32 & 0,28  \\ \hline
									$\downarrow$		& 0,29	& 0,00	& 0,21 & 0,51  \\ \hline
    					\end{tabular}
    					\\ \hline
    		\end{tabular}
    		\]
    		\caption{	Description de deux lettres à l'aide d'une modélisation du contour à l'aide de chaînes de Markov.
    							Le contour est décrit en connexité quatre et les chaînes de Markov sont du premier ordre, soit
    							seize transitions différentes. Le modèle de la lettre "E" est estimé à partir des contours des deux
    							composantes connexes.}
    		\label{reco_des_contour_markov}
    		\end{figure}


\indexfr{Kullback-Leiber}
\indexfrr{distance}{Kullback-Leiber}
\indexfrr{distance}{euclidienne}

Il reste à déterminer la distance entre deux chaînes de Markov modélisant deux contours. La distance adéquate serait une distance de Kullback-Leiber définie pour les deux chaînes de Markov $M_1$ et $M_2$. Cette distance n'est pas aisément calculable, par conséquent, les chaînes de Markov seront considérées comme les jeux de caractéristiques et comparées grâce à une distance euclidienne.

Un article récent \citeindex{Verma2004} utilise également des caractéristiques extraites du contour. Grâce à un système de reconnaissance de l'écriture manuscrite qu'il décrit entièrement, il compare les performances en reconnaissance avec celles obtenues à l'aide d'autres caractéristiques semblables aux histogrammes présentés au paragraphe~\ref{reco_graphem_histo}. Parmi ces caractéristiques figurent entre autres le nombre de changement de direction, de points au dessus et en dessous des lignes d'appui. Les caractéristiques sont multipliées en quadrillant un graphème (comme la méthode du paragraphe~\ref{reco_graphem_matrice}) et en étudiant la partie du contour incluse dans chacune des cases.







\subsection{Seconde description d'un graphème à partir d'un contour}
\label{reco_graphem_contour}
\indexfrr{graphème}{contour}
\indexfrr{caractéristiques}{contour}
\indexfr{espace vectoriel} 
\indexfrr{graphème}{distance}

Dans les deux jeux de caractéristiques précédents (\ref{reco_graphem_matrice} ,\ref{reco_graphem_histo}, \ref{reco_point_caracteristique_kohonen}, \ref{reco_point_caracteristique_contour}), les graphèmes ont été convertis en un vecteur appartenant à un espace vectoriel de dimension fixe. Cette représentation permet de déduire simplement une distance entre graphèmes. Si $\vecteur{V_1^k}{V_N^k}$ pour $k \in \acc{1,2}$ sont deux vecteurs associés à deux graphèmes, la distance $d\pa{V^1,V^2}$ sera définie à partir de la distance euclidienne.

\indexfrr{distance}{euclidienne}


Ces deux représentations décrivent un graphème dans un espace vectoriel et permettent ainsi de leur appliquer des algorithmes classiques de classification. L'objectif de ce paragraphe est de construire une distance entre deux graphèmes reflétant une similitude entre leurs contours sans passer par l'intermédiaire d'un jeu de caractéristiques inclus dans un espace vectoriel de dimension fixe.

La méthode proposée s'inspire des articles \citeindex{Davies1979}, \citeindex{Tappert1984} ou encore de la thèse \citeindex{Connell2000} et s'appuie sur le contour des graphèmes. Il ne s'agit plus de décrire un graphème dans un espace vectoriel mais de calculer une distance comparant les contours de deux formes. Pour simplifier, le contour sera réduit au contour extérieur de la lettre et le graphème ne contiendra qu'une seule composante connexe (voir paragraphe~\ref{reco_connexion_composante_connexe}). 

\indexfrr{contour}{extérieur}  
\indexfr{composante connexe}
\indexfr{Freeman}

Soient deux graphèmes $G^1$ et $G^2$ et leurs contours $C^1$ et $C^2$ décrits grâce au code de Freeman.

			$$
			\forall i \in \acc{1,2}, \; C^i = \vecteur{c^i_1}{c^i_{N^i}}
			$$

Les contours sont invariants par rotation, les lettres "p" et "d" ont des contours identiques mais afin de les dissocier, on supposera que l'origine des contours sera leur point le plus haut.

La distance $d\pa{C^1,C^2}$ entre les deux contours doit être indépendante du facteur d'échelle. Un contour deux fois plus long n'est pas forcément celui d'une lettre différente. Par conséquent, les contours $C^1$ et $C^2$ vont être transformés en $F^1$ et $F^2$ de même longueur~:


			\begin{eqnarray}
			\left .
			\begin{array}{rcl}
			F^1 &=& \vecteur{f^1_1}{f^1_{N^{12}}} = 
								\pa{  \underset{N^2 \; fois}{\underbrace{c^1_1,...,c^1_1}}, \;\;  
									c^1_2,c^1_2,...,c^1_{N^1-1},c^1_{N^1-1}, \;\;  
									\underset{N^2 \; fois}{\underbrace{c^1_{N^1},...,c^1_{N^1}}}  } \text{ noté } C^1 * N^2 \\
			F^2 &=& \vecteur{f^2_1}{f^2_{N^{12}}} = 
								\pa{  \underset{N^1 \; fois}{\underbrace{c^2_1,...,c^2_1}}, \;\;  
									c^2_2 , c^2_2,..., c^2_{N^2-1},c^2_{N^2-1}, \;\;  
									\underset{N^1 \; fois}{\underbrace{c^2_{N^2},...,c^2_{N^2}}}  } \text{ noté } C^2 * N^1 
			\end{array}									
			\right |
			\label{reco_contour_produit}
			\end{eqnarray}
			
\indexfrr{distance}{contour}		

Les deux contours $F^1$ et $F^2$ sont de même longueur $N^{12} = N^1 N^2$. On définit alors la distance $g\pa{C^1,C^2,l}$ à une rotation près $l$ comme suit~:


		\begin{eqnarray}
		g\pa{C^1 * N^2 , \; C^2 * N^1, \; l} = g\pa{F^1,F^2,l} = 
		\summy{k=1}{N^{12}} \; \indicatrice{ f^1_k \neq f^2_{k+l \equiv N^{12}} }
		\label{reco_distance_contour}
		\end{eqnarray}

Puis, on définit la distance $d\pa{C^1,C^2}$ entre deux contours par~:


		\begin{eqnarray}
		d\pa{C^1,C^2} = \dfrac{1}{N^1 N^2} 	\; \underset{1 \infegal l \infegal N^{12}}{\min} \biggacc{
																					 { 4 \min \acc{l,N^{12}-l} + 
																					 		g\pa{C^1 * N^2 , \; C^2 * N^1, \; l} } }
		\label{reco_distance_contour_min}
		\end{eqnarray}																						

Le terme $4 \min \acc{l,N^{12}-l}$ permet de pénaliser la rotation d'un contour par rapport à l'autre. Dans ce cas, une rotation d'un quart de tour est aussi coûteuse que deux contours différant sur chacune de leurs composantes. De manière évidente, la distance (\ref{reco_distance_contour_min}) vérifie~:

		\begin{eqnarray*}
		d\pa{C^1,C^2} &\in& \cro{0,2} \\ 
		d\pa{C^1,C^2} &=& 0 \Longleftrightarrow C^1 = C^2 \\
		d\pa{C^1,C^2} &=& d\pa{C^2,C^1} \\
		\end{eqnarray*}
		
De même, en reprenant la notation (\ref{reco_contour_produit})~:

		\begin{eqnarray*}
		&& \forall k \in \N^*, \; k \, g\pa{C^1 * k N^2, \; C^2 * k N^1, \; l}  = g\pa{C^1 * N^2, \; C^2 * N^1,l}  \\
		\Longrightarrow && \forall k \in \N^*, \;  d\pa{C^1, C^2 } \supegal d\pa{C^1 * k, \; C^2 } 
							= d\pa{C^1, \; C^2 *k} =  d\pa{C^1 * k, \; C^2 *k }
		\end{eqnarray*}

Il n'est pas assuré que l'inégalité triangulaire soit vérifiée même si à $l$ fixé, $g\pa{F^1,F^2,l}$ la vérifie. Soient $C^1, C^2, C^3$ trois contours, pour tout triplet $\pa{i,j,l}$ tels que $i+j=l$, on a~:

		$$
		\begin{array}{rl}
		g\pa{C^1 * N^2 N^3, \; C^3 * N^1 N^2,l} \infegal &
				g\pa{C^1 * N^2 N^3, \; C^2 * N^1 N^3,i} + \\ & g\pa{C^2 * N^1 N^2, \; C^3 * N^1 N^2,j}
		\end{array}
		$$

On peut montrer que~:

		\begin{eqnarray}
		d\pa{C^1 * N^2 N^3, \; C^3 * N^1 N^2} &\infegal& d\pa{C^1 * N^2 N^3, \; C^2 * N^1 N^3} +  \nonumber \\
																						&&			 d\pa{C^2 * N^1 N^3, \; C^3 * N^1 N^2}
		\label{reco_contour_distance_tri}																				
		\end{eqnarray}

Cette inégalité (\ref{reco_contour_distance_tri}) ne montre pas que la fonction $d\pa{C^1,C^2}$ est une distance. Toutefois, elle s'en rapproche car il est possible de montrer que~:

		$$
		\forall k \in \N^*, \;  d\pa{C^1, C^2 } \infegal d\pa{C^1 * k, \; C^2 * k } + \dfrac{k-1}{N^1 N^2}
		$$

Par conséquent, l'inégalité vérifie par $d$ est~:

		\begin{eqnarray}
		d\pa{C^1, C^3} &\infegal& d\pa{C^1, C^2} + d\pa{C^2, C^3} + \dfrac{1}{N^2}
		\label{reco_contour_distance_tri2}																				
		\end{eqnarray}


Bien que la description d'un contour n'appartienne pas à un espace vectoriel de dimension fixe, cette pseudo inégalité triangulaire permet d'envisager l'utilisation d'algorithme de classification tel que celui développé en annexe\seeannex{space_metric_introduction}{espace métrique} et utilisé au paragraphe~\ref{reco_selection_caracteristique} permettant la recherche des voisins les plus proches et donc la classification.






\begin{xremark}{distance d'édition}
La comparaison \indexfrr{distance}{édition} de deux contours est plus pertinente si la distance (\ref{reco_distance_contour}) est remplacée par une distance d'édition\seeannex{definition_edit_dist_tronc}{distance d'édition} comme celle de Levenstein. \indexfr{Levenstein} De cette manière, certains écueils sont évités comme celui de la figure~\ref{reco_probleme_distance_contour_1} où la discrétisation du tracé des lignes peut poser des problèmes.
\end{xremark}


				\begin{figure}[ht]
				$$
				\tiny
				\begin{tabular}{cc}
\begin{tabular}{|p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}|}\hline
				. & . & . & . & . & . & . & . & . & . & . \\  
				. & X & X & X & X & X & . & . & . & . & . \\  
				. & X & . & . & . & X & X & . & . & . & . \\  
				. & X & X & . & . & . & X & X & . & . & . \\  
				. & . & X & X & . & . & . & X & X & . & . \\  
				. & . & . & X & X & . & . & . & X & X & . \\  
				. & . & . & . & X & X & . & . & . & X & . \\  
				. & . & . & . & . & X & X & X & X & X & . \\  
				. & . & . & . & . & . & . & . & . & . & . \\ \hline
				\end{tabular}
				&
\begin{tabular}{|p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}@{}p{2mm}|}\hline
				. & . & . & . & . & . & . & . & . & . & . \\  
				. & X & X & X & X & X & X & . & . & . & . \\  
				. & X & . & . & . & . & X & X & . & . & . \\  
				. & X & X & . & . & . & . & X & X & . & . \\  
				. & . & X & X & . & . & . & . & X & X & . \\  
				. & . & . & X & X & . & . & . & . & X & . \\  
				. & . & . & . & X & X & . & . & . & X & . \\  
				. & . & . & . & . & X & X & X & X & X & . \\  
				. & . & . & . & . & . & . & . & . & . & . \\ \hline
				\end{tabular}
				\end{tabular}
				$$
				\caption{	Ces deux trapèzes diffèrent par la longueur de leur base haute et il est
									préférable d'utiliser une distance d'édition plutôt qu'un simple décalage
									pour la distance (\ref{reco_distance_contour_min}) qui ne pourra faire correspondre 
									qu'une moitié de ces deux contours qui diffèrent de deux petits déplacements avec
									une distance d'édition.}
				\label{reco_probleme_distance_contour_1}
				\end{figure}










\subsection{Moments invariants}
\label{reco_moment_invariant_par}
\indexfrr{moments}{invariants}
\indexfr{rotation}
\indexfr{translation}

Les étapes de prétraitements d'images incluent fréquemment le redressement des lettres inclinées (voir paragraphe~\ref{image_redressement_sobel}, page~\pageref{image_redressement_sobel}) qui ressemble presque à une rotation des lettres. Plutôt que d'approfondir dans cette voie, les moments invariants ont été introduits par \citeindex{Hu1961} puis étendus par \citeindex{Li1992}, \citeindex{Jin2004}, afin de décrire une forme de telle façon que cette description soit invariante par rotation et translation. Ces moments d'ordres différents ne sont pas tous de même dimension ni de même ordre de grandeur, il est par conséquent impossible des utiliser tel quel. L'article \citeindex{Wong1995} poursuit cette recherche en étendant la liste des moments invariants et en étudiant le facteur renormalisateur obtenant les meilleurs taux de reconnaissance de caractères. L'image est définie par une fonction $f\pa{x,y} \in \acc{0,1}$ qui vaut $1$ si le pixel appartient à la forme et $0$ dans le cas contraire. L'article définit les moments $\mu_{pq}$ par~:

			\begin{eqnarray}
			\overline{x} &=& \int_x \int_y \; x \; f \pa{x,y} \; dx dy \nonumber \\
			\overline{y} &=& \int_x \int_y \; y \; f \pa{x,y} \; dx dy \nonumber \\
			\mu_{pq} &=& \int_x \int_y \; \pa{x- \overline{x}}^p \; \pa{y- \overline{y}}^p \; f\pa{x,y} \; dx dy
			\end{eqnarray}
Une première liste de moments invariants $I_{pq}$ par rotation est définie dans \citeindex{Hu1962}~:

			\begin{eqnarray}
			\begin{array} {rcl|rcl}
			I_{20} &=& \pa{\mu_{20} - \mu_{02}} - 2i \mu_11 	& I_{30} &=& \overline{I_{03}} \\
			I_{11} &=& \mu_{20} + \mu_{02}  									& I_{40} &=& \pa{\mu_{40} - 6\mu_{22} 
																															+ \mu_{04} } - 	4i\pa{\mu_{31} - \mu_{13}}		\\
			I_{02} &=& \overline{I_{20}} 											& I_{31} &=& \pa{\mu_{40} - \mu_{04}} 
																														- 2i\pa{\mu_{31} + \mu_{13}}		\\
			I_{30} &=& \pa{\mu_{30}- 3\mu_{12}} 
										- i \pa{3\mu_{21} + \mu_{30}}				& I_{22} &=& \mu_{40} + 2 \mu_{22} + \mu_{04}		\\
			I_{21} &=& \pa{\mu_{30} + \mu_{12}} 
										- i \pa{\mu_{21} + \mu_{03}}  			& I_{13} &=& \overline{I_{13}} \\
			I_{12} &=& \overline{I_{21}}											& I_{04} &=& \overline{I_{04}}	
			\end{array} \label{reco_moment_invariant}
			\end{eqnarray}


Ces premiers nombres se déduisent de moments invariants calculés en coordonnées polaires. L'article \citeindex{Wong1995} définit automatiquement d'autres moments invariants par rotation à partir de combinaisons non linéaires à partir des moments définis en (\ref{reco_moment_invariant}) dont une liste non exhaustive est donnée par la table~\ref{reco_moment_invariant_liste_wong}. Chaque moment est ensuite renormalisé par le facteur $\frac{1}{\alpha_i}$ où $\alpha_i$ est l'amplitude du moment~$i$.


		\begin{table}[ht]
		$$\begin{array}{|lll|} \hline
		I_{11}										&	I_{20} I_{02}							&\\ \hline
		I_{21} I_{12}							& I_{30} I_{03} 						& I_{21}^2 I_{02} \\
		I_{30} I_{12} I_{20}			& I_{30} I_{12}^3						& I_{30} I_{21} I_{02}^2 \\
		I_{30}^2 I_{02}^3					& I_{30} I_{12}^5 I_{02} 		& \\ \hline
		I_{22}										& I_{31} I_{13}							& I_{40} I_{04}  \\
		I_{31} I_{02}							& I_{40} I_{13}^2						& I_{40} I_{02}^2 \\
		I_{31} I_{12}^2						& I_{40} I_{13} I_{02}			& I_{40} I_{03} I_{12} \\
		I_{31} I_{03} I_{21}			& I_{40} I_{31} I_{03}^2 		& I_{40} I_{13} I_{12}^2 \\
		I_{40} I_{13}^3 I_{20}		& I_{40} I_{03}^2 I_{20}		& ...
 		\\ \hline \end{array}$$
		\caption{	Moments invariants calculés par la méthode présentée dans \citeindexfig{Wong1995}. Ces 
							moments sont complexes mais pour chacun d'eux, les parties réelle
							et imaginaire (si elles sont non nulles) peuvent être considérées comme caractéristiques.}
		\indexfr{complexe}
		\indexfrr{partie}{réelle}
		\indexfrr{partie}{imaginaire}
		\indexfr{caractéristiques}
		\label{reco_moment_invariant_liste_wong}
		\end{table}


Le récent article \citeindex{Heikkilä2004} tente d'apparier une image bruitée à son modèle à partir de ces moments et mesure la sensibilité des caractéristiques choisies par rapport au bruit.\indexfr{sensibilité au bruit}








\subsection{Moments de Zernike}
\label{reco_feature_moment_zernike}
\indexfrr{Zernike}{moments}
\indexfrr{moments}{Zernike}
\indexfrr{moments}{invariants}
\indexfrr{fonction}{orthogonale}


Les moments de Zernike (voir \citeindex{Trier1996}) sont des projections de l'image d'entrée $f\pa{x,y} \in \cro{0,1}$ sur un espace de fonctions engendré par la suite de fonctions orthogonales $\pa{ V_{mn}\pa{x,y} }$ définie pour $n \supegal 0$, $\abs{m} \infegal n$ et $n - \abs{m}$ est pair~:


			\begin{eqnarray}
			V_{mn}\pa{x,y}						&=&		R_{mn}\pa{x,y} e^{\pa{i \, m \, \theta\pa{x,y} }} \\
			\text{où } R_{mn}\pa{x,y} &=&   \summy{s = 0}{ \frac{n - \abs{m} }{2} } \;
																			\frac{ \pa{-1}^s \, \pa{ x^2 + y^2 } ^ {  \frac{n}{2} - s } \, \pa{n-s}! }
																					 { s! \, \pa { \frac{ n + \abs{m} } {2} - s } ! \,
																					 				 \pa { \frac{ n - \abs{m} } {2} - s } ! }
			\end{eqnarray}

$\theta\pa{x,y}$ correspond à l'angle entre les vecteur $\vectt{x}{y}$ et $\vectt{1}{0}$. Le moment de Zernike $A_{nm}$ d'ordre $n$ et de répétition $m$ est défini comme~:

			\begin{eqnarray}
			A_{nm}		&=&	\frac{n+1}{\pi} \;	\summyone{x^2 + y^2 \infegal 1} \, 
													f\pa{x,y} \, \overline{V_{nm}\pa{x,y}}
			\end{eqnarray}

L'image doit être incluse dans un disque de rayon l'unité mais dans ce cas, l'image originale peut être reconstruite~:

			\begin{eqnarray}
			f\pa{x,y} &=&  \underset{n \rightarrow \infty} {\lim } \; \summy{i=0}{n} \; 
											\summyone{ \twoindices{ \abs{m} \infegal n}{ n-\abs{m} \text{ pair} } } \;
											A_{nm} \, V_{nm}\pa{x,y}
			\end{eqnarray}
			
\indexfrr{moments}{invariants}
\indexfr{rotation}
			
En pratique, les coordonnées $x$ et $y$ utilisées ici sont obtenues à partir des coordonnées des pixels en déplaçant l'origine de l'image au centre de gravité puis en changeant son échelle de telle sorte que $x^2 + y^2 \infegal 1$ pour tout pixel.
			
L'intérêt des moments de Zernike est l'invariance par rotation des modules $\abs{A_{nm}}$ -~ils ne dépendent que de $x^2 + y^2$~-. Si celle-ci n'est pas souhaitable (distinction entre les chiffres six et neuf par exemple), il suffit de considérer les parties réelles et imaginaires de ces moments. Comme pour les moments invariants présentés au paragraphe~\ref{reco_moment_invariant_par}, il est nécessaire de renormaliser les caractéristiques obtenues afin qu'une distance euclidienne puisse être utilisée pour la recherche de plus proches voisins (voir paragraphe~\ref{reco_selection_caracteristique}).









\subsection{Moments estimés à partir d'ondelettes}
\label{reco_feature_moment_ondelette_par}
\indexfrr{moments}{Zernike}
\indexfrr{moments}{Li}
\indexfrr{moments}{ondelettes}
\indexfrr{moments}{invariants}
\indexfr{ondelettes}


Plus récemment, l'article \citeindex{Shen1999} compare les moments de Zernike, de Li et des moments invariants estimés à partir d'ondelettes, sujet de l'article. Les résultats obtenus sont en faveur des moments calculés à partir des ondelettes (100\% de reconnaissance), puis ceux de Zernike légèrement moins bons  (98,7\%) et enfin ceux de Li (75,3\%). La base d'ondelettes choisie dans  \citeindex{Shen1999} est celle définie par (\ref{reco_ondelette_shen}) dont les paramètres sont les suivants~:
 
			$$
			\begin{array}{cccc}
			p = 3 &	a = 0,697066 & f_0 =	0,409177 & \sigma_w^2 = 0,561145
			\end{array}
			$$
			
L'ondelette est alors définie par~:			
			
			\begin{eqnarray}
			\psi\pa{r} 	&=& \frac{4a^{p-1}} {\sqrt{2\pi \pa{p+1}}} \; \sigma_w \;
											\cos \cro{ 2\pi f_0 \pa{2r-1}} \;
											\exp \cro{ - \frac{\pa{2r-1}^2} {2\sigma_w^2 \pa{p+1}} }
			\label{reco_ondelette_shen}
			\end{eqnarray}

Où $r = \sqrt{x^2+y^2}$, finalement, le moment invariant $F_{m,n,q}$ est défini par~:

			\begin{eqnarray}
			F_{m,n,q}  	&=&	\frac{1}{\pi} \;	\summyone{x^2,y^2 \infegal 1} \, 
													f\pa{x,y} \, e^{i \, q \, \theta\pa{x,y} } \;
													2^{\frac{m}{2}} \; \psi \pa{2^m r - \frac{1}{2}n }
			\end{eqnarray}

$\theta\pa{x,y}$ correspond à l'angle entre les vecteur $\vectt{x}{y}$ et $\vectt{1}{0}$. Ces moments sont calculés pour $m,q \in \acc{1,2,3,4}$, et $n \in \acc{0, 1, ..., 2^{m+1}}$.







\subsection{Profil en coordonnées polaires}
\label{reco_profil_polair}
\indexfr{coordonnées polaires}
\indexfr{profils}
\indexfr{histogramme}
\indexfr{abscisse curviligne}


Plutôt que de projeter le profil d'une lettre sur l'un des axes du plan, il est possible de calculer une fonction $f : \theta \in \cro{0,2\pi} \longrightarrow f\pa{\theta}$ en coordonnées polaires ayant pour origine le centre de gravité $G$ de l'image (voir figure~\ref{reco_profil_coordonnee_polaire}, \citeindex{Trier1996}).


		\begin{figure}[ht]
		$$
		\begin{tabular}{|c|} \hline
		\includegraphics[height=2cm, width=2cm]{\filext{../reco/image/letapol}} \\ \hline
		\end{tabular}
		$$
		\caption{Parcours d'une lettre en tournant autour de son centre de gravité.}
		\label{reco_profil_coordonnee_polaire}
		\end{figure}

La fonction $f$ peut être le nombre de pixels noirs le long de la demi-droite ayant pour origine $G$ et formant un angle $\theta$ ayant l'axe des abscisses, elle peut être la distance entre l'origine et le point d'intersection entre cette demi-droite et le contour, elle peut être le nombre de transitions entre pixels blancs et noirs, ces caractéristiques sont les mêmes que celles présentées au paragraphe~\ref{reco_graphem_histo} à ceci près qu'elles sont estimées sur une droite en rotation par rapport au centre de gravité et non par rapport à une droite parallèle à l'un des axes du plan. Il suffit ensuite de découper l'intervalle $\cro{0,2\pi}$ en $N$ intervalles identiques pour obtenir un vecteur de $N$ caractéristiques. La figure~\ref{reco_profil_coordonnee_polaire_profil} illustre le résultat obtenu, les profils obtenus pour deux lettres "A" sont sensiblement identiques et diffèrent largement de la lettre "I".



		\begin{figure}[ht]
		$$
		\begin{tabular}{|c|c|c|} \hline
		\includegraphics[height=1cm, width=1cm]{\filext{../reco/image/profila1}} &
		\includegraphics[height=1cm, width=1cm]{\filext{../reco/image/profila2}} &
		\includegraphics[height=1cm, width=1cm]{\filext{../reco/image/profili1}} \\ 
		\includegraphics[height=3cm, width=4cm]{\filext{../reco/image/profila1_}} &
		\includegraphics[height=3cm, width=4cm]{\filext{../reco/image/profila2_}} &
		\includegraphics[height=3cm, width=4cm]{\filext{../reco/image/profili1_}} \\
		A & A & I \\ \hline
		\end{tabular}
		$$
		\caption{Différents profils de lettres, les deux profils de lettre "A" sont assez proches montrant 
							trois pics, tandis que le profil de la lettre "I" n'en contient que deux.}
		\label{reco_profil_coordonnee_polaire_profil}
		\end{figure}


L'angle $\theta$ peut être remplacé par l'abscisse curviligne $t$ le long du contour~: $f : t \in \cro{0,T} \longrightarrow f\pa{t} = \pa{x\pa{t},y\pa{t}} \in \R^2$. Le raisonnement est identique à celui qui précède. En revanche, l'image ne doit contenir qu'une seule composante connexe ou s'y ramener en les connectant si elles sont plusieurs (voir paragraphe~\ref{reco_connexion_composante_connexe}, page~\pageref{reco_connexion_composante_connexe}).

\indexfrr{contour}{origine}
\indexfrr{origine}{contour}

Toutefois, le problème de l'origine du contour est encore indéterminé. L'article \citeindex{Wunsch1995} suggère que ce point soit choisi comme étant le plus proche du coin supérieur gauche. Afin d'obtenir des profils comparables, l'origine est choisie comme étant le point le plus proche du coin inférieur droit. Ce choix est arbitraire et peut entraîner de lourdes variations dans le profil obtenu pour le contour, la figure~\ref{reco_profil_coordonnee_polaire_letc} illustre deux lettres "F" pour lesquelles les deux profils obtenus seront déphasés. Quel que soit le point de départ, il est vraisemblable que ces cas apparaissent, l'essentiel est que chaque configuration possible ne soit pas un cas isolé ce qui est peu vraisemblable pour de grandes bases de données.



		\begin{figure}[ht]
		$$
		\begin{tabular}{|c|c|} \hline
		\includegraphics[height=1cm, width=0.8cm]{\filext{../reco/image/letc1}} &
		\includegraphics[height=1cm, width=0.8cm]{\filext{../reco/image/letc2}} \\ \hline
		\end{tabular}
		$$
		\caption{	Deux lettres "F" pour lesquelles la règle qui consiste à choisir le point du contour 
							le plus proche du coin inférieur droit retourne deux points différents. 
							Les deux profils seront donc déphasés.}
		\label{reco_profil_coordonnee_polaire_letc}
		\end{figure}










\subsection{Descripteurs de Fourier du contour}
\label{reco_feature_fourier_contour}
\indexfr{Fourier}
\indexfr{contour}
\indexfr{abscisse curviligne}



En supposant que l'image à décrire ne soit composée que d'une seule composante connexe, il est possible de considérer le contour extérieur de cette forme comme une fonction $f : t \in \cro{0,T} \longrightarrow f\pa{t} = \pa{x\pa{t},y\pa{t}} \in \R^2$ où $t$ est l'abscisse curviligne le long du contour, $T$ sa longueur totale et $f\pa{t}$ le rayon mesuré depuis le contour jusqu'au centre de gravité de l'image. Si l'image est composée de plusieurs composantes connexes, il est possible de les connecter entre elles afin de n'en avoir plus qu'une seule (voir paragraphe~\ref{reco_connexion_composante_connexe}, page~\pageref{reco_connexion_composante_connexe}). L'article \citeindex{Kuhl1982} (voir également \citeindex{Trier1996}) propose de construire la transformée de Fourier des deux courbes $x\pa{t}$ et $y\pa{t}$~:


		\begin{eqnarray}
		\hat{x}\pa{t} &=&  a_0 + \summy{n=1}{N} \; \cro{  a_n \cos \frac{2n\pi t}{T} + b_n \sin \frac{2n\pi t}{T} } \\
		\hat{y}\pa{t} &=&  c_0 + \summy{n=1}{N} \; \cro{  c_n \cos \frac{2n\pi t}{T} + d_n \sin \frac{2n\pi t}{T} } 
		\end{eqnarray}

\indexfr{Freeman}
\indexfr{4-connexité}

Les coefficients obtenus seront les caractéristiques qui décriront l'image. Etant donné que le contour extrait est une chaîne de Freeman composée d'une multitude de segments linéaires. Si on suppose que ce contour est décrit en 4-connexité, il est donc composé de $T$ petits segments de longueur $1$, le contour balaye les points $\pa{x_t,y_t} _ { 1 \infegal t \infegal T} $ et $\pa{x_0,y_0} = \pa{x_T, y_T}$. La valeur exacte des coefficients est donnée par les formules suivantes~:

		\begin{eqnarray}
		a_n 		&=&  \frac{T}{2n^2 \pi^2} \; \summy{i=1}{T} \; \pa{x_t - x_{t-1}} \; 
		                                     \cro{\cos \frac{2n\pi i}{T} - \cos \frac{2n\pi \pa{i-1}}{T} } \\
		b_n 		&=&  \frac{T}{2n^2 \pi^2} \; \summy{i=1}{T} \; \pa{x_t - x_{t-1}} \; 
		                                     \cro{\sin \frac{2n\pi i}{T} - \sin \frac{2n\pi \pa{i-1}}{T} } \\
		c_n 		&=&  \frac{T}{2n^2 \pi^2} \; \summy{i=1}{T} \; \pa{y_t - y_{t-1}} \; 
		                                     \cro{\cos \frac{2n\pi i}{T} - \cos \frac{2n\pi \pa{i-1}}{T} } \\
		d_n 		&=&  \frac{T}{2n^2 \pi^2} \; \summy{i=1}{T} \; \pa{y_t - y_{t-1}} \; 
		                                     \cro{\sin \frac{2n\pi i}{T} - \sin \frac{2n\pi \pa{i-1}}{T} }
		\end{eqnarray}                      
		
\indexfr{ellipse}		
		
Comme dans la méthode précédente, deux jeux de caractéritisques ne sont comparables que si les deux origines des contours coïncident. On introduit alors un paramètre de phase mesurant le décalage $\theta_1$ de cette origine par rapport à l'axe principal du contour~:

		\begin{eqnarray}
		\theta_1 &=&	\frac{1}{2}  \; \arctan \frac	{ 2\pa{a_1 b_1 + c_1 d_1} }
																								{	\sqrt{ a_1^2 - b_1^2 + c_1^2 - d_1^2 }	}
		\end{eqnarray}




L'article \citeindex{Kuhl1982} suggère de placer l'origine du contour au sommet du plus grand axe de l'ellipse, les nouveaux coefficients de la tranformée de Fourier $\pa{a_n,b_n,c_n,d_n}_{ n \supegal 1}$ sont obtenus comme suit~:

		\begin{eqnarray}
		\forall n \supegal 1, \; \cro{ \matfour{a^*_n}{b^*_n}{c^*_n}{d^*_n} } &=&
																	\cro { \matfour{a_n}{b_n}{c_n}{d_n} }	
																\cro { \matfour{\cos n\theta_1}{-\sin n\theta_1}{\sin n\theta_1}{\cos n\theta_1} }	
		\end{eqnarray}

Enfin, la forme est tournée de telle sorte que le grand axe de l'ellipse coïncide avec l'axe des abscisses de sorte que les coefficients obtenus seront invariants par rotation. L'axe principal de l'ellipse forme un angle $\theta_2 = \arctan \frac{c_1^*}{a_1^*}$ et~:

		\begin{eqnarray}
		\forall n \supegal 1, \; \cro{ \matfour{a^{**}_n}{b^{**}_n}{c^{**}_n}{d^{**}_n} } &=&
																	\cro { \matfour{a_n^*}{b_n^*}{c_n^*}{d_n^*} }	
																\cro { \matfour{\cos n\theta_2}{-\sin n\theta_2}{\sin n\theta_2}{\cos n\theta_2} }	
		\end{eqnarray}

Les suites de coefficients $\pa{a^{**}_n,b^{**}_n,c^{**}_n,d^{**}_n}$ sont les coefficients désirés.









\subsection{Autres descriptions}



				\begin{figure}[ht]
		    $$\begin{tabular}{|c|} \hline
		    \filefig{../reco/fig_zhang}
		    \\ \hline \end{tabular}$$
				\caption{	Classification sémantique des représentations (ou descriptions) de formes
									établie dans l'article \citeindexfig{ZhangD2004}. 
									La table~\ref{reco_figure_class_semantique_forme_table} précise quelques
									termes utilisés dans ce schéma.}
				\label{reco_figure_class_semantique_forme}
				\indexfrr{distance}{Hausdorff}
				\indexfr{Hausdorff}
				\indexfr{excentricité}
				\indexfr{elastic matching}
				\indexfr{variation d'échelle}
				\indexfr{flou}
				\indexfr{squelette}
				\end{figure}



				\begin{table}[ht]
		    $$\begin{tabular}{|c|} \hline \\
		    \begin{minipage}{16cm}
		    Les invariants (premier cadre)
				regroupent toutes sortes de nombres intrinsèques à une forme et invariants par
				des transformations telles que les rotations ou les projections. Ces nombres peuvent être
				des orientations, des rapports de surfaces, de longueurs... L'excentricité est le rapport 
				petit axe sur grand axe. Pour deux ensembles de points $A = \ensemble{A_1}{A_p}$ et
				$B = \ensemble{B_1}{B_q}$, la distance de Hausdorff est définie par~: 
				$H\pa{A,B} = \max \acc{h\pa{A,B},h\pa{B,A}}$ où
				$h\pa{A,B} = \underset{a \in A}{\max}  \underset{b \in B}{\min}\norme{a-b}$.
				La signature d'une forme correspond au profil en coordonnées polaires 
				(voir paragraphe~\ref{reco_profil_polair}). 
				Plus généralement, une signature est une courbe 
				à une dimension déduite d'un contour. L'"elastic matching" correspond à une distance égale au
				coût d'une transformation permettant de passer d'un contour à un autre. Les méthodes 
				auto-régressives envisagent les contours comme des séries temporelles. 
				La variation d'échelle
				est une description de la forme à plusieurs échelles différentes, semblable à l'introduction 
				de flou. La méthode grille réduit l'image contenant la forme à une grille binaire. Les matrices de forme 
				généralisent la méthode présentée au paragraphe~\ref{reco_graphem_matrice} à toutes sortes de 
				quadrillages plaquées sur l'image, quadrillages rectangulaires, circulaires...
				\end{minipage}
		    \\ \\ \hline \end{tabular}$$
				\caption{	Quelques termes utilisés pour le schéma~\ref{reco_figure_class_semantique_forme},
									 extraits de \citeindexfig{ZhangD2004}.}
				\label{reco_figure_class_semantique_forme_table}
				\indexfrr{distance}{Hausdorff}
				\indexfr{Hausdorff}
				\indexfr{excentricité}
				\indexfr{elastic matching}
				\indexfr{variation d'échelle}
				\indexfr{flou}
				\indexfr{squelette}
				\end{table}




Les descriptions présentées ci-dessus ne sont qu'un échantillon des méthodes utilisées en reconnaissance des formes. Les plus courantes sont passées en revue dans l'article \citeindex{Trier1996} qui a inspiré certaines des caractéristiques abordées dans ce document. Plus récemment, l'article \citeindex{ZhangD2004} établit une classification sémantique des représentations de formes, celle-ci est esquissée par la figure~\ref{reco_figure_class_semantique_forme}.


\indexfr{ondelettes}
\indexfr{morphing}
\indexfr{elastic matching}
\indexfr{Fourier}
\indexfr{ACP}
\indexfr{coordonnées polaires}

Toutefois, parmi ces descriptions, la conception d'un système de reconnaissance fondé sur des modèles de Markov cachés impose une séparation en deux familles~: les caractéristiques qui font partie d'un espace vectoriel de dimension finie -~par conséquent beaucoup plus simples d'utilisation~- et celles dont l'espace de réprésentation est de dimension infinie comme la représentation du contour par une chaîne de Freeman. 

Dans la première catégorie, les ondelettes sont souvent utilisées pour décrire des images et sont parfois appliquées aux caractères comme au paragraphe~\ref{reco_feature_moment_ondelette_par} ou dans l'article \citeindex{Chen2003}. Il existe des méthodes plus élaborées incluant transformée de Fourier, analyse en composantes principales (ACP) comme dans l'article \citeindex{Glendinning2003}. Ces méthodes s'appliquent directement sur les graphèmes sans que ceux-ci ne soient prétraités. D'autres utilisent une information plus synthétique comme le contour qui transforme une image à deux dimensions en une fonction de l'abscisse curviligne. Cette fonction peut-être lissée ou traitée comme un signal au moyen de série de Fourier (paragraphe~\ref{reco_feature_fourier_contour}) ou des ondelettes (\citeindex{Tao2001}). 

La seconde catégorie de caractéristiques permet plus de liberté puisqu'il ne s'agit ici que de définir une distance entre caractères. Ce peut être une distance fondée sur le "morphing" d'images (voir~\citeindex{Sederberg1992}) ou ce qu'on appelle "elastic matching" (voir~\citeindex{Uchida2003}). Dans ces deux cas, la distance correspond au coût d'une transformation permettant de transformer une image en une autre.

\indexfrr{contour}{actif}
\indexfr{snake}
\indexfr{Malahanobis}

Cette seconde catégorie inclut également une autre approche complètement différente qui est proposée dans l'article \citeindex{Khorsheed2003} puisqu'elle est basée sur le squelette plutôt que le contour pour la reconnaissance de mots arabes. Chaque arc du squelette est ensuite vectorisé mais ce découpage d'une image en une multitude de petits morceaux implique une description plus succinte de ces arcs regroupés en un graphe. La description d'un mot n'est plus une séquence de graphèmes mais un graphe de segments. Les travaux développés dans cet article sont toutefois très liés à la structure de l'écriture arabe. \indexfrr{écriture}{arabe} Cette description fondée sur le squelette est celle aussi choisie dans l'article \citeindex{Ruberto2004} et décrite au paragraphe~\ref{squelettisation_graphe_construction}, la distance entre deux caractères\footnote{L'auteur de l'article \citeindex{Ruberto2004} n'applique pas sa méthode aux caractères explicitement mais à la reconnaissance des formes en général.} est alors définie comme une distance entre graphes, eux-mêmes construits à partir du squelette. 

Une dernière approche est détaillée dans les articles \citeindex{Amit1997} et la thèse \citeindex{Senior1994} qui consiste à extraire de la base d'apprentissage les caractéristiques les plus fiables. L'article \citeindex{Amit1997} a déjà été présenté au paragraphe~\ref{biblio_geman}, page~\pageref{biblio_geman}. La thèse \citeindex{Senior1994} propose une extraction de caractéristiques à base de contours actifs (ou snakes), ce ne sont pas les seuls mais elles permettent de mieux détecter des formes courantes telles que le "u", "u" inversé, une boucle, un trait vertical. La convergence du contour actif vers une de ces formes complète la décription des graphèmes faite à partir de caractéristiques extraites depuis le squelette de l'image. La forme du contour est comparée à des formes les plus probables grâce à une distance de Malahanobis\footnote{La distance de Malahanobis est une distance euclidienne modifiée. Soit $X$, $Y$, deux vecteurs de $R^n$, la distance de Malahanobis entre ces deux vecteurs est définie par~: $ d\pa{X,Y} = \pa{X-Y}' V^{-1} \pa{X-Y}'$ où $V$ est une matrice de variance covariance. Lorsque la matrice $V$ est estimée sur un nuage de points indépendants et distribués selon une loi normale, cette distance est équivalente à une distance euliclidienne après un changement de répère défini par la matrice $\sqrt{V}$ ($\sqrt{V}$ est définie puisque $V$ est symétrique est donc diagonalisable).\indexfrr{matrice}{diagonalisable}\indexfrr{matrice}{symétrique}}, si cette distance dépasse un certain seuil, le caractère ou graphème en question possède cet attribut de forme.

\indexfr{dissimilarités}
Toutes ces caractéristiques sont construites depuis l'image et, d'une certaine manière, décrivent un de ses aspects. L'article \citeindex{Bicego2004} explore la possibilité de construire un ensemble de dissimilarité élaboré à partir de modèles probabilistes, dans ce cas, des modèles de Markov cachés. Le vecteur de caractéristiques utilisés pour décrire une image est cette fois-ci un vecteur de probabilités $\pa{\pr{image \sac M_i}}_i$. $M_i$ est un modèle de Markov caché ayant appris l'ensemble des images de la classe~$i$ ou de préférence un ensemble représentatif de cette classe. Cette méthode propose l'avantage de pouvoir réduire à un seul vecteur tout image ou tout type d'objet comme une séquence d'observations par exemple.



Un article a récemment été publié (\citeindex{LiuCL2003}) comparant, dans le cadre de la reconnaissance de chiffres manuscrits, les taux de reconnaissance pour divers types de caractéristiques et différents classifieurs tels que les réseaux de neurones, les \textit{support vector machines} ou d'autres modèles reposant sur des apprentissages. Les caractéristiques utilisées sont construites à partir d'images binaires converties en images 20x20 en niveaux de gris. C'est ce que propose de faire le paragraphe suivant~: comparer les performances en classification des caractéristiques développées ci-dessus par le biais d'un classifieur basé sur les plus proches voisins.








%---------------------------------------------------------------------------------------------------------------
\section{Sélection des caractéristiques}
%---------------------------------------------------------------------------------------------------------------
\label{reco_selection_caracteristique}

\indexfrr{caractéristiques}{sélection}
\indexfr{classification}




\subsection{Sélection des caractéristiques des graphèmes}
\label{reco_sel_feat_graph}



Parmi les jeux de caractéristiques proposés (paragraphe~\ref{reco_description_grapheme}), quelle représentation est la mieux adaptée à la reconnaissance de caractères manuscrits~? L'idéal serait de comparer les performances obtenues en reconnaissance pour l'ensemble du système de reconnaissance. Deux inconvénients majeurs s'opposent à ce procédé, d'une part, ce processus est long car il inclut de nombreux apprentissages (chaînes de Markov cachées, réseaux de neurones)~; d'autre part, les apprentissages doivent être réitérés de nombreuses fois afin que leurs résultats puissent être comparés. En effet, l'apprentissage des réseaux de neurones comme des chaînes de Markov cachées sont effectués grâce à des optimisations utilisant le gradient, ces méthodes convergent vers un minimum de la fonction d'erreur (ici, la vraisemblance des modèles) qui peut être local. Par conséquent, il faudrait répéter plusieurs fois la même phase d'apprentissage afin de s'assurer de la pertinence des comparaisons entre modèles appris sur des jeux de caractéristiques différents. Une autre remarque concerne le nombre de coefficients de ces modèles qui varie forcément puisque les caractéristiques proposées sont de dimensions différentes. Dans ce cas, il faut choisir entre comparer les performances de modèles à coefficients constants et ne considérer que le meilleur modèle, quel que soit le nombre de ses coefficients. Enfin, le jeu de caractéristiques (paragraphe~\ref{reco_graphem_contour}) est une séquence et ne pourrait donc pas être comparé via cette méthode aux autres descriptions de graphèmes qui appartiennent à des espaces vectoriels de dimension finie. 



\indexsee{plus proches voisins}{kPPV}
\indexfr{kPPV@kPPV}
\indexfrr{base}{apprentissage}
\indexfrr{base}{test}
\indexsee{kNN}{KPPV}
\indexsee{k-nearest neigbours}{KPPV}

Ces considérations incitent à se diriger vers d'autres méthodes de sélection de caractéristiques, sachant qu'elles sont utilisées dans le cadre d'un problème de classification. Les documents \citeindex{Fukunaga1990} ou \citeindex{Choi2003b} proposent des alternatives mais ils supposent néanmoins que ces caractéristiques soient incluses dans un espace vectoriel de dimension finie. La solution retenue s'appuie sur l'algorithme des plus proches voisins et une base d'environ 20000 caractères majuscules (A~à~Z). La base est divisée en deux parties de même importance. La première constitue la base d'apprentissage (notée~$B_a$), c'est dans cette base que seront cherchés les voisins des éléments de la seconde base intitulée la base de test (notée~$B_t$). Soit $g \in B_t$, $V_a^k\pa{g}$ est l'ensemble de $k$ voisins de $g$ inclus dans la base $B_a$. La classe d'un élément $g$ est notée $c\pa{g}$ et l'estimation de cette classe à partir du voisanage est notée $\widehat{c\pa{k,g}}$. On définit pour chaque classe $i$ de A~à~Z et chaque élément $g$, le coefficient $w\pa{k,g,i}$ défini comme suit~:

		\begin{eqnarray}
		w\pa{k,g,i} = \left \{ \begin{array}{ll}
												\infty  & \text{si } g \in V_a^k\pa{g} \text{ et } c\pa{g} = i \\
												\summyone{h \in V_a^k\pa{g}} \; \indicatrice{c\pa{h} = i}  \; \dfrac{1}{ d\pa{g,h} } 
														& \text{sinon }
														\end{array} \right.
		\end{eqnarray}

On définit également un critère~:

		\begin{eqnarray}
		Cr\pa{k,g,i} = \left \{ \begin{array}{ll}
														1 & \text{si } g \in V_a^k\pa{g} \text{ et } c\pa{g} = i \\
														0 & \text{si } g \in V_a^k\pa{g} \text{ et } c\pa{g} \neq i \\
														\dfrac{ w\pa{k,g,i} } 
														{ \summy{i=A}{Z} \; w\pa{k,g,i} }  & \text{sinon}
														\end{array} \right.
		\end{eqnarray}

Finalement~:

		\begin{eqnarray}
		\widehat{c\pa{k,g}} = \underset{A \infegal i \infegal Z } { \arg \max } \; Cr\pa{k,g,i}
		\end{eqnarray}
		
Par définition, $Cr\pa{k,g,i} \in \cro{0,1}$ et quel que soit l'élément $g \in B_a$ de la base d'apprentissage et quelle que soit $k$ la taille du voisinage choisie, $\widehat{c\pa{k,g}} = c\pa{g}$. On définit le taux $t\pa{k,B}$ de bonne classification pour une base $B$ contenant $N$ éléments~:

		\begin{eqnarray}
		t\pa{k,B} = \frac{1}{N} \; \summyone{g \in B} \, \indicatrice{ \widehat{c\pa{k,g}} = c\pa{g} }
		\end{eqnarray}

D'après ce qui précède, $\forall k > 0$, $t\pa{k,B_a} = 1$. Pour une valeur de $k$ donnée, le meilleur jeu de caractéristiques est celui qui maximise le taux $t\pa{k,B_t}$. Dans la pratique, plusieurs valeurs de $k$ seront utilisées. La table~\ref{reco_carac_distance_assoc} rappelle les différents jeux de caractéristiques ainsi que les paramètres qui permettent de les construire.



		\begin{table}[ht]
		$$
		\begin{tabular}{|l|l|l|l|} \hline
		\textbf{Caractéristiques}  &		\textbf{distance} & \textbf{paramètres} & 
																			\textbf{notations} \\ \hline
		\begin{tabular}{l} matrice \\ $\S$ \ref{reco_graphem_matrice} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de divisions horizontales, \\	
							nombre de divisions verticales
							\end{tabular}	
							&
							$Mat\pa{x,y}$
							\\ \hline
		\begin{tabular}{l} profils \\ $\S$ \ref{reco_graphem_histo} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de divisions horizontales, \\	
							nombre de divisions verticales
							\end{tabular}	
							&
							$Prof\pa{x,y}$
							\\ \hline
		\begin{tabular}{l} carte de Kohonen \\ $\S$ \ref{reco_point_caracteristique_kohonen} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de points sur l'axe des $x$, \\	
							nombre de points sur l'axe des $y$ 
							\end{tabular}	
							&
							$Koho\pa{x,y}$
							\\ \hline
		\begin{tabular}{l} chaîne de Markov \\ (contour)	$\S$ \ref{reco_point_caracteristique_contour} 
													\end{tabular} &  
							%(\ref{reco_dist_contour_markov}) page~\pageref{reco_dist_contour_markov} & 
							euclidienne &
							\begin{tabular}{l}
							ordre de la chaîne de \\
							Markov (1,2), connexité (4,8)
							\end{tabular}	
							&
							$Mark\pa{d,c}$
							\\ \hline
		\begin{tabular}{l} contour	(séquence) \\ $\S$ \ref{reco_graphem_contour} \end{tabular} & 
							édition (\ref{reco_distance_contour_min}) & 
							\begin{tabular}{l}
							connexité (4,8)	
							\end{tabular}	
							&
							$Cont\pa{c}$
							\\ \hline
		\begin{tabular}{l} moments invariants \\ $\S$ \ref{reco_moment_invariant_par} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de moments \\
							invariants de Hu
							\end{tabular}	
							&
							$Hu\pa{c}$
							\\ \hline
		\begin{tabular}{l} profils polaires \\ $\S$ \ref{reco_profil_polair} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de divisions de \\
							l'intervalle $\cro{0,2\pi}$	
							\end{tabular}	
							&
							$Pol\pa{c}$
							\\ \hline
		\begin{tabular}{l} moments de Zernike \\ $\S$ \ref{reco_feature_moment_zernike} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de moments invariants \\
							de Zernike
							\end{tabular}	
							&
							$Zer\pa{c}$
							\\ \hline
		\begin{tabular}{l} moments avec ondelettes \\ $\S$ \ref{reco_feature_moment_ondelette_par} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de moments invariants \\
							à partir d'ondelette
							\end{tabular}	
							&
							$Ond\pa{c}$
							\\ \hline
		\begin{tabular}{l} descripteurs de Fourier \\ $\S$ \ref{reco_feature_fourier_contour} \end{tabular} & 
							euclidienne & 
							\begin{tabular}{l}
							nombre de coefficients \\
							de Fourier
							\end{tabular}	
							&
							$Fou\pa{c}$
							\\ \hline
		\end{tabular}
		$$
		\caption{	Caractéristiques, distances associées, paramètres relatifs à chaque description de graphèmes,
							et notations utilisées lors de la présentation des résultats de la sélection
							du meilleur jeu de caractéristiques.}
		\label{reco_carac_distance_assoc}
		\end{table}


La table~\ref{reco_carac_distance_assoc} précise la distance utilisée pour chaque jeu de caractéristiques. Il n'est pas toujours évident que toutes les dimensions doivent intervenir à poids égal dans le calcul de la distance comme c'est le cas pour une distance euclidienne. Les jeux de caractéristiques faisant intervenir des moments sont renormalisés de sorte que tous les moments aient la même amplitude. L'article \citeindex{Waard1995}\seeannex{classification_graphem_carac_dist}{distance optimisée} propose une méthode permettant d'apprendre le poids de chaque dimension dans le but d'optimiser les performances en classification. Cette méthode pourrait être employée comme un prolongement de celle présentée dans ce paragraphe.


		\begin{table}[ht]
		$$
		\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
		A	& 5,3 \% 	& 	H	& 0,2 \% & 	O	& 5,5 \% & 	V	& 5,0 \% \\ \hline 
		B	& 5,5 \% 	& 	I	& 0,4 \% & 	P	& 0,7 \% & 	W	& 5,7 \% \\ \hline 
		C	& 5,5 \% 	& 	J	& 5,5 \% & 	Q	& 0,1 \% & 	X	& 5,8 \% \\ \hline 
		D	& 5,3 \% 	& 	K	& 0,2 \% & 	R	& 5,4 \% & 	Y	& 8,7 \% \\ \hline 
		E	& 5,6 \% 	& 	L	& 5,9 \% & 	S	& 0,4 \% & 	Z	& 5,6 \% \\ \hline 
		F	& 5,6 \% 	& 	M	& 5,5 \% & 	T	& 0,5 \% & 		&				\\ \hline 	
		G	& 0,24 \% & 	N	& 5,6 \% & 	U	& 0,2 \% & 		&				\\ \hline 
		\end{tabular}
		$$
		\caption{ Fréquence des caractères dans une base d'apprentissage qui en contient 10000. Cette répartition
							est identique à celle de la base de test.}
		\label{reco_feature_rep}
		\end{table}





		\begin{table}[ht]
		$$
		\begin{tabular}{|l|c|c||l|c|c|} \hline
		\textbf{jeu} 	& \textbf{$k=3$} & \textbf{$k=10$}   & \textbf{jeu} 	& \textbf{$k=3$} & \textbf{$k=10$} 	\\ \hline
		$Prof\pa{3,3}$  	&	92,5 \%	& 92,6 \%  &	$Pol\pa{10}$		&	82,3 \% &	82,9 \%	\\ \hline
		$Prof\pa{5,5}$  	&	92,2 \%	& 92,1 \%  &	$Pol\pa{20}$		&	90,1 \%	&	90,0 \%	\\ \hline
		$Prof\pa{7,7}$  	&	90,9 \%	& 90,3 \%  &	$Pol\pa{30}$		&	89,5 \%	&	89,3 \%	\\ \hline
		$Mat\pa{3,3}$  		&	91,3 \%	& 91,3 \%  &	$Pol\pa{50}$		&	90,7 \%	&	90,4 \%	\\ \hline
		$Mat\pa{5,5}$  		&	96,6 \%	& 96,0 \%  &	$BPol\pa{20}$		&	93,2 \%	&	92,8 \%	\\ \hline
		$Mat\pa{7,7}$  		&	97,1 \%	& 96,4 \%  &	$BPol\pa{30}$		&	92,8 \%	&	92,3 \%	\\ \hline
		$Koho\pa{3,3}$  	&	93,4 \%	& 92,7 \%  &	$BPol\pa{50}$		&	93,6 \%	&	93,2 \%	\\ \hline
		$Koho\pa{5,5}$  	&	95,2 \%	& 94,3 \%  &	$Ond\pa{10}$		&	64,3 \%	&	67,0 \%	\\ \hline
		$Zer\pa{30}$			& 54,4 \% &	59,6 \%  &	$Ond\pa{20}$		&	74,5 \%	&	76,0 \%	\\ \hline
		$Hu\pa{30}$				& 44,7 \% &	49,2 \%	 &	$Ond\pa{30}$		&	79,5 \%	&	80,3 \%	\\ \hline
		$Fou\pa{30}$			& 19,3 \% & 20,0 \%  &  $Mark\pa{1,8}$ 	&   -     & 56,3 \% \\ \hline
		$Cont\pa{30}$			&   -     & 97,1 \%  &  $Mark\pa{2,4}$ 	&   -     & 62,5 \% \\ \hline
		\end{tabular}
		$$
		\caption{ Taux de bonne classification $t\pa{k,B_t}$ sur la base de test $B_t$ pour différents jeux
							de caractéristiques et différentes tailles de voisinages. Ces résultats ont été obtenus 
							sur une base d'apprentissage et une base de test de 10000 lettres majuscules.}
		\label{reco_feature_selec_choice1}
		%\label{reco_feature_selec_choice2}
		\end{table}







\indexfr{LAESA'}

Afin d'améliorer la rapidité de l'algorithme de recherche des plus proches voisins, la recherche des plus proches voisins est effectuée grâce à l'algorithme LAESA'\seeannex{space_metric_laesa_laesa}{LAESA'}. Le tableau~\ref{reco_feature_rep} montre la fréquence des caractères utilisés dans la base d'apprentissage, le tableau~\ref{reco_feature_selec_choice1} liste les résultats obtenus par cette méthode de classification pour différents jeux de caractéristiques.


Ces résultats montrent dans l'ensemble que les caractéristiques basées sur des moments (Zernike, Li, Fourier) n'obtiennent pas de bons résultats excepté pour les moments basés sur des ondelettes. Aucun de ces jeux ne rivalise avec des caractéristiques de type "géométrique" telles que les jeux $Mat$, $Prof$, $Cont$, $Pol$, $BPol$, $Koho$. Les meilleures caractéristiques semblent être celles du jeu $Mat\pa{7,7}$ avec plus de $97~\%$ de bonne classification. Le nombre de voisins ne semble pas avoir beaucoup d'influence, l'écart dépend du type de caractéristiques choisi et n'est pas significatif dans la plupart des cas. Les meilleurs jeux de caractéristiques sont de types $Mat$, $Prof$, $Pol$, $BPol$. Ces résultats sont similaires à ceux déduits de l'expérience sur la base MNIST\footnote{base d'images accessible depuis le site	\textit{http://yann.lecun.com/exdb/mnist/}.}.

Il faut noter que le seul jeu non vectoriel $Cont$ permet d'obtenir également de bonnes performances mais le temps de classification reste rédhibitoire (plusieurs secondes par mot) et ne permet d'envisager leur utilisation. Toutefois, il apparaît que l'utilisation du contour "brut" est préférable aux autres descriptions $BPol$ et $Fou$. Ce résultat suggère que, pour peu qu'on sache constuire une distance entre images, elle serait préférable à toute distance calculée sur des caractéristiques extraites de ces images.

		\begin{table}[ht]
		$$
		\begin{tabular}{|l|c|c||l|c|c|} \hline
		\textbf{jeu} 	& \textbf{$k=3$} & \textbf{$k=10$}   & \textbf{jeu} 	& \textbf{$k=3$} & \textbf{$k=10$} 	\\ \hline
		$Prof\pa{3,3}$  	&	85,1 \%	& 86,7 \%  &	$Pol\pa{10}$	&	78,7 \% &	79,4 \%	\\ \hline
		$Prof\pa{5,5}$  	&	89,7 \%	& 90,5 \%  &	$Pol\pa{20}$	&	86,3 \%	&	86,6 \%	\\ \hline
		$Prof\pa{7,7}$  	&	90,9 \%	& 91,7 \%  &	$Pol\pa{30}$	&	85,2 \%	&	85,8 \%	\\ \hline
		$Mat\pa{3,3}$  		&	67,3 \%	& 70,9 \%  &	$Pol\pa{50}$	&	87,0 \%	&	87,5 \%	\\ \hline
		$Mat\pa{5,5}$  		&	86,0 \%	& 87,2 \%  &	$BPol\pa{20}$	&	88,4 \%	&	88,6 \%	\\ \hline
		$Mat\pa{7,7}$  		&	94,8 \%	& 94,9 \%  &	$BPol\pa{30}$	&	87,8 \%	&	87,8 \%	\\ \hline
		$Koho\pa{3,3}$  	&	85,3 \%	& 92,7 \%  &	$BPol\pa{50}$	&	89,3 \%	&	89,3 \%	\\ \hline
		$Koho\pa{5,5}$  	&	92,0 \%	& 92,4 \%  &	$Ond\pa{10}$	&	55,1 \%	&	59,9 \%	\\ \hline
		$Zer\pa{30}$			& 45,8 \% &					 &	$Ond\pa{20}$	&	60,3 \%	&	63,0 \%	\\ \hline
		$Hu\pa{30}$				& 45,7 \% &					 &	$Ond\pa{30}$	&	66,9 \%	&	68,9 \%	\\ \hline
		$Fou\pa{30}$			& 43,2 \% &          &                &         &         \\ \hline
		\end{tabular}
		$$
		\caption{ Même expérience effectuée sur la base MNIST contenant 60000 images de chiffres 28x28 pixels
							pour la base d'apprentissage et 10000 images pour la base de test.}
		\label{reco_feature_selec_mnist}
		\end{table}
							







\indexfr{classifiabilité}

Aucun prétraitement n'a été effectué sur ces bases d'apprentissage utilisées pour définir le voisinage d'une image. Or, parmi ces images, on peut se demander quel sous-ensemble est le plus à même de mener à la meilleure classification. Ce choix du meilleur sous-ensemble est relié à la définition de classifiabilité abordée dans l'article \citeindex{Dong2003}. Cette méthode permettrait d'affiner les résultats présentés dans les tableaux~\ref{reco_feature_selec_choice1} et~\ref{reco_feature_selec_mnist} voire de rendre possible l'utilisation de classifieurs basés sur des plus proches voisins puisque ceux-ci ont le désavantage d'être coûteux en espace de stockage et en temps lorsqu'il faut déterminer le voisinage d'un point dans un ensemble de plusieurs milliers d'éléments.


\indexfr{Davies-Bouldin}
\indexfrr{classifieur}{discret}
\indexfrr{classifieur}{continu}

Afin de pouvoir utiliser ces graphèmes à partir de modèles de Markov cachés au travers d'un réseau de neurones, il est nécessaire de les classer. La figure~\ref{reco_classification_grapheme_nb_cl} montre l'évolution de critère de Davies-Bouldin\seeannex{classification_selection_nb_classe_bouldin}{classification} (voir~\citeindex{Davies1979}) en fonction du nombre de classes. Le minimum est obtenu pour 22 classes, toutefois, une dizaine d'autres minima locaux sont assez proches de la valeur obtenue pour le minimum global de la courbe. Le nombre de classes de graphèmes n'est donc pas a priori un paramètre qu'il soit possible de déterminer, il agit simplement sur le nombre de coefficients des modèles.





				\begin{figure}[ht]
				$$\begin{tabular}{|c|} \hline
    		\includegraphics[height=3cm, width=5cm]{\filext{../reco/image/grclnb}} \\ \hline
    		\end{tabular}$$
    		\caption{	Critère de Davies-Bouldin en fonction du nombre de classes pour les caractéristiques
    					des graphèmes. Cette courbe a été obtenue sur une base d'apprentissage de 400000 graphèmes,
    					une base de test de 130000 images. Les caractéristiques utilisées sont celles notées 
    					$Prof\pa{5,5}$ (voir table~\ref{reco_feature_selec_choice1}).}
    		\label{reco_classification_grapheme_nb_cl}
    		\end{figure}


\begin{xremark}{sensibilité au bruit}
Il peut être intéressant d'étudier aussi la sensibilité au bruit de différentes caractéristiques. On peut se demander quelle serait la dégradation du taux de reconnaissance en fonction de l'amplitude d'un bruit artificiellement ajouté à l'image. \indexfr{sensibilité au bruit} Cette idée permettrait de séparer deux jeux de caractéristiques aux performances équivalentes.
\end{xremark}




\subsection{Sélection des caractéristiques des accents}
\indexfrr{accent}{description}
\indexfrr{caractéristiques}{accent}
\label{reco_sel_feat_acc}

L'écriture d'un mot peut contenir diverses éléments surplombant les lettres comme les accents, les points, les barres de~"T" et sont souvent décalés par rapport à leur lettre d'appartenance (voir figure~\ref{reco_lettre_accent_decale}).

		\begin{figure}[ht]
    $$\frame{$\begin{array}[c|c]{c}\includegraphics[height=1cm, width=3cm]
    {\filext{../image/image/lahs_black}}\end{array}$}$$
    \caption{	L'accent de la lettre "E" est décalé et surplombe la lettre "N".}
    \label{reco_lettre_accent_decale}
		\end{figure}

Il est possible d'inclure les particules aériennes au graphème qu'elles surplombent mais comme le montre la figure~\ref{reco_lettre_accent_decale}, ceci peut mener à un résultat erronné. Il est possible également de les considérer comme des graphèmes à part entière et de les insérer dans la séquence de graphèmes, la figure~\ref{reco_lettre_accent_decale} montre encore que l'ordonnancement n'est pas évident. Une dernière solution est de juxtaposer la description des accents à celles des graphèmes comme le montre la figure~\ref{reco_lettre_accent_feature}.


		\begin{figure}[ht]
    $$\frame{$\begin{array}[c|c]{c}\includegraphics[height=2.5cm, width=2cm]
    {\filext{../reco/image/feat_acc}}\end{array}$}$$
    \caption{	La description de la lettre "E" est ajoutée aux descriptions des graphèmes avoisinants.}
    \label{reco_lettre_accent_feature}
		\end{figure}
		
\indexfr{particules aériennes}
\indexfr{Davies-Bouldin}

Toutefois, ce procédé a comme inconvénient d'accroître la taille du vecteur de caractéristiques associé à chaque graphème. Etant donné que la variabilité des particules aériennes est moindre que celle des graphèmes, celles-ci seront préalablement classées de manière à convertir un large vecteur de caractéristiques en un vecteur de moindre dimension (moins de cinq). Le nombre de classes obtenu à partir d'une classification non supervisée est déterminé par le critère de Davies-Bouldin\seeannex{classification_selection_nb_classe_bouldin}{classification} (voir~\citeindex{Davies1979}). La figure~\ref{reco_accent_sel_class} montre la courbe obtenue. 



		\begin{figure}[ht]
    $$\begin{tabular}{|c|} \hline \\
    \includegraphics[height=4cm, width=4cm]{\filext{../reco/image/accdb}}
    \\ \hline \end{tabular}$$
    \caption{	Critère de Davies-Bouldin en fonction du nombre de classes pour les caractéristiques
    					des particules aériennes. Cette courbe a été obtenue sur une base d'apprentissage de 24000 accents,
    					une base de test de 8000 images. Les caractéristiques utilisées sont celles notées 
    					$Prof\pa{3,3}$ (voir table~\ref{reco_feature_selec_choice1}).}
    \label{reco_accent_sel_class}
		\end{figure}

Le minimum est atteint pour trois classes. Le résultat atteint pour le jeu de caractéristiques $Prof\pa{3,3}$ l'est aussi pour le jeu $Mat\pa{3,3}$. La table~\ref{reco_feature_selec_choice1} montre qu'une trop grande division de l'image nuit à la reconnaissance, c'est pourquoi les dimensions $\pa{3,3}$ ont été choisies comme jeu de caractéristiques pour ces imagettes composées le plus souvent d'un simple trait.

Il reste à mettre en \oe uvre la relation de voisinage. Un vecteur de caractéristiques $V$ associé à un graphème comporte deux parties $V = \pa{G,A}$ où $G$ est la partie propre au graphème et $A$ celle dédiée aux accents. On note $M$ la suite $M = \pa{m_i}_ { i \in \mathbb{Z}}$ définie par~:

		\begin{eqnarray}
		m_i		&=&		\frac{2^{2-\abs{i}}}{10}  \; \indicatrice{ \abs{i} \infegal 2} \label{reco_accent_add_graphem_1}
		\end{eqnarray}

Cette suite vérifie $\summyone{i} m_i = 1$. On considère que chaque accent $a$ est associé au graphème le plus proche $\sigma\pa{a}$ et a pour vecteur de probabilités de classification $P_a$. Pour un graphème d'indice $i$, $A_i$ la partie décrivant les graphèmes est définie comme suit~:

		\begin{eqnarray}
		A_i &=&  \summyone{a} \; m_{ i - \sigma\pa{a} } \, P_a  \label{reco_accent_add_graphem_2}
		\end{eqnarray}
		








\subsection{Sélection des caractéristiques des liaisons entre graphèmes}
\label{reco_sel_feat_link}

\indexfrr{liaison}{description}
\indexfrr{caractéristiques}{liaison}
\indexfr{Davies-Bouldin}

Dans la modélisation qui suit (voir paragraphe~\ref{reco_modele_presentation_1}), les liaisons entre graphèmes doivent elles-aussi être décrites par un vecteur de caractéristiques. Aucune annotation n'est disponible pour ces liaisons, de plus, afin d'éviter l'utilisation de modèles trop lents en terme de temps de calcul, la description de ces liaisons doit être courte. Le tableau~\ref{reco_feature_selec_choice_link} donne la liste des caractéristiques choisies. Soient deux graphèmes $G_1$ et $G_2$, on note leurs boîtes englobantes $r_i = \pa{ \pa{x_1^i,y_1^i}, \pa{x_2^i,y_2^i} }$ et $r = r^1 \bigcup r^2 = \pa{ \pa{x_1,y_1} = \pa{ \min\acc{x_1^1,x_1^2}, \min\acc{y_1^1,y_1^2}} , \pa{x_2,y_2} = \pa{ \max\acc{x_1^1,x_1^2}, \max\acc{y_1^1,y_1^2}} }$. $S\pa{r}$ désigne la surface d'un rectangle. Par convention, la première transition entre graphème est celle qui précède le premier graphème, dans ce cas, on note $G_1 = \emptyset$.



		\begin{table}[ht]
		$$\begin{tabular}{|l|l|c|c|} \hline
		\textbf{indice} 	& \textbf{sens}			& \textbf{expression} 
											& \textbf{valeur par défaut ($G_1 = \emptyset$)}\\  \hline
		$0$  							& rapport des surfaces 		& $ \frac{ S\pa{r^1} + S\pa{r^2}}{ S\pa{r} } $ 
																								& $1$			\\ \hline
		$1$  							& rapport des longueurs		& $ \frac{ x_2^1 - x_1^1 }{ x_2 - x_1 } $ 
																								& $0$			\\ \hline
		$2$  							& rapport des longueurs		& $ \frac{ x_2^2 - x_1^2 }{ x_2 - x_1 } $ 
																								& $1$			\\ \hline
		$3$  							& rapport des hauteurs		& $ \frac{ y_2^1 - y_1^1 }{ y_2 - y_1 } $ 
																								& $0$			\\ \hline
		$4$  							& rapport des hauteurs		& $ \frac{ y_2^2 - y_1^2 }{ y_2 - y_1 } $ 
																								& $1$			\\ \hline
		$5$  							& positions relatives  		& $ \frac{ x_1^1 - x_1^2 }{ x_2 - x_1 } $ 
																								& $1$			\\ \hline
		$6$  							& positions relatives  		& $ \frac{ y_1^1 - y_1^2 }{ y_2 - y_1 } $ 
																								& $0$			\\ \hline
		\end{tabular}$$
		\caption{ Caractéristiques utilisées pour décrire une liaison (ou transition) entre graphèmes.}
		\label{reco_feature_selec_choice_link}
		\end{table}


Une fois ces caractéristiques désignées, étant donné que les modèles de reconnaissance ont besoin de vecteurs de probabilités, on effectue une classification non supervisée de cet ensemble comme au paragraphe précédent. La figure~\ref{reco_link_feature_sel_class} illustre l'évolution du critère de Davies-Bouldin permettant de choisir le nombre de classes approprié puis la figure~\ref{reco_link_feature_sel_class_example} donne quelques exemples pour chaque classe ainsi déterminée.


		\begin{figure}[ht]
    $$\begin{tabular}{|c|c|} \hline
    \begin{tabular}{l}
    \includegraphics[height=3cm, width=4cm]{\filext{../reco/image/linkdb}} \\
    \begin{small} \begin{tabular}{ll}
    nb cl  	& : nombre de classes \\
    DB  		& : critère de Davies-Bouldin \\
    app  		& : base d'apprentissage \\
    test   	& : base de test 
    \end{tabular} \end{small}
    \end{tabular}
    & 
    \begin{small} \begin{tabular}{c|cc|c|cc} 
    nb cl & DB app & DB test & nb cl & DB app & DB test \\ \hline
		2	 & 0,35   &	 0,35  & 	9	 &	 1,41  & 	 1,40   \\
		3	 & 0,95   &	 0,94  & 	10 &	 1,40  & 	 1,19   \\
		4	 & 0,92   &	 0,93  & 	11 &	 1,20  & 	 1,19   \\
		5	 & 1,25   &	 1,24  & 	12 &	 1,35  & 	 1,35   \\
		6	 & 1,46   &	 1,45  & 	13 &	 1,22  & 	 1,22   \\
		7	 & 1,69   &	 1,68  & 	14 &	 1,20  & 	 1,20   \\
		8	 & 1,60   &	 1,60  & 	15 & 	 1,23  & 	 1,23   
		\end{tabular} \end{small}
    \\ \hline \end{tabular}$$
    \caption{	Critère de Davies-Bouldin en fonction du nombre de classes pour les caractéristiques
    					décrivant les liaisons entre graphèmes. Le minimum est atteint pour deux classes avec un 
    					critère de $0,35$. Le second minimum est atteint pour quatre classes
    					avec un critère égal à $0,92$. Il n'y a pas de différence sensible entre les bases 
    					d'apprentissage et de test.}
    \label{reco_link_feature_sel_class}
		\end{figure}

\indexfr{IOHMM}

Deux classes permettent d'obtenir un critère de Davies-Bouldin minimal d'après la courbe~\ref{reco_link_feature_sel_class}. Toutefois, les deux classes ainsi déterminées isolent les liaisons de début de mots des autres. Ce résultat est attendu puisque les liaisons de début de mot sont toutes égales. Par conséquent, cette valeur ne sera pas prise en compte et le bon nombre de classes correspond au second minimum global atteint pour quatre classes. De plus, ce faible nombre de classes convient aux modèles IOHMM développés plus loin (voir paragraphe~\ref{reco_liaison_grapheme}) car ils évitent un trop grand nombre de coefficients.


		\begin{figure}[ht]
    $$\begin{tabular}{|cc|cc|cc|cc|} \hline
    \begin{tabular}{c}
    	\includegraphics[height=2.5cm, width=2cm]{\filext{../reco/image/linkcl0}} 
    	\\ 1 \\ $O \longrightarrow o$
    	\end{tabular} 
    &
 		%\begin{tabular}{c} 0,83 \\ 0,66 \\ 0,52 \\ 0,93 \\ 0,46 \\ 0,46 \\ 0,43 \end{tabular} 
 		&
    \begin{tabular}{c}
	    \includegraphics[height=2.5cm, width=2cm]{\filext{../reco/image/linkcl1}} 
    	\\ 2 \\ $\begin{subarray}{c} o \longrightarrow o \\ O \longrightarrow O \end{subarray}$
    	\end{tabular} 
    &
    %\begin{tabular}{c} 0,99 \\ 0,56 \\ 0,56 \\ 0,90 \\ 0,89 \\ 0,44 \\ 0,01 \end{tabular} 
    &
    \begin{tabular}{c}
	    \includegraphics[height=2.5cm, width=2cm]{\filext{../reco/image/linkcl2}} 
    	\\ 3 \\ $ \longrightarrow \begin{subarray}{c} o \\ O \end{subarray} $
    	\end{tabular} 
    &
    %\begin{tabular}{c} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \end{tabular} 
    &
    \begin{tabular}{c}
		  \includegraphics[height=2.5cm, width=2cm]{\filext{../reco/image/linkcl3}} 
    	\\ 4 \\ $o \longrightarrow O$
    	\end{tabular} 
    &
    %		\begin{tabular}{c} 0,82 \\ 0,49 \\ 0,64 \\ 0,48 \\ 0,93 \\ 0,34 \\ -0,38 \end{tabular} 
    \\ \hline
    \end{tabular}$$
    \caption{	Chaque classe de liaisons entre graphèmes est illustrée par ses représentants les plus probables.
    					$o$ et $O$ représentent respectivement des graphèmes de petite et grande taille. La troisième classe
    					est affectée aux transitions commençant un mot.	}
    \label{reco_link_feature_sel_class_example}
		\end{figure}

Le résultat de la figure~\ref{reco_link_feature_sel_class_example} montre quatre classes et, bien que la classification soit non supervisée, il est cependant possible d'interpréter les regroupements effectués. La seconde classe regroupe les transitions entre graphèmes de même importance. La première classe regroupe les transitions entre un grand et un petit graphème, la quatrième, la transition opposée. Quant à la troisième classe, elle regroupe les débuts de mot. 

Il est cependant difficile d'évaluer la pertinence des caractéristiques choisies puisque les liaisons ne sont pas identifiables comme le sont les caractères. La classification non supervisée obtenue ci-dessus mène à une configuration qui n'est pas incohérente puisqu'elle aboutit à un résultat interprétable. Néanmoins, il serait préférable d'évaluer les performances en reconnaissance pour différents jeux de caractéristiques en utilisant par exemple la méthode présentée au paragraphe~\ref{reco_reco_knn_sequence} qui se contente de mesurer l'apport de cette séquence de liaisons.











\subsection{Amélioration de la base d'apprentissage}
\label{reco_base_app}


Les tests de classification utilisés pour sélectionner le meilleur jeu de caractéristiques ont été effectués sur une base de données dans laquelle les classes ne sont pas équitablement distribuées. Il y a peu d'exemples pour les lettres rares comme la lettre "W". L'article \citeindex{Barandela2003} propose de limiter le nombre d'exemples pour les classes sur-représentées. Lorsque la plus petite des classes ne contient que quelques dizaines d'exemples comparés aux milliers que contient la plus grande, cette méthode semble difficilement applicable.

\indexfr{érosion}
\indexfr{dilatation}
\indexfr{inclinaison}

L'autre idée consiste à accroître le nombre d'exemples des classes sous-représentées. La première idée part des images qu'on duplique en y introduisant un bruit comme une érosion, une dilatation de l'image, une rotation de quelques degrés. Il est possible ensuite de bruiter les caractéristiques obtenues pour les rares exemples d'une classe ou d'en fabriquer d'autres à partir de moyennes pondérées.

Soient $\vecteur{Y_1}{Y_N}$ les exemples d'une classes, pour obtenir $N'$ autres exemples, on procède comme suit~:

		\begin{eqnarray}
		\forall i \in \ensemble{1}{N'}, \; Y_i = \summy{k=1}{N} \; \alpha_{ki} \, X_k
		\label{reco_formule_ponderation_exemple}
		\end{eqnarray}

La matrice $\pa{\alpha_{ki}}$ est une matrice aléatoire vérifiant~:
\indexfrr{loi}{uniforme}

		\begin{eqnarray}
		\forall i, \; \summy{k=1}{N} \; \alpha_{ki} = 1 \text{ et }
		\forall k,i, \;  \alpha_{ki} \sim \mathcal{M}\pa{\frac{1}{N},N}
		\end{eqnarray}
		
Pour valider ces hypothèses, un test est construit sur la même base de caractères que celle utilisée au paragraphe~\ref{reco_sel_feat_graph}, la base d'apprentissage servant à construire le système de voisinage est améliorée selon deux méthodes possibles~:

		\begin{enumerate}
		\item Traitement d'image~: multiplication des images par lissage, correction de l'inclinaison, flou 
										(réalisé par l'application d'un filtre (3x3)).
		\item Moyenne~: les exemples ajoutés sont des moyennes pondérées aléatoirement illustrées par la formule 
					(\ref{reco_formule_ponderation_exemple}).
		\end{enumerate}


Les tests en reconnaissance sont comparés avec une base d'apprentissage non traitée. Les résultats sont rassemblées dans la table~\ref{reco_taux_reco_base_amelioree}. L'amélioration que procure la multiplication des images dans la base d'apprentissage par traitement d'image n'est pas significative.


		\begin{table}[ht]
		$$\begin{tabular}{|l|c|c|c|}  \hline
												&  jeux	de						&  taux de 				&	taux de  	\\ 
		méthode							&  caractéristiques		&  reconnaissance	& référence \\ \hline
		traitement d'image	& $Prof\pa{5,5}$			&	 92,16 \%				&	92,07 \%	\\
		moyenne							& $Prof\pa{5,5}$			&	 93,68 \%				&	92,07 \%	\\ \hline
		\end{tabular}$$
		\caption{	Taux de reconnaissance obtenu pour une base d'apprentissage améliorée comparé
							à celui obtenu pour une base d'apprentissage non améliorée. La méthode de classification
							utilise les plus proches voisins (voir paragraphe~\ref{reco_sel_feat_graph}).}
		\label{reco_taux_reco_base_amelioree}							
		\end{table}


\indexfr{rejet}

Cette méthode a également été utilisée afin de pouvoir rejeter une partie des images difficiles à classer. Chaque image est présente à six exemplaires après un lissage, une correction de l'inclinaison, et trois images floues obtenues par l'application successive d'un filtre (3x3). Il est alors possible de vérifier pour une image donnée si la classification de ces six exemplaires est concordante auquel cas la classification pour être considérée comme correcte, ceci mène aux résultats de la table~\ref{reco_taux_reco_base_amelioree_rejet}.


		\begin{table}[ht]
		$$\begin{tabular}{|c|c||c|r||c|c|}  \hline
		jeux	de						&  nombre de    &  taux de 				&	taux de docu-  	& taux de		& taux de docu-	\\ 
		caractéristiques		&  concordances  &  reconnaissance	& ments traités & référence	&	ments traités	\\
		 \hline
		$Prof\pa{5,5}$			&	 3 &  92,16 \%				&	100,00 \% \;\;  & 92,07 \%	& 100 \% 	\\
		$Prof\pa{5,5}$			&	 4 &  93,13 \%				&	98,11 \% 	\;\;	& 92,07 \%	& 100 \% 	\\
		$Prof\pa{5,5}$			&	 5 &  94,17 \%				&	95,92 \% 	\;\;	& 92,07 \%	& 100 \% 	\\
		$Prof\pa{5,5}$			&	 6 &  96,48 \%				&	89,11 \% 	\;\;	& 92,07 \%	& 100 \% 	\\ \hline
		\end{tabular}$$
		\caption{	Taux de reconnaissance et taux de rejet en comparant les résultats de classification obtenus pour six exemplaires
							différents d'une même image modifiée par lissage, rotation, flou.}
		\label{reco_taux_reco_base_amelioree_rejet}							
		\end{table}


L'altération des images ne semble pas avoir un impact suffisamment significatif. La seconde expérience montre néanmoins que l'utilisation de plusieurs versions de la même image permet de rejeter certains cas d'erreurs (table~\ref{reco_taux_reco_base_amelioree_rejet}). En revanche, la création artificielle de jeux de caractéristiques comme moyennes des caractéristiques d'une même classe permet d'augmenter le nombre d'exemples pour des classes sous-représentées et aboutit à une amélioration non négligeable des taux de reconnaissance (table~\ref{reco_taux_reco_base_amelioree}). De plus, cette méthode ne fait qu'accroître la base de données utilisées pour les apprentissages et ne modifie pas la reconnaissance proprement dite. 

Ces méthodes supposent de connaître la classe des images à classer et est de ce fait difficilement applicable pour des images contenant des mots. Lors de la reconnaissance, ceux-ci sont segmentés en graphèmes qui ne correspondent pas forcément à des lettres. Néanmoins, si chaque image de mot était segmentée en lettres, il serait alors possible d'augmenter les exemples pour les lettres sous-représentées.




















\subsection{Construction de réseaux de neurones classifieurs}
\indexfrr{classification}{caractéristiques}
\indexfrr{caractéristiques}{classification}
\indexfrr{réseau de neurones}{classifieur}
\indexfr{centres mobiles}

Dans chacun des trois cas présentés ci-dessus (paragraphe~\ref{reco_sel_feat_graph} à~\ref{reco_sel_feat_link}), le résultat obtenu est un système de classification non supervisée basé sur des centres mobiles. Celui-ci va être converti dans chacun des cas en un réseau de neurones classifieur\seeannex{subsection_classifieur}{réseau de neurones} appris pour retourner le même résultat. L'apprentissage du réseau de neurones est plus adapté au système de reconnaissance qui sera construit. Ceux-ci pourront alors entraîner ce réseau de neurones de sorte que sa classification améliore la reconnaissance. Par conséquent, après maintes itérations d'apprentissage de ce réseau de neurones classifieur, il est possible que la classification qu'il effectue alors diverge de celle obtenue initialement.

\indexfr{classe sous-représentée}

Les réseaux de neurones supportent mal les classes sous-représentées, par conséquent, l'apprentissage est effectué sur une base d'apprentissage pour laquelle les exemples sont multipliés artificiellement (comme au paragraphe~\ref{reco_base_app}). Trois réseaux de neurones sont ainsi obtenus, le premier classe les graphèmes, le second les accents, le dernier les liaisons entre graphèmes. Le résultat de la classification des accents sera ajouté au vecteur de caractéristiques des graphèmes selon les expressions~(\ref{reco_accent_add_graphem_1}) et~(\ref{reco_accent_add_graphem_2}). A chaque image correspond donc deux séquences de caractéristiques illustrées par la figure~\ref{reco_deux_sequence_liaison_grapheme}.

		



























\subsection{Valeurs aberrantes}
\indexfr{valeur aberrante}

\label{reco_densite_valeur_aberrante}

La segmentation en graphèmes, appliquée à des images de mauvaise qualité ou une écriture peu lisible, produit souvent des résultats de mauvaise qualité (figure~\ref{reco_bad_grapheme_aber}). Peu courants, ces graphèmes introduisent un bruit non négligeable qui sera appris par les modèles de reconnaissance. La lettre "e", très courante, n'en sera pas affectée mais la lettre "w", peu représentée en langue française, présente à peine plus d'une centaine de fois dans les bases d'apprentissage, pourrait être mal reconnue par la suite.

				\begin{figure}[ht]
				$$\begin{array}{|c|} \hline
    		\includegraphics[height=2cm, width=5cm]{\filext{../reco/image/bad_gr}} \\ \hline
    		\end{array}$$
    		\caption{	Graphèmes de mauvaise qualité, peu lisibles. Hors contexte, 
    							pour la plupart, ils ne semblent faire partie	d'aucune lettre.}
    		\label{reco_bad_grapheme_aber}
    		\end{figure}

\indexfr{lisible}
\indexfr{qualité de l'écriture}

L'autre objectif est par la suite de pouvoir construire un estimateur de qualité de l'écriture. Plus un mot est lisible, plus sa reconnaissance est susceptible de retourner un résultat correct. Si tous les graphèmes sont lisibles, alors le mot a toutes les chances de l'être aussi. La lisibilité est une notion subjective et difficile à transcrire en termes mathématiques. Toutefois, on peut supposer qu'un graphème est lisible si beaucoup d'autres lui ressemblent, donc si de nombreux autres graphèmes sont proches dans l'espace qui a été choisi pour les représenter. Un estimateur à noyau de la densité permettra de caractériser cette proximité. 

\indexfrr{densité}{noyau}
\indexfr{estimateur à noyau}
\indexfr{noyau}
\indexfr{i.i.d.}

Les résultats présentés sont inspirés du livre \citeindex{Silverman1986} et de l'article \citeindex{Herbin2001}\seeannex{classification_herbin_noyau}{densité, noyau}. L'estimateur à noyau choisi est basé sur un noyau gaussien multidimensionnel et pour un échantillon de variables i.i.d. (identiquement et indépendamment distribuées) $\vecteur{X_1}{X_N} \in \pa{\R^d}^N$, on a~:


			\begin{eqnarray}
			\hat{f}_H\pa{x} 		&=& \dfrac{1}{N} \; \summy{i=1}{N} \; \dfrac{1}{\det H} \; K\pa{ H^{-1} \pa{x - X_i}} 
			\;\; \text{où} \;\; 
			K\pa{x} 	= \dfrac{1}{ \pa{2 \pi}^{ \frac{d}{2}} } \; e^{ - \frac{ \norme{x}^2 } {2} } % \nonumber
									\label{reco_density_non_param} 
			\end{eqnarray}

\indexfr{Silverman}

L'estimation dépend de la matrice $H$ qui peut dépendre localement de $x$ ou non. Etant donné que $N$ est de l'ordre de 200000 et $d$ de l'ordre de quelques dizaines, la matrice $H$ a été restreinte à sa diagonale $H = diag\vecteur{h_1}{h_d}$ et est déterminée selon la règle de Silverman~:


			\begin{eqnarray}
			\forall j \in \intervalle{1}{d}, \; \hat{h_j} &=&  \hat{\sigma}_j \; N^{-\frac{1}{d+4}}
					\label{reco_silverman_rule} \\
			\text{avec } \hat{\sigma}_j &=& \sqrt{\mathbb{V}X_j} \nonumber\\
			\text{où } X_j & \text{est}& \text{la variable aléatoire réelle correspondant à la } 
																		j^\text{ième} \text{ coordonnée} \nonumber
			\end{eqnarray}
 
\indexfr{quantile}

Afin d'éviter les graphèmes les moins fréquents, la suite $\vecteur{X_{\sigma\pa{1}}}{X_{\sigma\pa{N}}}$ est ordonnée de telle sorte que $ \hat{f}_H\pa{X_{\sigma\pa{1}}} < \dots < \hat{f}_H\pa{X_{\sigma\pa{N}}} $. Pour $\alpha \in \cro{0,1}$, on définit $q_\alpha = \hat{f}_H\pa{X_{\sigma\pa{\cro{\alpha N}}}}$. Pour un graphème quelconque $x$, on peut supposer que si $\hat{f}_H\pa{x} \infegal q_{\,5\%}$, alors le graphème est plutôt mal écrit car il fait partie de l'ensemble des 5\% de graphèmes les moins probables. La figure~\ref{reco_fig_density_graphem_app_test} montre les courbes $\pa{\alpha, q_\alpha \pa{H_{Sil}} }$ obtenues pour les bases d'apprentissage et de test. Ces deux courbes diffèrent complètement et cela signifie que la règle de Silverman ne convient pas sur ce genre de problème. Par conséquent, la seconde règle adoptée est de trouver un vecteur $H\pa{\gamma}$ colinéaire avec $H_{Sil}$ ($H\pa{gamma} =  \gamma \, H_{Sil}$) de telle sorte que les deux courbes coïncident. La quantité $\Delta\pa{\gamma}$ définie en~(\ref{reco_densite_noyau_delta}) décroît presque constamment lorsque $\gamma$ augmente (voir figure~\ref{reco_fig_density_graphem_choice}) pour devenir quasi-nulle lorsque $\gamma$ est très grand, ce cas correspond à une densité uniforme. Il suffit de choisir la plus petite valeur pour $\gamma$ de sorte que $\Delta\pa{\gamma}$ soit inférieur à un certain seuil.


			\begin{eqnarray}
			\Delta\pa{\gamma} = \summyone{i} \cro { q_{\alpha_i}^{app} \pa{H\pa{\gamma}} - 
								q_{\alpha_i}^{test} \pa{H\pa{\gamma}} }^2
			\label{reco_densite_noyau_delta}
			\end{eqnarray}

			\begin{figure}[ht]
			$$
			\begin{tabular}{|c|c|} \hline
			\includegraphics[height=4cm, width=6cm]{\filext{../reco/image/dens_app}}  &
			\includegraphics[height=4cm, width=6cm]{\filext{../reco/image/dens_tst}}  \\ \hline
			\end{tabular}
			$$
			\caption{	Courbes $\pa{\alpha, q_\alpha^{app}\pa{H_{Sil}} }$ et $\pa{\alpha, q_\alpha^{test}\pa{H_{Sil}} }$ 
								avec $\alpha \in \acc{ \frac{i}{20} \sac 0 \infegal i \infegal 20}$
								obtenues sur les bases d'apprentissage et de test et en 
								appliquant la règle de Silverman. Leur différence de forme suggère
								que la règle de Sileverman (\ref{reco_silverman_rule}) fonctionne mal sur ce type de données. }
			\label{reco_fig_density_graphem_app_test}
			\end{figure}

Cette idée a d'abord été vérifiée sur des données simulées illustrées par la figure~\ref{reco_fig_density_graphem_choice}. L'image~\ref{reco_fig_density_graphem_choice}a montre l'évolution du critère $\Delta\pa{\gamma}$ en fonction de $\gamma$. Dans ce cas, la meilleure valeur pour $\gamma$ est $1$, ce qu'illustre l'image~\ref{reco_fig_density_graphem_choice}b pour un mélange de deux lois normales.



			\begin{figure}[ht]
			$$
			\begin{tabular}{|c|c|} \hline
			\includegraphics[height=6cm, width=6cm]{\filext{../reco/image/dens_del}}  &
			\includegraphics[height=6cm, width=6cm]{\filext{../reco/image/dens_nua}}  
							  \\ 
			$(a)$ & $(b)$  \\ \hline
			\end{tabular}
			$$
			\caption{	La première image représente la courbe $\pa{ \log \gamma, \, \log \Delta\pa{\gamma} }$,
								la seconde courbe représente les quartiles de la variable aléatoire $f_X\pa{X}$ où $f_X$ est
								l'estimateur à noyau de la variable $X$ sur un échantillon de 1000 points simulés selon un 
								mélange de deux lois normales. Pour cet exemple, la valeur choisie pour $\gamma$ est~$1$,
								ce qui revient à appliquer la règle de Silverman.}
			\label{reco_fig_density_graphem_choice}
			\end{figure}


La figure~\ref{reco_fig_density_graphem_choice_cas_reel} illustre les résultats obtenus dans le cas réel. La courbe $\pa{ \log \gamma, \, \log \Delta\pa{\gamma} }$ est quasiment une droite qui permet de calculer une valeur approximative pour $\gamma$ égale à $4,18$. Les résultats obtenus à partir de cette valeur sont donnés par la figure~\ref{reco_fig_density_graphem}.



			\begin{figure}[ht]
			$$\begin{tabular}{|c|c|}  \hline
			\includegraphics[height=3cm, width=3cm]{\filext{../reco/image/dens_res}}  &
					\begin{tabular}{c|c}
					$\log \gamma$ & $ \log \Delta\pa{\gamma}$ \\ \hline
					0,00   & 	 19,05   \\
					0,30   & 	 7,01    \\
					0,70   &	-9,19    \\
					1,00   &	-21,33   
					\end{tabular} \\
			$ \log \gamma \approx -40,4 \cro{ \log \Delta\pa{\gamma}}  + 19,1$ & 
			\\ \hline
			\end{tabular}
			$$
			\caption{	Courbe $\pa{ \log \gamma, \, \log \Delta\pa{\gamma} }$ observée avec des graphèmes décrits par 
								le jeu de caractéristiques $Prof\pa{5,5}$. Pour $\gamma = 4,18$, 
								on trouve $\log \Delta\pa{\gamma} \approx -6$.
								}
			\label{reco_fig_density_graphem_choice_cas_reel}
			\end{figure}
			





			\begin{figure}[ht]
			$$
			\begin{tabular}{|c|} \hline
			\includegraphics[height=6cm, width=6cm]{\filext{../reco/image/densgr10}}  \\ \hline
			\end{tabular}
			$$
			\caption{	Densité obtenue pour les graphèmes suivants, selon le jeu de caractéristiques $Prof\pa{5,5}$
								décrits aux paragraphes~\ref{reco_graphem_matrice} et~\ref{reco_graphem_histo}. Le 
								quatrième graphème possède une densité qui l'inclut dans les 5\% de graphèmes très peu 
								probables, la lettre $a$ est en effet accompagnée d'un morceau de la lettre $y$.}
			\label{reco_fig_density_graphem}
			\end{figure}


\indexfrr{Algorithme}{EM}
\indexfrr{segmentation}{graphème}

L'estimation de la densité permet de révéler les graphèmes très mal segmentés comme le montre la figure~\ref{reco_fig_density_graphem}. A l'inverse, les graphèmes les plus probables sont des traits verticaux comme les barres de la lettre $u$ ou celle de la lettre $i$. Néanmoins cette modélisation est assez coûteuse puisque l'expression (\ref{reco_density_non_param}) nécessite une somme sur un grand nombre de données. Il est possible d'optimiser ce calcul en restreignant la somme uniquement aux voisins les plus proches\seeannex{space_metric_introduction}{recherche des plus proches voisins}. Il est aussi possible de compresser cette information en approximant cette densité par un mélange de lois normales multidimensionnelles. Ces densités sont estimées à partir de l'algorithme EM\seeannex{classification_melange_loi_normale}{mélange de lois normales} (Expectation Maximization, voir \citeindex{Dempster1977}) pour lequel le mélange peut inclure un nombre variable de lois normales. 

Cette densité a dans un premier temps été utilisée afin de sélectionner de manière automatique les graphèmes mal segmentés afin d'ajuster les paramètres de la segmentation en graphèmes présentée au paragraphe~\ref{image_choix_segmentation}. Il a également été envisagé d'intégrer ce critère dans l'élaboration d'un critère de confiance associé aux résultats de la reconnaissance. Cet objectif supposait toutefois d'obtenir cette densité par un autre moyen plus rapide qu'un estimateur à noyau comme un réseau de neurones ou un mélange de lois gaussiennes par exemple. Cette direction nécessite le choix d'un modèle, la sélection du nombre de ses coefficients, des directions de recherche qui n'ont pas encore été développées. 

\indexfrr{densité}{non paramétrique}
\indexfrr{densité}{semi paramétrique}
\indexfrr{densité}{paramétrique}
\label{modification_janvier_2004_new}

La méthode présentée ici s'appuie sur une modélisation non paramétrique de la densité plutôt qu'une modélisation paramétrique comme par exemple un mélange de lois gaussiennes. L'article \citeindex{Hoti2004} présente une modélisation semi-paramétrique\seeannex{classification_modelisation_densite}{densité semi-paramétrique} qui pourrait également être étudiée. La densité du couple $\pa{x,y}$ est alors estimée par~: $f_{x,y}(x,y) = f_{y\sac x}(y) \, f_x(x)$. La densité $f_x\pa{x}$ est estimée de façon non paramétrique tandis que $f_{y|x}\pa{y}$ est modélisée par une loi gaussienne. $x$ et $y$ pourraient par exemple être deux jeux de caractéristiques différents avec $x$ un vecteur de petite dimension (moins d'une dizaine). Plus la dimension du vecteur $x$ est faible, plus le calcul de la densité en un point est rapide.

\indexfrr{directions de recherche}{densité et décision}












%---------------------------------------------------------------------------------------------------------------
\section{Reconnaissance de mots cursifs à l'aide de plus proches voisins}
%---------------------------------------------------------------------------------------------------------------
\label{reco_reco_knn_sequence}
\indexfr{plus proches voisins}
\indexfr{reconnaissance}
\indexfr{classification}

Avant de commencer la description de modèles complexes associant chaînes de Markov et réseau de neurones, un premier système de reconnaissance est implémenté, basé sur un système de classification fondé sur les plus proches voisins. Chaque image est traduite par une séquence d'observations de longueur variable. Pour mettre en \oe uvre cette idée, il suffit de construire une distance entre séquences d'observations.



\subsection{Distance d'édition}
\indexfrr{distance}{édition}
\label{reco_edit_distance_word}

Soient deux images de mots dont sont extraites deux séquences d'observations $O^1 = \vecteur{O^1_1}{O^1_{T_1}}$ et $O^2 = \vecteur{O^2_1}{O^2_{T_2}}$, on cherche à définir une distance $d\pa{O^1,O^2}$ qui traduise la proximité de ces deux mots. Il est possible de construire cette distance comme une distance d'édition\seeannex{edit_distance_annexe}{distance d'édition} où le coût de comparaison entre deux observations est estimé par une distance euclidienne. Il reste donc à déterminer le coût d'insertion ou de suppression d'une observation.

On suppose que $X$ et $Y$ sont deux variables indépendantes de même loi normale $\loinormale{\mu}{\sigma^2}$. La variable $X-Y$ suit une loi normale de paramètre $\loinormale{0}{2 \sigma^2}$. Par conséquent, $\frac{1}{2\sigma^2}\pa{X-Y}^2$ suit une loi $\chi_2$ dont on déduit que~:

			$$
			\pr{ \frac{1}{2\sigma^2}\pa{X-Y}^2 \infegal 0,455 } \sim 0,5
			$$

D'une manière grossière, on suppose que les observations sont des vecteurs dont chaque dimension suit une loi normale et sont indépendantes les unes des autres. Soit $\vecteur{O^1}{O^N}$ l'ensemble des observations de la base d'apprentissage, on désigne par $V_i$ la variance de la i$^{\text{ème}}$ dimension~:

			\begin{eqnarray}
			\forall i, \; V_i = \frac{1}{N} \, \summy{k=1}{N} \pa{O^k_i}^2 - \cro{ \frac{1}{N} \, \summy{k=1}{N} O^k_i}^2
			\end{eqnarray}
			
Le coût d'insertion et de suppression $c$ est défini par~: 

			\begin{eqnarray}		
			c = 0,455 \times 2 \; \summyone{i} V_i \sim \summyone{i} V_i
			\end{eqnarray}

Si les observations vérifient les hypothèses d'indépendance et de normalité -~ce qui est rarement le cas~-, soient deux observations $O^1$ et $O^2$, alors~:

			$$
			\pr{ \norme{O^1 - O^2}^2 \infegal c } \sim 0,5
			$$

La figure~\ref{reco_knn_reco_word_example_edit_distance} donne le calcul des distances pour deux groupes de deux images correspondant au même mot. Dans ce cas, les mots appartenant à la même classe forment un couple de plus proches voisins. Il reste à évaluer cette idée à grande échelle.


				\begin{figure}[ht]
				$$\begin{tabular}{|l|r|r|r|r|} \hline
    		images &  \includegraphics[height=1cm, width=3cm]{\filext{../reco/image/wreco1}} & 
    		\includegraphics[height=1cm, width=3cm]{\filext{../reco/image/wreco2}} & 
    		\includegraphics[height=1cm, width=3cm]{\filext{../reco/image/wreco3}} & 
    		\includegraphics[height=1cm, width=3cm]{\filext{../reco/image/wreco4}} \\ 
    		indices & $(1)$ & $(2)$ & $(3)$  & $(4)$  
    		\\ \hline	
    		$d\pa{(1), .}$ & -    & 	 9   &	 27   &	 35   \\
				$d\pa{(2), .}$ & 9   &	 -     &	 26   &	 33   \\
				$d\pa{(3), .}$ & 27  & 	 26 &  	 -   &  	 16  \\
				$d\pa{(4), .}$ & 35  & 	 33 &  	 16  & 	 -     \\
				\hline \end{tabular}$$
    		\caption{	Quatre images de mots et leurs distances respectives selon la distance décrite au 
    							paragraphe~\ref{reco_edit_distance_word}. La distance entre deux mots identiques n'excède pas 
    							20, la distance entre deux mots différents est aussi supérieure à ce seuil.}
    		\label{reco_knn_reco_word_example_edit_distance}
    		\end{figure}







\subsection{Résultats}

Trois expériences sont réalisées afin d'observer les performances en reconnaissance d'un tel système. La première (ICDAR1) utilise un ensemble de 1600 mots anglais, chacun est présent dans la base d'images au moins dans quatre d'entre elles et dans au plus cent images. La seconde (ICDAR2) expérience utilise un ensemble de 116 mots anglais, leur nombre d'occurrences est compris entre 100 et 743 (voir figure~\ref{reco_knn_reco_word}). Dans les deux cas, la base d'images contient environ 25000 images de mots anglais\footnote{Cette base est extraite de celle donnée à l'adresse \textit{http://www.cs.nott.ac.uk/$\sim$dge/words.tar.gz} sur le site ICDAR 2003 (voir \citeindex{ICDAR2003}). Les mots ont été ordonnés par ordre d'occurrence décroissante, ceux dont la fréquence est inférieure à 4 ou supérieure à 743 ont été éliminés afin de conserver 50000 mots répartis sur deux expériences. Les mots les plus fréquents qui ont été rejetés sont les suivants~: with, do, was, not, as, me, this, am, it, in, that, my, of, a, and, the, to, i, ce sont de petits mots qu'il n'est en général pas utile de reconnaître. Les mots les moins fréquents sont de grands mots et très significatifs (antidepressants, astrological, backgammon...).} (voir \citeindex{ICDAR2003}). 

La troisième expérience (PRENOMS) utilise un ensemble de 157 prénoms français non composés, leur nombre d'occurrences est compris entre 50 et 3400 environ\footnote{Quelques-uns des prénoms moins courants sont "Clémence", "Aimée", "Nelly", les plus courants sont "Pierre", "Louis", et de loin les plus courants, "Jean" et "Marie".}. La base d'images contient 30000 prénoms.





				\begin{figure}[ht]
				$$\begin{tabular}{|c|c|c|} \hline
    		\includegraphics[height=3cm, width=5cm]{\filext{../reco/image/dicocc}} & 
    		\includegraphics[height=3cm, width=5cm]{\filext{../reco/image/dicocc2}} & 
    		\includegraphics[height=3cm, width=5cm]{\filext{../reco/image/dicocc3}} \\
    		$(a)$ = ICDAR1 & $(b)$ = ICDAR2  & $(c)$  = PRENOM 
    		\\ \hline	\end{tabular}$$
    		\caption{	Fréquence des mots dans les bases d'apprentissage et de test, les graphiques $(a)$ et $(b)$ 
    							correspondent respectivement à la première et à la seconde expérience, l'image $(c)$ 
    							correspond à la troisième expérience.}
    		\label{reco_knn_reco_word}
    		\end{figure}

\indexfrr{base}{apprentissage}
\indexfrr{base}{test}

Pour ces trois expériences, la base d'images est divisée en deux sous-bases, les bases d'apprentissage et de test de taille égale et de répartition homogène. L'expérience consiste à chercher pour les séquences d'observations de la base de tests les dix plus proches voisins de la base d'apprentissage et de classer l'image selon la classe des voisins ainsi que leur proximité. 

Bien que rappelés par la table~\ref{reco_kppv_word_recognition}, les résultats de ces expériences ont déjà été en partie présentés au paragraphe~\ref{image_illustration_resultat} (page~\pageref{image_illustration_resultat}, table~\ref{image_kppv_word_recognition}). Ils permettent de sélectionner le meilleur couple (segmentation en graphèmes~- caractéristiques) pour une reconnaissance de mots sans avoir à comparer les performances de modèles de Markov cachés nécessitant plus d'une semaine d'apprentissage. Mis à part que les éléments à classer sont des séquences, la méthode de reconnaissance est identique à celle présentée au paragraphe~\ref{reco_sel_feat_graph}.

				\begin{table}[ht]
				$$\begin{tabular}{|c|c|c|}  \hline
				expérience			&			jeu 				&     taux de reconnaissance 		\\ \hline
				$ICDAR1$				& $Prof\pa{5,5}$	&		14,65 \%		\\ 
				$ICDAR1$				& $Mat\pa{5,5}$		&		20,61 \%		\\ \hline
				$ICDAR2$				& $Prof\pa{5,5}$	&		45,11 \%		\\
				$ICDAR2$				&	$Mat\pa{5,5}$	 	& 	52,12 \%		\\ \hline
				$PRENOMS$				& $Prof\pa{5,5}$	& 	30,87 \%		\\
				$PRENOMS$				& $Mat\pa{5,5}$	 	& 	40,21 \%		\\ \hline
				$ICDAR2$				&	$Mat\pa{5,5}$ + liaisons & 	54,75 \%		\\
				\hline \end{tabular}$$
				\caption{	Taux de reconnaissance pour une reconnaissance de mot à l'aide de plus proches voisins.
									La dernière expérience reprend la base ICDAR2 mais cette fois-ci utilise une distance qui est la somme
									de la distance entre les deux séquences de graphèmes et de celle entre les deux
									séquences de liaisons.
									}
				\label{reco_kppv_word_recognition}
				\end{table}
				


Ces chiffres de la table~\ref{reco_kppv_word_recognition} montrent tout d'abord l'importance de la taille du vocabulaire. Le taux de reconnaissance de la première expérience ICDAR1 est trois fois plus important que celui de la seconde ICDAR2 où les mots sont dix fois moins nombreux et les exemples d'apprentissages dix fois plus nombreux. Bien que ce tableau n'énumère pas les performances obtenues pour chaque jeu de caractéristiques, ces expériences ont montré que les jeux de type $Mat$ se comportent mieux que les autres, validant en cela les résultats du paragraphe~\ref{reco_sel_feat_graph}. La dernière expérience tend à montrer que les liaisons permettent d'accroître les performances en reconnaissance même si l'essentiel du résultat dépend de la forme des graphèmes.






\indexfrr{mots}{fréquence}


L'inconvénient majeur de cette méthode est qu'il est nécessaire que la base d'apprentissage ait au moins un exemplaire du mot à reconnaître afin que le processus de reconnaissance puisse avoir une chance de l'identifier alors que les modèles de Markov cachés n'ont besoin que d'exemples de lettre. Cela ne signifie pas qu'une telle méthode de reconnaissance doive être abandonnée, la base d'apprentissage peut d'abord être complétée en générant des séquences d'observations pour les mots manquants en utilisant par exemple les méthodes décrites au paragraphe~\ref{reco_generation_word_sequence}. Toutefois, comme un tel système est à la fois gourmand en espace de stockage et en temps de calcul, cette extension ne paraît pas envisageable. En revanche, il serait préférable de restreindre un tel système à l'utilisation des mots fréquents tels que les mots de liaisons et ce pour deux raisons. La première est que ces mots sont disponibles en grand nombre ce qui rend un tel système performant. La seconde raison est que ces mots si fréquents sont souvent mal écrits parce que l'\oe il humain est habitué à les lire, ils introduisent donc du bruit lors de l'apprentissage des modèles de Markov cachés (voir paragraphe~\ref{reco_modele_presentation_1}) qui eux permettent aisément de reconnaître des mots dont on ne dispose pas d'exemple dans la base d'apprentissage. Un système de reconnaissance pourrait donc allier un classifieur à base de plus proches voisins spécialisé dans la reconnaissance des mots les plus fréquents et des modèles de Markov cachés spécialisés dans la reconnaissance des mots les moins fréquents.

Cette reconnaissance pourrait également tout-à-fait être appliquée à la sélection de caractéristiques plutôt que la méthode développée au paragraphe~\ref{reco_selection_caracteristique} qui s'intéresse à la reconnaissance de caractères et non de mots. Toutefois, la distance utilisée ici est plus complexe et nécessite plus de temps de calcul, ce qui prenait deux heures pour obtenir un taux de reconnaissance caractère nécessiterait douze heures pour obtenir un taux de reconnaissance mot. La méthode présentée ici permettrait d'affiner les résultats obtenus au paragraphe~\ref{reco_selection_caracteristique}.



Cette méthode de reconnaissance reconnaît les mots sans se soucier des lettres ou des syllabes. Il est d'ailleurs possible d'effectuer un parallèle avec la méthode globable d'apprentissage de la lecture qui rend difficile la lecture de nouveaux mots. Les modèles de Markov cachés, quant à eux, s'apparentent plus à la méthode syllabique puisque ceux-ci permettent de construire un modèle de reconnaissance par lettre puis de les assembler ensuite pour former un modèle de mot. Ils peuvent ainsi tout reconnaître et pallier les lacunes de la première méthode si celle-ci était utilisée en premier.








%---------------------------------------------------------------------------------------------------------------
\section{Présentation des modèles de reconnaissance}
%---------------------------------------------------------------------------------------------------------------
\label{reco_modele_presentation_1}



\subsection{Modèle hybride réseau de neurones et modèles de Markov cachés}


\indexfrr{séquence}{observations}
\indexfrr{séquence}{liaisons}
\indexfrr{modèle}{reconnaissance}

L'utilisation de modèles de reconnaissance suppose que l'image a été préalablement segmentée en graphèmes. De cette séquence de graphèmes, on tire deux séquences de vecteurs de dimension fixe, la première est la séquence d'observations $O=\vecteur{O_1}{O_T}$, elle transcrit la forme des graphèmes. La seconde séquence est celle des liaisons notée $L=\vecteur{L_1}{L_T}$. 


\indexfr{réseau de neurones}
\indexfr{MMC}

Les modèles de reconnaissance (décrits dans \citeindex{Augustin2001}) associent réseaux de neurones et modèles de Markov cachés et fonctionnent selon le schéma figure~\ref{reco_decryptage_mot} afin de décrypter un mot. Le décryptage consiste ici à déterminer une probabilité mesurant la cohérence entre la séquence de graphèmes et le mot associé au modèle de reconnaissance. Une forte probabilité désigne une séquence proche de celles présentes dans la base d'apprentissage ayant servi à l'estimation des modèles.


				\begin{figure}[t]
				$$
				\begin{array}{|c|} \hline
    		\includegraphics[height=10cm, width=15cm]{\filext{../reco/image/decrypte}} \\ \hline
    		\end{array}
    		$$
    		\caption{	Décryptage d'un mot à l'aide d'un modèle hybride réseau de neurones et 
    							modèle de Markov caché. Le réseau de neurones permet de reconnaître en classant
    							chaque graphème parmi la centaine de classes disponibles. Le modèle de Markov caché 
    							associé au mot "Georges" résulte de la juxtaposition des modèles associés aux lettres qui le 
    							composent. Chacun d'entre eux est illustré par les séquences de classes de graphèmes
    							les plus courantes qui permettent d'écrire une lettre. La ligne parcourant successivement les 
    							modèles des lettres du mot "Georges" correspond à la meilleure association 
    							entre la séquence d'observations
    							extraites de l'image et la juxtaposition de celles apprises par chacune des lettres.
    		}
    		\label{reco_decryptage_mot}
    		\end{figure}


L'ensemble d'un tel système de reconnaissance sera détaillé par la suite (aussi dans les annexes\seeannex{annexe_hmm_def}{chaîne de Markov cachée}). En résumé, il est composé de~:

		\begin{enumerate}
		\item Un réseau de neurones classifieur, il classe chaque graphème parmi une centaine de classes, 
					autrement dit, il détermine la classe à laquelle la forme du graphème correspond. Les liaisons 
					ne sont pas prises en compte.
		\item Un jeu de modèles de Markov cachés incluant 26 modèles, un pour chaque lettre mais il peut également 
					intégrer des modèles associés aux signes de ponctuation, aux chiffres, aux minuscules, aux majuscules. 
					Cette liste dépendant des hypothèses du problème à résoudre.
		\end{enumerate}


Les performances des modèles hybrides ainsi construits sont comparées pour une même séquence. Si deux mots $m_1$ et $m_2$ ont pour modèles de Markov cachés $M_1$ et $M_2$, le mot $m_1$ est plus probable que le mot $m_2$ si $\pr{O \sac M_1} > \pr{O \sac M_2}$. L'expression de cette probabilité ainsi que les modèles $M_1$ et $M_2$ sont détaillés dans les paragraphes qui suivent.















\subsection{Liaisons}
\label{reco_liaison_grapheme}


Les modèles présentés au paragraphe précédent s'appuient sur une séquence d'observations $\vecteur{O_1}{O_T}$ décrivant la forme des graphèmes. Comme le montre la figure~\ref{reco_exemple_information_liaison}, cette information n'est pas toujours suffisante pour déterminer une séquence de lettres. La première idée consiste à ajouter un vecteur de caractéristiques décrivant cette liaison à celui décrivant la forme. La dimension des vecteurs de la séquence $\vecteur{O_1}{O_T}$ en est augmentée ainsi que le nombre de classes d'observations nécessaires pour classer ces nouvelles observations. Cette seconde séquence est notée $\vecteur{L_1}{L_T}$. Bien qu'il n'y ait que $T-1$ liaisons pour $T$ graphèmes, afin de conserver les mêmes dimensions, la liaison $L_1$ désigne le début du mot et la valeur qui lui est affectée décrit une liaison entre graphèmes infiniment éloignés. Les autres liaisons $L_t$ décrivent la liaison entre les graphèmes $O_{t-1}$ et $O_t$.


				\begin{figure}[ht]
				$$
				\begin{array}{|c|} \hline
    		\includegraphics[height=1.5cm, width=5cm]{\filext{../reco/image/ex_link}} \\ \hline
    		\end{array}
    		$$
    		\caption{	Information contenue dans les liaisons~: sur cette segmentation graphème,
    							la lettre "G" est découpée en deux morceaux, le premier ressemble à un "C", 
    							le second à un "J". Si aucune information relative à la liaison entre ces deux graphèmes
    							n'est prise en compte, la séquence "CJ" est tout aussi probable que la séquence "G".}
    		\label{reco_exemple_information_liaison}
    		\end{figure}


\indexfr{IOHMM}

Si on note $C_G$ le nombre de classes de graphèmes et $C_L$ le nombre de classes de liaisons, le nombre de classes nécessaires pour classer les observations est susceptible d'atteindre la borne $C_G \times C_L$. La première idée consiste à regrouper ces deux séquences en une seule $\vecteur{\vecteurim{O_1}{L_1}}{\vecteurim{O_T}{L_T}}$ puis d'utiliser les mêmes types modèles avec ces nouvelles données. Quelques expériences ont été menées en ce sens et les résultats obtenus n'ont pas été meilleurs que ceux obtenus avec la séquence $\vecteur{O_1}{O_T}$. Par conséquent, cette voie a été abandonnée pour construire des modèles prenant en compte deux séquences d'observations, la première décrivant les graphèmes, la seconde décrivant les liaisons. Ces modèles sont inspirés des Input-Output Hidden Markov Models (IOHMM) développés dans \citeindex{Bengio1996}. Les vecteurs inclus dans les séquences doivent d'abord être probabilisés avant d'être utilisés par des modèles de Markov cachés. Parmi les options les plus courantes, on distingue la modélisation de la densité des observations par un mélange de lois gaussiennes ou la classification des observations de manière à obtenir les probabilités suivantes~:


			\begin{itemize}
			\item	séquence des probabilités des classes d'observations~: $\vecteur{C^O_1}{C^O_T}$ et
			
							\begin{eqnarray*}
							C^O_i 		&=& \vecteur{C^O_{i,1}}{C^O_{i,N_O}}' \in \cro{0,1}^{N_O} 
								\text{ et } \forall k, \; C^O_{i,k} = \pr { O_i \in classe \; k \sac O_i }
							\end{eqnarray*}
			
			\item	séquence des probabilités des classes de liaisons~: $\vecteur{C^L_1}{C^L_T}$
			
							\begin{eqnarray*}
							C^L_i 		&=& \vecteur{C^L_{i,1}}{C^L_{i,N_L}}' \in \cro{0,1}^{N_L} 
								\text{ et } \forall k, \;  C^L_{i,k} = \pr { L_i \in classe \; k \sac L_i }
							\end{eqnarray*}
			
			\end{itemize}
			
\indexfr{réseau de neurones}			
\indexfr{hybride}

Les modèles développés par \citeindex{Augustin2001} utilisent les réseaux de neurones, comme ces travaux s'incrivent dans la continuité de ceux de \citeindex{Augustin2001}, c'est celle-ci qui a été choisie. Ces probabilités sont le résultat de réseaux de neurones classifieurs\seeannex{subsection_classifieur}{classification}. Les modèles IOHMM présentés ici seront toujours hybrides alliant réseau de neurones pour reconnaître les formes ou les liaisons et modèles de Markov cachés pour modéliser les séquences. 




\subsection{Modèles de lettres}
\indexfrr{modèle}{lettre}
\label{reco_modele_lettres_new}

A chaque mot est associé un modèle conçu comme étant la juxtaposition de modèles de lettres que ce paragraphe a pour but d'introduire. Ces modèles de lettres constituent la brique élémentaire du système de reconnaissance. Soit $q_t$ l'état d'une chaîne de Markov à l'instant $t$ et $C^L_t$ la classe de la $L_t$ à l'instant $t$, voici l'hypothèse supplémentaire du modèle IOHMM par rapport au modèle HMM\seeannex{interdoc_mmc}{chaîne de Markov cachée}~:

				\begin{eqnarray*}
				\pr { q_t \sac q_{t-1}, 	 \overline{L_t}} &=&  
																	\summy{i=1}{N_l} \; \pr { q_t, C^L_t = i \sac q_{t-1}, L_t} \\
						&=& \summy{i=1}{N_l} \; \pr {q_t \sac C^L_t = i, q_{t-1}} \; \pr { C^L_t = i \sac q_{t-1}, L_t} \\
						&=& \summy{i=1}{N_l} \; \pr {q_t \sac C^L_t = i, q_{t-1}} \; \pr { C^L_t = i \sac  L_t}
				\end{eqnarray*}

				\begin{table}[ht]
				$$
				\begin{array}{|lll|} \hline
				\text{notation}			&	\text{signification}													&   \text{modèle attaché} \\ \hline
				c_{i,c}							& \pr { C^O_t = c \sac q_t = i } 								&  	MMC		\\ 
				\gamma_c\pa{O_t}		&	\pr { C^O_t = c \sac O_t }										&  	RN^O	\\ 
				a_{i,j,d}						& \pr { q_t = j   \sac q_{t-1}, C^L_t = d }			& 	MMC   \\ 
				\pi_{j,d}		  			& \pr { q_1 = j   \sac  C^L_1 = d }							& 	MMC 	\\ 
				\delta_d \pa{L_t}		& \pr { C^L_t = d \sac  L_t}										& 	RN^L	\\ 
				\theta_{j}		  		& \pr { fin       \sac  q_T = j }   						& 	MMC 	\\ 
				f\pa{L_t}						& \text{densité des liaisons}    								&    - 		\\  %\pr { L_t }						
				g\pa{O_t}						& \text{densité des observations}								&    - 		\\  %\pr { O_t }
				p_c									& \pr { C_t^0 = c }															&    - 		\\ \hline
				\end{array}
				$$
				\caption{Notations utilisées pour désigner les paramètres des modèles IOHMM hybrides.}
				\label{reco_iohmm_notations}
				\end{table}
				
				\begin{table}[ht]
				\begin{eqnarray}
				\begin{array}{|rcl|crcl|}\hline
				\forall i, \; \summyone{c} \, c_{i,c} &=& 1 &&
					\summyone{c} \; \gamma_c\pa{O_t} &=& 1  \\ 
				\forall i, \; \summyone{j} \, \cro{ \theta_j + \summyone{d} \, a_{i,j,d}} &=& 1 &&
					\forall i, \; \summyone{d} \pi_{i,d} &=& 1   \\ 
				\summyone{d} \; \delta_d\pa{O_t} &=& 1 &&
					\summyone{c} \; p_c &=& 1   \\ 
				\displaystyle \int f\pa{L} dL &=& 1 &&
					 \displaystyle \int g\pa{O} dO &=& 1  \\ \hline
				\end{array}
				\end{eqnarray}				
				\caption{Contraintes vérifiées par les coefficients donnés par la table~\ref{reco_iohmm_notations}.}
				\label{reco_iohmm_notations_contrainte}
				\end{table}
				
La table~\ref{reco_iohmm_notations} regroupe la liste des paramètres utilisés pour le calcul des probabilités des séquences ainsi que pour l'apprentissage. D'après la définition des IOHMM, ces paramètres doivent vérifier les contraintes énoncées dans la table~\ref{reco_iohmm_notations_contrainte}. Avec ces notations, la probabilité d'un chemin ou séquence d'états du IOHMM devient~:

				\begin{eqnarray*}
				 		\pr{ \vecteurno{O_1}{O_T}, \; \vecteurno{L_1}{L_T}, \; \vecteurno{q_1}{q_T} } 
				&=& 										\pi_{q_1}\pa{L_1} \; b_{q_1}\pa{O_1} \;
				  		\cro { \prody{t=2}{T} \; a_{q_{t-1},q_t} \pa{L_t} \;  b_{q_t}\pa{O_t} }
										 \theta_{q_T}  \\
				\text{avec } a_{q_{t-1},q_t} \pa{L_t} &=& \summy{c=1}{N_C} \, \pr { q_t \sac q_{t-1}, 	 L_t}
																							\underbrace{f\pa{L_t}}_{\begin{subarray}{c} \text{densité} \\ 
																																			\text{ des liaisons} \end{subarray}}
				\end{eqnarray*}


Afin de simplifier les expressions résultant du calcul des probabilités d'émission, les expressions intermédiaires suivantes sont calculées à chaque itération~$t$~:				

				\begin{eqnarray}
				b_i\pa{O_t} 			&=& g\pa{O_t} \;  \summy{c=1}{N_O} \; \dfrac{\gamma_c\pa{O_t} \; c_{i,c} }
																										{p_c}   \\
				a_{i,j}\pa{L_t}		&=& f\pa{L_t} \;  \summy{c=1}{N_L} \; a_{i,j,c} \; \delta_c\pa{L_t} \\
				\pi_i\pa{L_1}			&=&	f\pa{L_1} \;  \summy{c=1}{N_L} \; \pi_{i,c} \; \delta_c\pa{L_1} 	
				\end{eqnarray}

L'objectif est tout d'abord de calculer la probabilité d'émission des deux séquences d'observations et de liaisons notée~:

				\begin{eqnarray}
				\pr{\vecteurno{O_1}{O_T}, \;   \vecteurno{L_1}{L_T}} = 
							\summyone{ \vecteur{q_1}{q_T}  } \; 
							\pr{ \vecteurno{O_1}{O_T}, \; \vecteurno{L_1}{L_T}, \; \vecteurno{q_1}{q_T} }
				\end{eqnarray}

Comme pour le calcul de la probabilité d'émission d'une séquence d'émission dans le cas d'un modèle de Markov caché\seeannex{hmm_alpha_definition_forward}{algorithme forward}, on construit les suites  $\pa{\alpha_{t,i}}$ et $\pa{ \beta_{t,i} }$ définies par~:

				\begin{eqnarray}
				\alpha_{t,i} &=&  \pr { q_t = i, \;      \vecteurno{O_1}{O_t} , \;  \vecteurno{L_1}{L_t} } \\
				\beta_{t,i}  &=&  \pr {  \vecteurno{O_{t+1}}{O_T} , \; \vecteurno{L_{t+1}}{L_T} \sac q_t = i }  \\
				\beta'_{t,i} &=&  \beta_{t,i}  b_i\pa{O_t} 
				\end{eqnarray}

La dernière suite est plus souvent utilisée car elle est plus commode dans le calcul des probabilités. De manière analogue à l'algorithme~\ref{hmm_algo_forward_es}, la suite $\pa{\alpha_{t,i}}$ est calculée de la manière suivante~:

				\begin{eqnarray}
				\forall i, \; 		\alpha_{1,i}	&=&	\pi_i\pa{L_t} \; b_i\pa{O_1} \\
				\forall j, \; \forall t,  \; \alpha_{t,j}	&=& 
													\summy{i=1}{Q} \; \alpha_{t-1,i} \; a_{i,j} \pa{L_t} \; b_j\pa{O_t} \\
				\pr{\vecteurno{O_1}{O_T}, \;   \vecteurno{L_1}{L_T}} &=&  \summy{i=1}{Q} \; \alpha_{T,i} \; \theta_i
				\end{eqnarray}

	
De manière analogue à l'algorithme~\ref{hmm_algo_backward_es}, la suite $\pa{\beta'_{t,i}}$ est calculée de la manière suivante~:

				\begin{eqnarray}
				\forall i, \; 		\beta'_{T,i} &=& \theta_i \; b_i\pa{O_T} \\
				\forall i, \; \forall t,  \; \beta'_{t,i}	&=& 	b_i\pa{O_t}
																\summy{j=1}{Q} a_{i,j} \pa{L_{t+1}} \; \beta'_{t+1,j}    \\
				\pr{\vecteurno{O_1}{O_T}, \;   \vecteurno{L_1}{L_T}} &=&
													\summy{i=1}{Q} \; \pi_i\pa{L_1} \; \beta'_{i,1} 
				\end{eqnarray}
				
				
Les formules précédentes permettent d'utiliser un modèle IOHMM, il ne reste plus qu'à apprendre les paramètres qui le composent. Cet apprentissage s'effectue à partir d'une base de données contenant pour chaque image d'indice $k$, une séquence d'observations $\vecteur{O_1^k}{O_{T_k}^k}$, une séquence de liaisons $\vecteur{L_1^k}{L_{T_k}^k}$ et une annotation $A_k$ correspondant au contenu de l'image\footnote{Cette information est souvent le mot écrit dans cette image, puisqu'à chaque lettre est associé un modèle, l'annotation permet de déterminer quels modèles doivent inclure dans leur apprentissage les séquences d'observations et de liaisons extraites de l'image. Etant donné qu'un modèle de mot est la juxtaposition de plusieurs modèles de lettres, les formules de la table~\ref{reco_iohmm_reestimation_parametre} seront utilisées pour chaque modèle de lettre et chaque sous-séquence extraites des séquences d'observations et de liaisons pondérées par leur vraisemblance (voir paragraphe~\ref{hmm_reco_modele_lettre_apprentissage}). } Les notations intermédiaires suivantes sont utilisées~:

				\begin{eqnarray}
				P_k 						&=& \pr{ \vecteurno{O^k_1}{O^k_{T_k}}, \vecteurno{L^k_1}{L^k_{T_k}}}  \\
				\alpha^k_{t,i} 	&=&  \pr { q_t = i, \;      \vecteurno{O_1^k}{O_t^k} , \;  \vecteurno{L_1^k}{L_t^k} } \\
				\beta^k_{t,i}  	&=&  \pr {  \vecteurno{O_{t+1}^k}{O_T^k} , \; \vecteurno{L_{t+1}^k}{L_T^k} \sac q_t = i } 
				\end{eqnarray}

\indexfr{apprentissage}
\indexfr{formules Baum-Welch}
\indexfr{réestimation}


				\begin{table}[t]
				\begin{eqnarray*}
				\\ \hline
				\end{eqnarray*}
				\begin{eqnarray} 
				\overline{\theta_i} &=&  \dfrac	{ \summy{k=1}{K} \; \frac{1}{P_k} \; 	\alpha^k_{{T_k},i} \; 
																																							\beta^k_{{T_k},i} }
																				{	\summy{k=1}{K} \; \frac{1}{P_k} \; 
																					\cro{ \summy{t=1}{T_k} \; \alpha^k_{t,i} \; \beta^k_{t,i} } } \\
				%
				\overline{\pi_{i,d}} &=& \frac{1}{K}
																\summy{k=1}{K} \; \frac{1}{P_k} \; 
																											\alpha^k_{1,i} \; \delta_d \pa{L_1} \; \beta^k_{1,i} \\
				%																											
				\overline{a_{i,j,d}} &=& \dfrac	{ \summy{k=1}{K} \; \frac{1}{P_k} \; \cro{
																					\summy{t=1}{T_k-1} \; \alpha^k_{t,i} \; a_{i,j,d} \; 
																																\delta_d\pa{L_t} \; \beta'^k_{t+1,j}
																				}}
																				{ \summy{k=1}{K} \; \frac{1}{P_k} \cro{
																					\summy{t=1}{T_k} \; \alpha^k_{t,i} \; \beta^k_{t,i}
																				}} \\
				%
				\overline{c_{i,c}} 	&=&  \dfrac { \summy{k=1}{K} \; \frac{1}{K}
																					\summy{t=1}{T_k} \; \dfrac{		\alpha^k_{t,i} \; c_{i,c} \;
																																				\gamma_c\pa{O_t} \; \beta^k_{t,i}
																																	  }
																																		{   \summy{u=1}{N_O} \; 
																																				c_{i,u} \; \gamma_u\pa{O_t}
																																		}
																			  }
																				{ \summy{k=1}{K} \; \frac{1}{P_k} \cro{
																					\summy{t=1}{T_k} \; \alpha^k_{t,i} \; \beta^k_{t,i}
																				}}  
				\end{eqnarray}
				\begin{eqnarray*}
				\\ \hline
				\end{eqnarray*}
				\caption{	Formules de mise à jour des paramètres afférents à la partie chaîne de Markov cachée
									d'un modèle IOHMM.}
				\label{reco_iohmm_reestimation_parametre}
				\end{table}		



Les formules de mise à jour des paramètres d'un IOHMM sont décrites dans la table~\ref{reco_iohmm_reestimation_parametre}, celles-ci s'inspirent de celle d'un modèle de Markov caché classique\seeannex{par_apprentissage_hmm}{apprentissage MMC} et se démontrent de la même manière. Les réseaux de neurones classifieurs sont eux-aussi appris en construisant des bases d'apprentissages formées des vecteurs suivants pour les observations\seeannex{hmm_sec_rn_obs_cont}{apprentissage réseau de neurones} $\pa{X,Y}$, $Y = RN^O\pa{X}$~:

				\begin{eqnarray}
				\left | \begin{array}{rcl}
				X &=& O_t^k \\
				Y &=& \pa{ \frac{1}{P_k} \; \summyone{i} \; 
										\dfrac{ \alpha^k_{t,i} \; c_{i,c} \; \gamma_c\pa{O_t^k} \; \beta^k_{t,i}}
													{ \summy{u=1}{N_O} \; c_{i,u} \; \gamma_u\pa{O_t^k}}
									} _ {1 \infegal c \infegal N_O}
				\end{array} \right.
				\label{reco_app_rn_obs_reco_eq1}
				\end{eqnarray}

Si on note $Y_{tc}^k$ la $c^\text{ème}$ sortie désirée du réseau de neurones $RN^O$ pour le vecteur d'observations $O^k_t$, celle donnée par la formule ci-dessus ne maximise pas la vraisemblance mais il est possible de déterminer cette valeur en appliquant l'algorithme~EM de sortie que~:

				\begin{eqnarray}
        \overline{Y_{tc}^k} = \dfrac{1}{ P_k } \pa { 
        					\summyone{q_t} \; 
        							\dfrac{ \alpha_{q_t}^k \pa{t}  \, Y_{tc}^k  \, c_{q_t,c} \, \beta_{q_t}^k\pa{t} }
        									{  \summy{u=1}{C} \, Y_{tu}^k  \,  c_{q_t,u} }
        									}
				\label{reco_app_rn_obs_reco_eq2}
				\end{eqnarray}
				
Le vecteur $Y$ de l'expression (\ref{reco_app_rn_obs_reco_eq1}) est dans ce cas composé du vecteur $\vecteur{ \overline{Y_{t1}^k} } { \overline{Y_{t{N^O}}^k} }$ obtenu après convergence de la formule (\ref{reco_app_rn_obs_reco_eq2}). La valeur initiale de $Y_{tc}^k$ est égale à $\gamma_c\pa{O_t^k}$. Les suites $\alpha\pa{.}$ et $\beta\pa{.}$ sont bien sûr recalculées entre deux mises à jour. On obtient un résultat analogue en ce qui concerne les liaisons, $\pa{X,Y}$, $Y = RN^L\pa{X}$~:

				\begin{eqnarray}
				\left | \begin{array}{rcl}
				X &=& L_t^k \\
				Y &=& \pa{ \frac{1}{P_k} \; \summyone{i,j} \; 
										 \alpha^k_{t,i} \; a_{i,j,d} \; 
														\delta_d\pa{L_t^k} \; \beta'^k_{t+1,j}
									} _ {1 \infegal d \infegal N_L}
				\end{array} \right.
				\label{reco_app_rn_link_reco_eq1}
				\end{eqnarray}

La remarque précédente concernant les sorties désirées pour le réseau de neurones associées aux observations est valable pour les liaisons, le vecteur $Y$ de l'expression (\ref{reco_app_rn_link_reco_eq1}) est composé du vecteur $\vecteur{ \overline{Y_{t1}^k} } { \overline{Y_{t{N^L}}^k} }$ obtenu après convergence de la formule (\ref{reco_app_rn_link_reco_eq2}). La valeur initiale de $Y_{td}^k$ est égale à $\delta_d\pa{L_t^k}$. Les suites $\alpha\pa{.}$ et $\beta\pa{.}$ sont bien sûr recalculées entre deux mises à jour. 


				\begin{eqnarray}
        \overline{Y_{td}^k} = \frac{1}{P_k} \; \summyone{i,j} \; 
										 \alpha^k_{t,i} \; a_{i,j,d} \; 
														Y_{td}^k \; \beta'^k_{t+1,j}
				\label{reco_app_rn_link_reco_eq2}
				\end{eqnarray}


Une dernière remarque concerne les sorties désirées obtenus pour l'apprentissage des réseaux de neurones de classifications des graphèmes et des liaisons. Ces sorties sont souvent soient nulles soient égales à un et ces valeurs ne facilitent pas l'apprentissage de ces réseaux comme le suggère la remarque~\ref{nn_remark_classification_output_alpha}\seeannex{nn_remark_classification_output_alpha}{réseau de neurones} qui propose une modification de ces sorties permettant de faciliter leur apprentissage.





\subsection{Topologie des modèles}
\label{reco_topologie_arbitraire}
\indexfrr{topologie}{modèles}
\indexfr{architecture}

Les formules d'apprentissage qui s'appliquent aux IOHMM présentés au paragraphe~\ref{reco_liaison_grapheme} nécessitent que la structure des modèles soit fixée au préalable. Il faut éviter que celle-ci contienne trop de coefficients auquel cas le calcul des probabilités est coûteux et les modèles risquent de faire du sur-apprentissage, ni trop peu de coefficients afin que les modèles puissent véritablement identifier la lettre à laquelle ils sont affectés.

Dans un premier temps, la structure est fixée arbitrairement en tenant compte de la complexité de chaque lettre, celle-ci est fonction du nombre moyen de graphèmes par lettre. Il est estimé manuellement à partir d'une centaine de documents et donné par la table~\ref{reco_nombre_moyen_grapheme_lettre}. L'article \citeindex{Günter2003} propose quant à lui de construire des modèles de Markov simplifiés afin d'estimer statistiquement la longueur moyenne des séquences  associées à chaque modèle de lettre.


			\begin{table}[ht]
			$$ \begin{tabular}{|cc|cc|cc|cc|cc|cc|} \hline
			a		&	2	&		f		&	2	&		k		& 2	&		p		&  2 	&		u		&	2	&		z		& 2 	\\ \hline
			b		&	2	&		g		& 2	&		l		& 1	&		q		&  2 	&		v		&	2	&				&  		\\ \hline
			c		&	2	&		h		& 2	&		m		& 3	&		r		&  2 	&		w		&	3	&				& 		\\ \hline
			d		&	2	&		i		&	1	&		n		& 2	&		s		&  2 	&		x		&	2	&				& 		\\ \hline
			e		&	2	&		j		&	2	&		o		& 2	&		t		&  2 	&		y		&	2	&				& 		\\ \hline
			\end{tabular}$$
			\caption{	Nombre moyen de graphèmes par lettre~: ce nombre est estimé sur une centaine de documents
								par un opérateur humain, les moyennes sont arrondies au premier entier supérieur.. 
								Une fois les modèles appris, il pourra être affiné grâce à un 
								algorithme de Viterbi permettant de trouver la meilleure association entre les graphèmes
								et les modèles de lettres. Les minuscules et majuscules ne sont pas dissociées.}
			\label{reco_nombre_moyen_grapheme_lettre}
			\end{table}
			
Le nombre d'états $e_k$ choisi pour une lettre $k$ est fonction de $g_k$ le nombre moyen de graphème donné par la table~\ref{reco_nombre_moyen_grapheme_lettre}. Après plusieurs expériences, la règle suivante semble être pertinente~:

			\begin{eqnarray}
			e_k		&=&		x \pa{g_k+1} \text{ avec } x \in \N
			\end{eqnarray}

La matrice des transitions est triangulaire strictement supérieure de manière à éviter les cycles. Les coefficients non nuls relatifs aux classes d'observations sont limités à trois pour un état ($c_{i,c}$), ce nombre permet de retrouver un nombre de coefficients légèrement supérieur à ceux observés dans les modèles développés par \citeindex{Augustin2001}. Par conséquent, on pose $x = 3$, le nombre de coefficients d'un modèle $M_k$ associé à la lettre $k$ correspond à~:
			
			\begin{eqnarray*}
			&& \underbrace{e_k^2 C_L}_{\text{ probabilités de transitions} } + 
				 \underbrace{e_k C_L}_{\text{ probabilités d'entrée}} +
				 \underbrace{e_k}_{\text{ probabilités de sortie}} +
				 \underbrace{3 e_k}_{\text{ probabilités d'émission}}  \\
			&& \text { et } C_L \text{ est le nombre de classes liaison.}
			\end{eqnarray*}


Il est préférable de surestimer le nombre optimal de coefficients de façon à pouvoir faire décroître ce nombre en supprimant les coefficients inutiles (voir paragraphe~\ref{reco_selection_architecture}).


Le choix d'une topologie est difficile. L'article \citeindex{Bengio1995} étudie quatre différentes topologies~:
	
	\indexfrr{topologie}{entièrement connectée}
	\indexfrr{topologie}{gauche-droite}
	\indexfrr{topologie}{triangulaire}
	\indexfrr{topologie}{périodique}
		\begin{enumerate}
		\item Le modèle périodique, chaque état boucle est connecté à lui-même et à l'état suivant, formant
					une chaîne d'état.
		\item Le modèle gauche-droite, semblable à un graphe.
		\item Le modèle gauche-droite triangulaire, celui choisi dans ce paragraphe, 
					pour lequel la matrice de transition est triangulaire.
		\item Le modèle entièrement connecté.
		\end{enumerate}
		
\indexfr{coefficient d'ergodicité de Dobrushin}\indexfr{ergodicité}\indexfr{Dobrushin}
Cet article mesure la complexité la difficulté de l'estimation de chacun de ces modèles par l'intermédiaire du coefficient d'ergodicité de Dobrushin (voir \citeindex{Senta1986})~:

			\begin{eqnarray}
			\tau(A) &=& \frac{1}{2} \underset{i,j}{\sup} \abs{ a_{ik} - a_{jk}} \text{ où $A$ est la matrice de transition}
			\end{eqnarray}
		
Ce coefficient d'ergodicité converge vers 0 lors de l'apprentissage, il converge d'autant plus vite que la topologie des modèles est complexe. Plus il converge vite, plus l'apprentissage est difficile. L'article \citeindex{Bengio1995} étudie cette convergence pour les quatre types de topologie présentés ci-dessus. L'article \citeindex{Abou-Mustafa2004} reprend ces résultats et conclut par le fait que les topologies les plus simples sont plus à même de généraliser et que les résultats de reconnaissance sont meilleurs lorsque les probabilités sont déterministes, c'est-à-dire proche de~0 ou~1.

 

%---------------------------------------------------------------------------------------------------------------
\section{Reconnaissance avec dictionnaire}
%---------------------------------------------------------------------------------------------------------------
\indexfr{dictionnaire}


\subsection{Principe}



On considère une séquence d'observations obtenue à partir d'une image ségmentée en graphèmes (figure~\ref{hmm_figure_exemple_grapheme}), la reconnaissance avec dictionnaire consiste à trouver le mot dans cette liste qui correspond à l'image.

\indexfr{graphème}


        \begin{figure}[ht]
        $$\frame{$\begin{array}[c]{c}\includegraphics[height=1cm, width=3cm]
        {\filext{../dessin2/imagemg}}\end{array}$}$$
        \caption{Un mot segmenté en graphèmes}
        \label{hmm_figure_exemple_grapheme}
        \end{figure}
        
La définition qui suit introduit les notations qui seront couramment utilisées par la suite.
        

		\begin{xdefinition}{reconnaissance avec dictionnaire}
		Soit $O=\vecteur{O_1}{O_T} \in \mathcal{O}$ une séquence d'observations avec $\mathcal{O}$ l'ensemble 
		des séquences d'observations, cette séquence décrit une image du mot $m^*$. Soit un dictionnaire 
		$D=\vecteur{m_1}{m_N}$ ou liste finie de mots, la reconnaissance avec le dictionnaire $D$ telle 
		qu'elle est présentée dans ce document consiste à trouver une fonction 
		$f_D : \pa{\mathcal{O},D} \rightarrow \R$ telle que~:
		        $$
		        \underset{m \in D}{\arg \max} \; f_D\pa{O,m} = m^*
		        $$
		\end{xdefinition}


\begin{xremark}{rejet}
Cette \indexfr{rejet} définition sous-entend que la solution $m^*$ appartient au dictionnaire et qu'il n'est pas possible de rejeter un mot n'en faisant pas partie. Il est cependant possible d'accepter ou de refuser la solution selon la valeur de $f_D\pa{O,m^*}$.
\end{xremark}

On impose également au dictionnaire d'être dynamique (définition~\ref{hmm_reco_def_dico_dyn}). Même si la définition du problème ne s'en trouve pas changée, les choix de modélisation doivent tenir compte de cette contrainte supplémentaire.


		\begin{xdefinition}{dictionnaire dynamique} \label{hmm_reco_def_dico_dyn}
		\indexfrr{dictionnaire}{dynamique}
		Un dictionnaire $D$ est dit dynamique si l'ajout ou la suppression de mot n'implique pas une nouvelle 
		recherche de la fonction $f_{D'}$ où $D'$ est un dictionnaire différent de $D$. L'estimation de 
		la fonction $f$ est par conséquent indépendante du dictionnaire $D$.
		\end{xdefinition}

La solution préconisée est la construction pour un mot $m$ d'un modèle probabiliste de mot $M_m$ associé à ce mot. On note $\pr{M \sac O}$ la probabilité du modèle sachant la séquence d'observations.

        \begin{eqnarray}
        f\pa{O,m} &=& \pr{M_m \sac O} = \pr{O \sac M_m} \dfrac{\pr{M_m}}{\pr{O}}
        \end{eqnarray}
        
On suppose que $\pr{M}$ est constante quel que soit le mot $m$ et dans ce cas~:

        \begin{eqnarray}
        \underset{m \in D}{\arg \max} \; f_D\pa{O,m} = \underset{m \in D}{\arg \max} \; \pr{O \sac M_m}
        \end{eqnarray}
        
        

La segmentation en graphèmes n'a pas encore été justifiée tout comme l'utilisation d'une séquence d'observations. Il est tout-à-fait possible de construire un modèle de mot prenant en compte l'image dans son ensemble et non des morceaux. Mais un simple réseau de neurones ou tout autre classifieur chargé de reconnaître le mot "CHARLES" par exemple devra être estimé sur une base d'apprentissage incluant des images de ce mot. Il est alors impossible de reconnaître ce mot s'il n'est pas présent dans les bases d'apprentissage (voir paragraphe~\ref{reco_reco_knn_sequence}) ce qui se produit lorsque le dictionnaire est dynamique comme le montre la table~\ref{hmm_table_occurence_prenom} qui décrit les occurrences des prénoms dans les bases d'apprentissage et de test pour un problème de reconnaissance de prénoms manuscrits.


        \begin{table}[ht]
        \[
        \scriptsize 
        \begin{tabular}{|c|c|c|}\hline
                     & occurrences dans             & occurrences dans              \\ 
        prénoms      & la base d'apprentissage      & la base de test               \\ \hline
        JEAN        & 1796                          & 615                           \\ \hline
        JEANE       & 0                             & 1                             \\ \hline
        JEANETTE    & 0                             & 1                             \\ \hline
        JEANINE     & 34                            & 9                             \\ \hline
        JEANNE      & 636                           & 186                           \\ \hline
        JEANNETTE   & 15                            & 8                             \\ \hline
        JEANNIE     & 2                             & 0                             \\ \hline
        JEANNINE    & 99                            & 37                            \\ \hline
        JEHAN       & 2                             & 0                             \\ \hline
        JEMMY       & 1                             & 0                             \\ \hline
        JENNY       & 20                            & 2                             \\ \hline
        JERD        & 1                             & 0                             \\ \hline
        JEREMIE     & 1                             & 1                             \\ \hline
        JEROME      & 15                            & 5                             \\ \hline
        \end{tabular}
        \]
        \caption{       Occurrences de prénoms dans les bases d'apprentissage et de test. Certains mots
                        sont présents dans la base de test et pas dans la base d'apprentissage. Cela
                        exclut leur apprentissage par un modèle de mot n'utilisant aucune segmentation
                        d'image comme celle du paragraphe~\ref{reco_reco_knn_sequence}.}
        \indexfr{prénom}                        
        \label{hmm_table_occurence_prenom}
        \end{table}

        
L'option retenue, la plus simple dans sa conception mais pas forcément dans sa mise en \oe uvre, est la segmentation d'un mot en graphèmes\indexfr{graphème} ou imagettes respectant les contraintes suivantes~:

        \begin{enumerate}
        \item Chaque graphème est inclus dans le dessin d'une lettre, c'est donc une lettre ou une partie de lettre.
        \item Les graphèmes doivent être ordonnés selon un ordre respectant l'ordre des lettres dans le mot.
        \end{enumerate}
        
Par conséquent, pour l'image du mot $m$ dont les lettres sont $m = \vecteur{l_1}{l_N}$, on dispose de la séquence de graphèmes $G=\vecteur{G_1}{G_T}$ extraite de l'image et $G_t$ représente une partie de la lettre $l_{a_t}$ où $a_t$ est l'indice de la lettre qui contient le graphème d'indice $t$. Ces trois séquences vérifient~:

        \begin{enumerate}
        \item $N \infegal T$
        \item $a_1 = 1$ et $a_T = N$
        \item si $t < T$, alors $a_t \infegal a_{t+1} \infegal \min \acc{ a_t + 1, N}$
        \end{enumerate}
        
        
\indexfrr{segmentation}{graphème}

Pour le processus de reconnaissance, il aurait été plus facile de segmenter l'image d'un mot en lettres mais ce traitement d'image est difficile à réaliser, c'est pourquoi cette segmentation plus souple est choisie (voir \citeindex{Augustin2001}) et même celle-ci n'est pas toujours facile à respecter (voir paragraphe~\ref{hmm_bi_lettre}). Les graphèmes obtenus par segmentation doivent à la fois être suffisamment gros pour être reconnus et ordonnés, suffisamment petits pour ne pas représenter plus d'une lettre.

Les contraintes énoncées ci-dessus sont également valables pour la séquence $O=\vecteur{O_1}{O_T}$ qui est la transcription de la séquence $G = \vecteur{G_1}{G_T}$ dans un format utilisable par les modèles de Markov cachés et les réseaux de neurones.

\indexfrr{modèle}{lettre}

La séquence $O=\vecteur{O_1}{O_T}$ est donc une succession de lettres ou morceaux de lettres respectant l'ordre de lecture. Ce découpage autorise la construction de modèles de reconnaissance de lettres à partir desquels seront formés les modèles de mots. Par conséquent, la fonction $f\pa{O,m}$ sera uniquement dépendante de vingt-six modèles de lettres et de la séquence d'observations. Le paragraphe suivant montre comment est construit un modèle $M_m$ associé au mot $m$.


        
        
        
        
        


\subsection{Construction de modèles de mot}


\label{hmm_reco_modele_lettre}
\indexfrr{modèle}{mot}
\indexfrr{modèle}{lettre}

        %\begin{figure}[ht]
        %$$\frame{$\begin{array}[c]{c}\includegraphics[height=8cm, width=4.5cm]
        %{\filext{../hmm_reco/image/letter}}\end{array}$}$$
        %\caption{       Modèle de lettre : la séquence d'observations $O=\vecteur{O_1}{O_T}$ est 
        %                donnée au réseau de neurones, celui-ci établit pour chaque observation ses probabilités
        %                d'appartenir à chacune des classes, enfin, le modèle de Markov caché (correspondant ici
        %                à la lettre "G") utilise ces probabilités comme des probabilités d'émissions et retourne
        %                la probabilité d'émettre la séquence $O$.
        %        }
        %\label{hmm_figure_exemple_modele_lettre}
        %\end{figure}
        

Les modèles utilisés sont des modèles de Markov cachés hybrides associés à un réseau de neurones\seeannex{hmm_reseau_neurone}{modèle hybride}. La figure~\ref{reco_decryptage_mot} illustre quelques modèles de lettres\seeannex{annexe_reco_illustration}{modéles de lettres}. Si le mot $m = \vecteur{l_1}{l_N}$, les modèles de lettres $H = \vecteur{H_1}{H_N}$ seront assemblés de manière à former le modèle $M_m$ comme montré par la figure~\ref{figure_modele_mot_simple_attention}. Cet assemblage est simplement une juxtaposition comme le montre la figure~\ref{reco_decryptage_mot}.


            \begin{figure}[ht]
                \[\fbox{\filefig{../reco/fig_georges}}\]
                \caption{       Modèle de mot pour "georges" construit à partir des modèles de lettres assoicés 
                                aux lettres "g", "e", "o", "r", "s". Les symboles "E", "S" signifient 
                                l'entrée et la sortie du modèle.}
                \label{figure_modele_mot_simple_attention}
            \end{figure}


        %\begin{figure}[ht]
        %$$\frame{$\begin{array}[c]{c}\includegraphics[height=8cm, width=9cm]
        %{\filext{../hmm_reco/image/word}}\end{array}$}$$
        %\caption{       Modèle de mot : la séquence d'observations $O=\vecteur{O_1}{O_T}$ est 
        %                donnée au réseau de neurones, celui-ci établit pour chaque observation ses probabilités
        %                d'appartenir à chacune des classes, enfin, le modèle de Markov caché (correspondant ici
        %                au mot "GEORGES") utilise ces probabilités comme des probabilités d'émission et retourne
        %                la probabilité d'émettre la séquence $O$.
        %        }
        %\label{hmm_figure_exemple_modele_mot}
        %\end{figure}
        

        
Maintenant que les modèles de mot sont définis, pour un mot $m = \vecteur{l_1}{l_N}$, il s'agit de calculer la probabilité $\pr{O \sac M_m}$ des deux séquences $\pa{O,L} = \pa{ \vecteurno{O_1}{O_T},  \vecteurno{L_1}{L_T}}$. Le modèle $M_m$ est la juxtaposition des modèles $M_m=\vecteur{H_{l_1}}{H_{l_N}}$. Pour simplifier, cette séquence sera notée $M_m=\vecteur{H_1}{H_N}$.

		\begin{xtheorem}{probabilité d'une séquence avec un modèle de mot}
		\indexfrr{modèle}{mot}
		\label{hmm_theo_proba_modele_mot}
		Soit un modèle de mot $M_m$ composé de modèles de lettre $M_m = \vecteur{H_1}{H_N}$ 
		et les séquences d'observations et de liaisons $\pa{O,L} = \pa{ \vecteurno{O_1}{O_T},  \vecteurno{L_1}{L_T}}$. 
		On définit la suite $\pa{\alpha_{t,t'}^{H_i}}_{t,t',i}$ par~:
		
		        \begin{eqnarray}
		        \forall \pa{t,t'} \in \intervalle{1}{T}^2, \, \forall i \in \intervalle{1}{N} \; 
		                \alpha_{t,t'}^{H_i} &=&     \left\{
		                                            \begin{array}{l}
		                                            \pr{ \vecteurno{O_t}{O_{t'}},
		                                            		 \vecteurno{L_t}{L_{t'}} \sac H_i } \text{ si } t \infegal t' \\
		                                            0 \text{ sinon}
		                                            \end{array}
		                                            \right.
		        \label{hmm_reco_alpha}                                                
		        \end{eqnarray}
		        
		Le calcul de cette suite fait appel à l'équation (\ref{hmm_eq_alpha_4}). 
		On définit également la suite $\pa{\gamma_t^{H_i}}_{t,i}$ par~:
		
		        \begin{eqnarray}
		        \begin{array}{rrcl}
		        \forall i \in \intervalle{1}{N}, \; & \gamma_0^{H_i} &=& 1 \\
		        \forall i \in \intervalle{1}{N}, \, \forall t \in \intervalle{1}{T}, \; & \gamma_t^{H_i} &=& 
		                                \summy{k=1}{t-1} \, \gamma_k^{H_{i-1}} \, \alpha_{k+1,t}^{H_i}
		        \end{array}
		        \label{hmm_reco_gamma_include}
		        \end{eqnarray}
		        
		Alors~:
		
		        \begin{eqnarray}
		        \pr{ O \sac M_m } &=&  \gamma_T^{H_N}
		        \label{hmm_reco_eqn_proba_2}
		        \end{eqnarray}        
		        
		\end{xtheorem}


\begin{xdemo}{théorème}{\ref{hmm_theo_proba_modele_mot}}
La démonstration de ce théorème est similaire à celle de l'équation (\ref{hmm_eq_alpha_4}).
\end{xdemo}


Ce théorème permet de définir l'algorithme suivant~:


		\begin{xalgorithm}{probabilité d'une séquence avec un modèle de mot}
		
		\begin{xalgostep}{initialisation 1}
		        Calcul de la suite $\pa{\alpha_{t,t'}^{H_i}}_{t,t',i}$ grâce à l'algorithme~\ref{hmm_algo_forward}.
		\end{xalgostep}
		        
		\begin{xalgostep}{initialisation 2}
		        \begin{xfor}{i}{1}{N}
		        $\gamma_0^{L_i} \longleftarrow 1$
		        \end{xfor}
		\end{xalgostep}
		        
		\begin{xalgostep}{itérations}
		        \begin{xfor}{t}{1}{T}
		                \begin{xfor}{i}{1}{N}
		                        $\gamma_t^{H_i} \longleftarrow 0 $ \\
		                        \begin{xfor}{k}{1}{t-1}
		                        $\gamma_t^{H_i} \longleftarrow \gamma_t^{H_i} + \gamma_k^{H_{i-1}} \, \alpha_{k+1,t}^{H_i}$ 
		                        \end{xfor}
		                \end{xfor}
		        \end{xfor}
		\end{xalgostep}
		
		La probabilité cherchée est $\gamma_T^{H_N}$.
		        
		\end{xalgorithm}












\subsection{Apprentissage des modèles de lettre}
\label{hmm_reco_modele_lettre_apprentissage}


\indexfrr{apprentissage}{lettre}
\indexfrr{modèle}{lettre}
\indexfrr{modèle}{mot}
\indexfr{apprentissage}

L'apprentissage des modèles de lettre s'appuie sur celui des modèles de Markov cachés. Toutefois, comme ceux-ci sont utilisés dans des modèles de mot, les formules données tableau~\ref{figure_formule_baumwelch-fig} ne s'appliquent pas telles quelles.

\indexfr{optimisation}

Pour simplifier, supposons que le dictionnaire se réduit à deux mots "CHARLES" et "CAROLE". Les modèles respectifs à ces deux mots sont notés $M_{CH}$ et $M_{CA}$ de paramètres $\Theta_{CH}$ et $\Theta_{CA}$ qui regroupent l'ensemble des paramètres de chacun des modèles de lettres qui les composent. L'estimation de $\Theta_{CH}$ et $\Theta_{CA}$ est toujours un problème d'optimisation sur une base d'apprentissage contenant plusieurs exemples de mots "CHARLES" et "CAROLE", les séquences correspondantes sont notées~:

        $$
                        O^{CH}=\vecteur{O_1^{CH}}{O_{N_{CH}}^{CH}} 
        \text{ et }    O^{CA}=\vecteur{O_1^{CA}}{O_{N_{CA}}^{CA}}
        $$

\indexfr{vraisemblance}

La vraisemblance des paramètres $\pa{\Theta_{CH},\Theta_{CA}}$ est~:

        \begin{eqnarray*}
        L\pa{\Theta_{CH},\Theta_{CA},O^{CH},O^{CA}} &=&
               	\prody{n=1}{N_{CH}} \pr{O_n^{CH} \sac M_{CH}}           \; 
               	\prody{n=1}{K_{CA}} \pr{O_n^{CA} \sac M_{CA}}             \\
        &=&     \prody{n=1}{N_{CH}} \pr{O_n^{CH} \sac \Theta_{CH}}      \; 
        				\prody{n=1}{K_{CA}} \pr{O_n^{CA} \sac \Theta_{CA}}         
        \end{eqnarray*}

Finalement, la log-vraisemblance est~:

        \begin{eqnarray}
        \ln L\pa{\Theta_{CH},\Theta_{CA},O^{CH},O^{CA}} &=& 
                  \summy{n=1}{N_{CH}} \ln \pr{O_n^{CH} \sac \Theta_{CH}}      
              +   \summy{n=1}{N_{CA}} \ln \pr{O_n^{CA} \sac \Theta_{CA}}       \label{hmm_reco_eq_likelihood}
        \end{eqnarray}

Si les modèles $M_{CH}$ et $M_{CA}$ n'ont aucun paramètre commun, l'optimisation de (\ref{hmm_reco_eq_likelihood}) est équivalente à deux optimisations séparées de $\Theta_{CH}$ et $\Theta_{CA}$. Les mots "CHARLES" et "CAROLE" ont cependant les lettres "C", "A", "L", "E", "R" en commun. L'apprentissage de tels modèles est résolu dans les paragraphes qui suivent, les formules de réestimation des paramètres des modèles de lettres ne seront pas démontrées mais les raisonnements développés dans les paragraphes~\ref{baumwelch_sens} à~\ref{hmm_demo_em_em} sont encore valables.


Toujours pour deux séquences d'observations et de liaisons $\pa{O,L} = \pa{\vecteurno{O_1}{O_T},\vecteurno{L_1}{L_T}}$, un modèle $M_m=\vecteur{H_1}{H_N}$, on note $q$ un état d'un modèle de lettre, la notation $q \in H_i$ désigne un état appartenant au modèle $H_i$, $q_t$ désigne l'état à l'instant $t$. On définit la suite~:

        \begin{eqnarray}
        \forall t \in \intervalle{1}{T}, \, \forall i \in \intervalle{1}{N}, \, \forall q \in H_i, \;\;
                \alpha_t^{H_i}\pa{q} = \pr {\vecteurno{O_1}{O_t}, \vecteurno{L_1}{L_t}, \, q_t = q \sac M_m} 
        \label{hmm_reco_alpha_include}
        \end{eqnarray}

Cette suite est semblable à celle définie (\ref{hmm_eq_alpha_1}) mais avec un indice de plus pour le modèle de lettre concerné. De même, la suite définie en (\ref{hmm_eq_beta_1}) est déclinée pour un modèle de mot~:

        \begin{eqnarray}
        \forall t \in \intervalle{1}{T}, \, \forall i \in \intervalle{1}{N}, \, \forall q \in H_i, \;\;
                \beta_t^{H_i}\pa{q} = \pr { \vecteurno{O_{t+1}}{O_T},\vecteurno{L_{t+1}}{L_T} \sac q_t = q, \,  M_m} 
        \label{hmm_reco_beta_include}                
        \end{eqnarray}

La suite (\ref{hmm_reco_alpha}) est enrichie d'un indice supplémentaire~:

        \begin{eqnarray}
        \forall \pa{t,t'} \in \intervalle{1}{T}^2, \, \forall i \in \intervalle{1}{N}, \; \nonumber \\
                \alpha_{t,t'}^{H_i}\pa{q} &=&         \left\{
                                                \begin{array}{l}
                                                \pr{ \vecteurno{O_t}{O_{t'}},
                                                			\vecteurno{L_t}{L_{t'}}, 
                                                			q_{t'} = q \sac L_i } \text{ si } t \infegal t' \\
                                                0 \text{ sinon}
                                                \end{array}
                                                \right.
        \label{hmm_reco_alpha_q}
        \end{eqnarray}
        

Le calcul de (\ref{hmm_reco_alpha_q}) s'effectue grâce à un algorithme forward (\ref{hmm_algo_backward}). Il est possible d'exprimer la suite (\ref{hmm_reco_alpha_include}) à partir des suites (\ref{hmm_reco_gamma_include}) et (\ref{hmm_reco_alpha_q}) comme suit~:

        \begin{eqnarray}
        \forall t \in \intervalle{1}{T}, \, \forall i \in \intervalle{1}{N}, \, \forall q \in H_i, \;\;
        \alpha_t^{H_i}\pa{q} &=& \summy{k=1}{t-1} \gamma_k^{H_{i-1}} \, \alpha_{k+1,t}^{H_i}\pa{q}
        \end{eqnarray}
        
Le calcul de la suite (\ref{hmm_reco_beta_include}) nécessite l'introduction de nouvelles suites, tout d'abord~:

        \begin{eqnarray}
        \forall \pa{t,t'} \in \intervalle{1}{T}^2, \, \forall i \in \intervalle{1}{N}, \; \nonumber \\
                \beta_{t,t'}^{H_i}\pa{q} &=&    \left\{
                                                \begin{array}{l}
                                                \pr{ \vecteurno{O_t}{O_{t'}},
                                                     \vecteurno{L_t}{L_{t'}},
                                                 				\sac q_{t-1} = q, \, L_i } 
                                                		\text{ si } t \infegal t' \\
                                                0 \text{ sinon}
                                                \end{array}
                                                \right.
        \label{hmm_reco_beta_q}
        \end{eqnarray}

Et~:

        \begin{eqnarray}
        \begin{array}{rrcl}
        \forall i \in \intervalle{1}{N}, \; & \delta_{T+1}^{H_i} &=& 1 \\
        \forall i \in \intervalle{1}{N}, \, \forall t \in \intervalle{1}{T}, \; & \delta_t^{H_i} &=& 
                                \summy{k=t+1}{T+1} \, \delta_k^{H_{i+1}} \, \alpha_{t,k}^{H_i}
        \end{array}
        \label{hmm_reco_delta_include}
        \end{eqnarray}

Par conséquent~:

        \begin{eqnarray}
        \forall t \in \intervalle{1}{T}, \, \forall i \in \intervalle{1}{N}, \, \forall q \in H_i, \;\;
        \beta_t^{H_i}\pa{q} &=& \summy{k=t+1}{T} \delta_k^{H_{i+1}} \, \alpha_{t,k}^{H_i}\pa{q}
        \label{hmm_reco_beta_include_end}
        \end{eqnarray}
        
        
En tenant compte qu'un mot peut contenir plusieurs fois la même lettre et donc qu'un coefficient peut faire partie de plusieurs modèles $H_i$, il est possible, armé de toutes ces suites, d'adapter les formules établies dans la table~\ref{figure_formule_baumwelch-fig} (voir table~\ref{hmm_reco_estimation_lettre}).



            \begin{table}[H]
                \begin{eqnarray*} \\ \hline
                \end{eqnarray*}
            		\begin{eqnarray} 
                \overline{\pi_{q,d}^{H_l}}  &=& \dfrac
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  \,
                                    \summy{t=1}{T_k} \,
                                    \gamma_{t}^{k,H_{i-1}} \, \pi_{q,d}^{H_i} \, \delta_d\pa{L_t} \,
                                     b_{q}^{k,H_i}\pa{O_{t}^k} \,
                                    \beta_{t}^{k,H_i}\pa{q}
                            }
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \gamma_{t}^{k,H_{i-1}} \, \delta_{t}^{k,H_{i}}
                            } \\ 
						                \overline{a_{q,q',d}^{H_l}} &=&     \dfrac
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \alpha_t^{k,H_i}\pa{q} \, a_{q,q',d}^{H_l} \, \delta_d \pa{L_t^k} \,
                                    b_{q'}^{k,H_i}\pa{O_{t+1}^k} \,
                                    \beta_{t+1}^{k,H_i}\pa{q'}
                            }
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \alpha_t^{k,H_i}\pa{q} \, \beta_{t}^{k,H_i}\pa{q}
                            } \\ 
                \overline{\theta_q^{H_l}}  &=& \dfrac
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \alpha_t^{k,H_i}\pa{q} \, \theta_q^{H_l} \, \delta_t^{k,H_i}
                            }
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \alpha_t^{k,H_i}\pa{q} \, \beta_{t}^{k,H_i}\pa{q}
                            } \\ 
                &&\text{ émissions discrètes (voir table~\ref{figure_formule_baumwelch-fig})}  \nonumber \\                      
                \overline{b_q^{H_l} \pa{o} } &=&     \dfrac
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \indicatrice{O_t = o} \, \alpha_t^{k,H_i}\pa{q} \, \beta_{t}^{k,H_i}\pa{q}
                            }
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \alpha_t^{k,H_i}\pa{q} \, \beta_{t}^{k,H_i}\pa{q}
                            }  \\               
                &&\text{ émissions continues (voir table~\ref{figure_formule_baumwelch-fig_2})} \nonumber \\            
                \overline{c_{q,c} } &=&     \dfrac
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \dfrac  {\alpha_t^{k,H_l}\pa{q} \, c_{q,c} \, \beta_t^{k,H_l}\pa{q} 
                                    \, \pr{O_t^k \sac c} }
                                            { \summy{d=1}{N} \pr{O_t^k \sac d} \, c_{q,d} }
                            }
                            {
                                    \summy{k=1}{K} \, \summy{i=1}{N_k} \, \indicatrice{H_l = H_i^k}  
                                    \, \summy{t=1}{T_k} \,
                                    \alpha_t^{k,H_i}\pa{q} \, \beta_{t}^{k,H_i}\pa{q}
                            }                    
                \end{eqnarray}
                \begin{eqnarray*} \\ \hline
                \end{eqnarray*}
                \caption{   Formules de réestimation de Baum-Welch pour des modèles de lettres, 
                						les suites utilisées
                            sont celles définies en (\ref{hmm_reco_gamma_include}), (\ref{hmm_reco_alpha_q}), 
                            (\ref{hmm_reco_delta_include}), (\ref{hmm_reco_beta_q}). Elles sont valables pour 
                            un modèle de lettre noté $H_l$. L'écriture $\indicatrice{H_l = H_i}$ est la fonction
                            qui retourne $1$ lorsque le modèle $H_i$ est associé à la lettre $l$.
                            La notation $s\pa{q}$ désigne les états qui suivent l'état $q$, 
                            le coefficient $a_{q,q'}^{H_i}$ désigne la probabilité de transition de l'état
                            $q$ à l'état $q'$ du modèle $H_i$ où $q' \in s\pa{q}$, $b_{q'}^{k,H_i}\pa{O_{t+1}}$
                            est la probabilité d'émission de l'observation $O_{t+1}^k$ par l'état $q'$
                            du modèle $H_i$.
                            }
                \label{hmm_reco_estimation_lettre}
                \indexfr{Baum-Welch}
                \indexfr{réestimation}
            \end{table}

La base d'apprentissage est une liste des séquences d'observations $\vecteur{O^1}{O^K}, \; \vecteur{L^1}{L^K}$ où chaque séquence est égale à $O^k = \vecteur{O_1^k}{O_{T_k}^k}$ et $L^k = \vecteur{L_1^k}{L_{T_k}^k}$, ces séquences d'observations $O^k$ et de liaisons $L_k$ correspondent au mot $m_k = \vecteur{l^k_1}{l^k_{N_k}}$ dont le modèle associé est le modèle $M_k = \vecteur{H^k_1}{H^k_{N_k}}$. La réestimation des paramètres des modèles de lettre utilise la définition des suites (\ref{hmm_reco_gamma_include}), (\ref{hmm_reco_alpha_q}), (\ref{hmm_reco_delta_include}), (\ref{hmm_reco_beta_q}) surmontées d'un indice supplémtenaire ($k$) correspondant aux séquences d'observations $O^k$ de liaisons $L^k$ à partir desquelles elles ont été calculées.


En pratique, les dénominateurs des formules de la table~\ref{hmm_reco_estimation_lettre} ne sont pas calculés sauf pour vérifier que le calcul des numérateurs est correct. Il est préférable de renormaliser en utilisant les contraintes sur les coefficients. Dans le cas contraire, à cause de la précision de calcul des ordinateurs et le grand nombre d'opérations, les contraintes sont de moins en moins vérifiées lorsque le nombre d'itérations de l'algortihme EM s'accroît.
                
L'apprentissage des réseaux de neurones associés aux observations et aux liaisons est identique à la méthode décrite au paragraphe~\ref{reco_modele_lettres_new} à la différence que la réestimation des sorties désirées pour ces réseaux de neurones fait intervenir non plus un modèle de lettre mais un modèle de mot.






\subsection{Comparaison HMM - IOHMM}
\label{reco_comparaison_hmm_iohmm}

\indexfr{IOHMM}
\indexfr{HMM}


L'apport des modèles IOHMM par rapport aux modèles de Markov cachés a été mesuré sur une base de 2000 images de mots anglais couramment employés\footnote{Cette base est extraite de celle donnée à l'adresse \textit{http://www.cs.nott.ac.uk/$\sim$dge/words.tar.gz} sur le site ICDAR 2003 (voir \citeindex{ICDAR2003}).}. Ces 2000 mots ont été choisis aléatoirement pour un dictionnaire de 500 mots. 1000 servent à l'estimation tandis que les 1000 autres forment la base de test. Un premier modèle de reconnaissance est construit sans tenir compte des liaisons et est estimé sur cette base. Ce modèle est constitué d'un jeu de vingt-six modèles de lettres dont le nombre d'états est défini selon la règle décrite au paragraphe~\ref{reco_topologie_arbitraire}. Le réseau de neurones classant les graphèmes contient vingt-six classes de sortie et dix neurones sur la couche cachée. Les caractéristiques utilisées sont $Mat(5,5)$.

L'expérience continue en reprenant ce même modèle, chaque transition est ensuite dupliquée à l'identique autant de fois qu'il y a de classes de liaisons. Un réseau de neurones classant les liaisons est ajouté, il possède quatre sorties et quatre neurones sur la couche cachée. Le système est réappris jusqu'à convergence. Les résultats sont donnés dans la table~\ref{reco_table_comp_hmm_iohmm}.




				\begin{table}[ht]
				$$\begin{tabular}{|r|r|r|} \hline
				base   												& HMM 			& IOHMM 		\\ \hline
				apprentissage									&	43,4 \%		&	52,4 	\%	\\
				test													&	42,6 \%		&	48,2	\%	\\ \hline
				\end{tabular}$$
				\caption{	Comparaison des modèles HMM (Hidden Markov Model)
									et IOHMM (Input Output Hidden Markov Model). 
									Le modèle IOHMM est construit à partir
									du modèle HMM en ajoutant les connexions relatives aux liaisons. Les chiffres obtenus 
									correspondent au taux de reconnaissance. Le faible écart entre les bases d'apprentissage et 
									de test suggère qu'il n'y a pas eu de surapprentissage. L'écart est néanmoins plus 
									grand dans le cas du modèle IOHMM qui contient plus de coefficients.}
				\label{reco_table_comp_hmm_iohmm}
				\end{table}


L'apport des liaisons est plus important pour les modèles de Markov cachés que pour la reconnaissance effectuée à partir de plus proches voisins au paragraphe~\ref{reco_reco_knn_sequence}. En effet, lors de la recherche des plus proches voisins, la distance entre deux images est la somme de deux distances entre séquences de liaisons et entre séquences d'observations. Dans ce cas, les liaisons sont inséparables des graphèmes. En revanche, lors de l'apprentissage des IOHMM, les coefficients relatifs aux liaisons accumulent les informations indépendamment des observations. 





\begin{xremark}{bases difficiles}
Les \label{reco_bases_difficiles} systèmes de reconnaissance incluent un grand nombre de paramètres si bien que la vraisemblance définit un grand nombre de minima locaux, d'autant plus grand que les bases d'apprentissage sont difficiles -~un grand nombre d'images dont l'écriture est peu lisible~-. Pour contourner ce problème, il est préférable d'isoler une partie réduite de la base d'apprentissage pour laquelle les images sont de bonnes qualité dont la reconnaissance ne pose pas de problèmes majeurs. Après la convergence des modèles de reconnaissance effectuée à partir de cette base réduite, des exemples plus difficiles sont régulièrement ajoutés à la base d'apprentissage lors de la réestimation des coefficients jusqu'à inclure l'ensemble de la base initiale. Dans certains cas, ce procédé a permis d'obtenir des performances acceptables que ne pouvait atteindre un apprentissage utilisant constamment l'ensemble de la base et qui n'a pas permis d'éviter le piège des nombreux minima locaux introduit par le grand nombre d'images bruitées.

\end{xremark}












%---------------------------------------------------------------------------------------------------------------
\section{Modélisation de groupes de lettres}
%---------------------------------------------------------------------------------------------------------------
\indexfrr{modèle}{groupe de lettres}
\label{hmm_bi_lettre}


\subsection{Présentation}

Les modèles de mot présentés dans le paragraphe~\ref{hmm_reco_modele_lettre} supposent que la segmentation en graphèmes\indexfr{graphème} est correcte, qu'aucun graphème ne regroupe ensemble les morceaux de plus d'une lettre. Cette hypothèse est loin d'être toujours vérifiée. Les travaux présentés dans l'article \citeindex{Chen1994} sont fondés sur un prétraitement qui ne peut segmenter le couple "TT" dans 75\% des cas. Il apparaît donc que certains couples de lettres voire certains groupes de lettres sont couramment mal segmentés (figure~\ref{image_grapheme_erreur}, page~\pageref{image_grapheme_erreur}). Certains types d'erreurs sont récurrents~:

\indexfrr{graphème}{erreur}
\indexfrr{segmentation}{erreur}
\indexfr{déformation}
        
        \begin{enumerate}
        \item les accents et les poids associés à une lettre voisine de la bonne lettre
        \item les liaisons hautes introduites par les lettres b,o,v,w qui impliquent une déformation
                de la lettre suivante
        \item les lettres hautes qui altèrent localement la segmentation (barre de t, double l, ...)
        \end{enumerate}


La proportion d'erreurs est difficile à évaluer autrement que manuellement. Toutefois, la méthode proposée dans ce paragraphe permet de tenir compte des erreurs les plus fréquentes et d'en donner une estimation. Ce paragraphe concerne la poursuite des travaux décrits dans \citeindex{Dupré2002} et plus récemment \citeindex{Dupré2004}.

Prenons l'exemple du mot "attention", les groupes de lettres "tt", "ti", "on", "tion" sont parfois mal segmentés. Même si le groupe "tion" contient plus de deux lettres, il peut parfois apparaître au travers d'un seul graphème. Ce groupe a été ajouté de manière à montrer que cette méthode ne se limite pas seulement aux couples de lettres mal segmentés. La liste des modèles de lettres disponibles est formée des vingt-six lettres usuelles et des quatre groupes de lettres précédemment cités. L'extension de l'alphabet de modèles aboutit à dix manières différentes d'écrire le mot "attention" (table~\ref{hmm_reco_table_dix_attention}). Ces dix manières peuvent être résumées en un seul graphe (figure~\ref{hmm_reco_figure_modele_mot_complique_attention}) qui décrit le squelette sur lequel le modèle du mot "attention" s'appuie. Ce modèle sera noté $G\pa{m}$ (ou modèle graphe).

        
            \begin{table}[ht]
                \[
                \begin{tabular}{|l|l|}
                \hline
                a,t,t,e,n,t,i,o,n   & a,tt,e,n,t,i,o,n \\
                a,t,t,e,n,t,i,on    & a,tt,e,n,t,i,on\\
                a,t,t,e,n,ti,o,n    & a,tt,e,n,ti,o,n\\
                a,t,t,e,n,ti,on     & a,tt,e,n,ti,on\\
                a,t,t,e,n,tion      & a,tt,e,n,tion\\
                \hline
                \end{tabular}
                \]
                \caption{       Dix manières différentes d'écrire le mot "attention" en utilisant
                                les vingt-six modèles de l'alphabet et les quatre groupes de lettres
                                "tt", "ti", "on", "tion".}
                \label{hmm_reco_table_dix_attention}
            \end{table}
           

\indexfrr{modèle}{graphe}            
            

            \begin{figure}[ht]
                \[
                \fbox{
                \filefig{../reco/fig_attention}
                }
                \]
                \caption{       Modèle pour le mot "attention", ce graphe résume les dix manières
                                différentes d'écrire ce mot d'après la table~\ref{hmm_reco_table_dix_attention}.
                                Il n'existe pas de graphe plus concis que celui noté $G\pa{m}$ et
                                représenté par ce schéma.}
                \label{hmm_reco_figure_modele_mot_complique_attention}
            \end{figure}

            
Cette modélisation rejoint celle décrite dans l'article \citeindex{Wang2000}. Ce dernier s'intéresse à la modélisation de toutes les paires de chiffres qui sont presque toutes équiprobables -~la paire 00 est généralement plus probable que les autres quand il s'agit de montant~-. En ce qui concerne les caractères, les lettres ne sont déjà pas équiprobables, les couples de lettres encore moins, il est donc impossible de modéliser tous les couples sachant que certains ne peuvent de toutes façons pas apparaître (le couple "ZZ").
            
            
\subsection{Probabilité}
            
Il reste maintenant à calculer la probabilité $\pr{O,L \sac G\pa{m}}$ qu'un tel modèle émette une séquence d'observations. Soit $m$ un mot et $\intervalle{l_1}{l_N}$ le plus grand ensemble de lettres permettant d'écrire le mot $m$ de toutes les manières possibles. L'ensemble $\intervalle{H_1}{H_N}$ correspond aux modèles de Markov cachés~: la lettre $l_i$ est modélisée par le modèle $H_i$.
            
Le calcul de $\pr{O,L \sac G\pa{m}}$ nécessite l'introduction de la matrice $A$ et des vecteurs $\Pi$ et $\Theta$~:

        \begin{itemize}
        \item Le vecteur $\Pi = \pa{\pi_i}_{1 \infegal i \infegal N}$, $\pi_i$ étant la probabilité
                de commencer par la lettre $i$.
        \item La matrice $A=\pa{a_{ij}}_{\begin{subarray} \, 1 \infegal i \infegal N \\ 
        																										1 \infegal j \infegal N \end{subarray}}$,
                $a_{ij}$ étant la probabilité d'atteindre la lettre $j$ en partant de la lettre $i$.
        \item Le vecteur $\Theta = \pa{\theta_i}_{1 \infegal i \infegal N}$, $\theta_i$ étant la probabilité
                de terminer le mot $m$ par la lettre $i$.
        \end{itemize}
            
\indexfr{chaîne de Markov}

La table~\ref{hmm_reco_figure_abt_attention} donne ces trois éléments pour le mot "attention". Ces coefficients sont estimés de manière similaire à ceux d'une chaîne de Markov dont les états sont les lettres. Par exemple~:

        $$
        a_{ij} = \dfrac {\text{nombre de chemins contenant les lettres $l_i$ et $l_j$}}
                        {\text{nombre de chemins contenant la lettre $l_i$}}
        $$
        
Ces coefficients vérifient les contraintes suivantes~:

        \begin{eqnarray*}
        \sum_{i=1}^{N} \pi_i &=& 1 \text{ et }  \forall i, \sum_{j=1}^{N} a_{ij} + \theta_i = 1 
        \end{eqnarray*}
        
        
        \begin{table}[ht]
        \[
        \scriptsize
        \begin{tabular}{|l|ccccccccccccc|} \hline
        $i$                             & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 1. & 11 & 12 & 13         \\ \hline
        $l_i$ et $L_i$                  & a & t & t & e & n & t & i & o & n & tt & ti & on & tion       \\ \hline
        $\pi_i$                         & 1 & . & . & . & . & . & . & . & . & .  & .  & .  & .          \\ \hline
        $\theta_i$                      & . & . & . & . & . & . & . & . & 1 & .  & .  & 1  & 1          \\ \hline
        $a_{1i}$ (a $\rightarrow$)      & . & 0,5 & . & . & . & . & . & . & . & 0,5  & .  & .  & .      \\ \hline
        $a_{2i}$ (t $\rightarrow$)      & . & . & 1 & . & . & . & . & . & . & .  & .  & .  & .          \\ \hline
        $a_{3i}$ (t $\rightarrow$)      & . & . & . & 1 & . & . & . & . & . & .  & .  & .  & .          \\ \hline
        $a_{4i}$ (e $\rightarrow$)      & . & . & . & . & 1 & . & . & . & . & .  & .  & .  & .          \\ \hline
        $a_{5i}$ (n $\rightarrow$)      & . & . & . & . & . & 0,4 & . & . & . & .  & 0,4  & .  & 0,2    \\ \hline
        $a_{6i}$ (t $\rightarrow$)      & . & . & . & . & . & . & 1 & . & . & .  & .  & .  & .          \\ \hline
        $a_{7i}$ (i $\rightarrow$)      & . & . & . & . & . & . & . & 0,5 & . & .  & .  & 0,5  & .      \\ \hline
        $a_{8i}$ (o $\rightarrow$)      & . & . & . & . & . & . & . & . & 1 & .  & .  & .  & .          \\ \hline
        $a_{9i}$ (n $\rightarrow$)      & . & . & . & . & . & . & . & . & . & .  & .  & .  & .          \\ \hline
        $a_{10i}$ (tt $\rightarrow$)    & . & . & . & 1 & . & . & . & . & . & .  & .  & .  & .          \\ \hline
        $a_{11i}$ (ti $\rightarrow$)    & . & . & . & . & . & . & . & 0,5 & . & .  & .  & 0,5  & .      \\ \hline
        $a_{12i}$ (on $\rightarrow$)    & . & . & . & . & . & . & . & . & . & .  & .  & .  & .          \\ \hline
        $a_{13i}$ (tion $\rightarrow$)  & . & . & . & . & . & . & . & . & . & .  & .  & .  & .          \\ \hline
        \end{tabular}
        \]
        \caption{       Matrice $A$ et vecteurs $\Pi$, $\Theta$ pour le mot "attention".}
        \label{hmm_reco_figure_abt_attention}
        \end{table}


Les suites $\pa{\alpha_{t,t'}^{H_i}}_{i,t,t'}$ et $\gamma_t\pa{H_i}$ sont presque identiques à celles définies en (\ref{hmm_reco_alpha}) et (\ref{hmm_reco_gamma_include}). 

        \begin{eqnarray}
        \begin{array}{rcl}
        \forall i  \; \gamma_1 \pa{L_i} &=& \pi_{H_i} \, \alpha_{1,1}^{H_i}\\
        \forall i  \; \forall t \geqslant 2, \; \gamma_t \pa{H_i} &=& \pi_{H_i} \; \alpha_{1,t}^{H_i} + 
                                                \summy{j=1}{N} \;  \summy{k=1}{t-1} 
                                                \gamma_{t-k}\pa{H_j} a_{l_j, l_i} \,
                                                        \alpha_{t-k+1,t}^{H_j}
        \end{array}                                                        
        \end{eqnarray}

L'expression de $\pr{O \sac G\pa{m}}$ devient~:
        
        \begin{eqnarray}
        \pr {O,L \sac G\pa{m}}  &=&       \summy{i=1}{N} \theta_{H_i} \; \gamma_T \pa {H_i}
        \label{hmm_reco_eqn_couple_proba_2}
        \end{eqnarray}

Le calcul de (\ref{hmm_reco_eqn_couple_proba_2}) est plus coûteux que (\ref{hmm_reco_eqn_proba_2}). Cependant, il est possible de tenir compte des nombreux coefficients nuls des matrices $A$, $\Pi$, $\Theta$ afin de réduire le coût de l'algorithme suivant~\ref{hmm_reco_proba_modele_graph_mot}. 

        
        
		\begin{xalgorithm}{probabilité d'une séquence avec un modèle de mot}
		\label{hmm_reco_proba_modele_graph_mot}
		
		\begin{xalgostep}{initialisation 1}
		        Calcul de la suite $\pa{\alpha_{t,t'}^{H_i}}_{t,t',i}$ grâce à l'algorithme~\ref{hmm_algo_forward}.
		\end{xalgostep}
		        
		\begin{xalgostep}{initialisation 2}
		        \begin{xfor}{t}{1}{T}
			        \begin{xfor}{i}{1}{N}
			        $\gamma_t^{L_i} \longleftarrow \pi_{H_i} \, \alpha_{1,1}^{H_i} $
			        \end{xfor}
			      \end{xfor}
		\end{xalgostep}
		        
		\begin{xalgostep}{itérations}
		        \begin{xfor}{t}{1}{T}
		                \begin{xfor}{i}{1}{N}
		                        \begin{xfor}{k}{1}{t-1}
		                                \begin{xfor}{j}{1}{N}
		                                $\gamma_t^{H_i} \longleftarrow \gamma_t^{H_i} + 
		                                \gamma_k^{H_{j}} \, a_{l_j,l_i} \, \alpha_{k+1,t}^{H_i}$ 
		                                \end{xfor}
		                        \end{xfor}
		                \end{xfor}
		        \end{xfor}
		\end{xalgostep}
		
		\begin{xalgostep}{terminaison}
		        $p \longleftarrow 0$ \\
		        \begin{xfor}{i}{1}{N}
		        $p \longleftarrow \theta_{H_i} \; \gamma_T \pa {H_i}$
		        \end{xfor}
		\end{xalgostep}
		
		La probabilité cherchée est $p$.
		        
		\end{xalgorithm}
		
        
        
        
Cette modélisation se rapproche d'un modèle de Markov caché dont les états acceptent des émissions de durées variables. Les probabilités $\alpha_{t,t'}\pa{.}$ peuvent être considérées comme les émissions d'une chaîne de Markov cachée définie par les coefficients $A$, $\Pi$, $\Theta$. Ces modèles sont plus souvent utilisés pour la reconnaissance de la parole (\citeindex{Mitchell1995}). 

\indexfrr{reconnaissance}{parole}
\indexfr{parole}



\subsection{Expérimentations}
\label{reco_bi_lettre_experimentation}


La première expérience est un problème de reconnaissance simplifiée qui s'appuie sur une base d'apprentissage de 1200 mots anglais et une base de test de 800 mots. Ces mots appartiennent tous au dictionnaire composé des mots présentés dans la table~\ref{reco_bi_lettre_toy_dico} et ne faisant intervenir que les neuf lettres b, d, e, f, g, i, n, o, p. Huit modèles de reconnaissance, un par lettre, sont donc estimés à partir de ces données et permettent d'obtenir les taux de reconnaissance de la table~\ref{reco_toy_reco_bi}.


								\begin{table}[ht]
								$$\begin{tabular}{|l|l|l|l|} \hline
								BE	&	DEED	&	FIND	&	INDEPEND	\\
								BEEN	&	DEEP	&	FINE	&	NEED	\\
								BEGIN	&	DEFEND	&	FINDING	&	NEEDED	\\
								BEING	&	DEFINE	&	FOND	&	NEEDING	\\
								BIN	&	DEPEND	&	FOOD	&	NINE	\\
								BED	&	DID	&	GIG	&	NO	\\
								BEND	&	DIE	&	GOOD	&	ONE	\\
								DEED	&	DOING	&	I	&	OPEN	\\
								END	&	DONE	&	ID	&	OPENED	\\
								DEPEND	&	EGG	&	IN	&	OPENING	\\
								BOND	&	ENDED	&	INDEED	&	PEN	\\ \hline
								\end{tabular}$$
								\caption{Dictionnaire réduit n'utilisant que les neuf lettres b, d, e, f, g, i, n, o, p.}
								\label{reco_bi_lettre_toy_dico}
								\end{table}


								\begin{table}[ht]
								$$\begin{tabular}{|r|c|c|} \hline
															&		taux de reconnaissance  & taux de reconnaissance du mot BEING \\ \hline
							  apprentissage &				70,8 \%							&			70,0 \%													\\
								test					&				62,3 \%							&			57,5 \%													\\ \hline
								\end{tabular}$$
								\caption{	Les performances en reconnaissance sont mesurés sur les base d'apprentissage
													(1200 mots) et de test (800 mots). 
													Elles sont sensiblement moins élevés sur la base de test car étant donné la petite 
													taille des bases, les modèles ont tendance à "sur-apprendre". Ces bases ont été construites
													de telles sortes que le mot BEING représente un dixième de ces deux bases.}
								\label{reco_toy_reco_bi}
								\end{table}							

La même expérience est recommencée en incluant cette fois-ci dans l'alphabet le groupe de lettres ING très fréquent en langue anglaise et, pour cette raison, parfois mal écrit. Les résultats apparaissent dans la table~\ref{reco_toy_reco_ing}.


								\begin{table}[ht]
								$$\begin{tabular}{|r|c|c|} \hline
															&		taux de reconnaissance  & taux de reconnaissance du mot BEING \\ \hline
							  apprentissage &				72,8 \%							&			86,6 \%													\\
								test					&				64,9 \%							&			71,2 \%													\\ \hline
								\end{tabular}$$
								\caption{	Même taux de reconnaissance que celui de la table~\ref{reco_toy_reco_bi}
													mesuré cette fois avec un alphabet incluant 
													le groupe de lettres supplémentaire ING. }
								\label{reco_toy_reco_ing}
								\end{table}							

Il apparaît dans la première expérience que le taux de reconnaissance du mot BEING est équivalent au taux de reconnaissance sur l'ensemble de la base. En revanche, lors de la seconde expérience, un modèle à associé au groupe ING. Le taux de reconnaissance du mot BEING est alors nettement plus important que dans la première expérience et explique à lui seul l'accroissement du taux de reconnaissance global. Ces deux expériences montrent l'intérêt de la modélisation des groupes de lettres souvent mal segmentés.


L'apport de ces modèles a ensuite été testé sur une base réelle de 40000 prénoms français, 30000 ont servi à l'estimation des modèles, 10000 aux tests. La table~\ref{table_evaluation_simple_bi_lettre} compare les performances obtenus en utilisant ou non les modèles de bi-lettres. Elle  montre l'accroissement des performances pour un alphabet étendu. Le taux de reconnaissance\footnote{Le taux de reconnaissance est la proportion de documents bien reconnus.} ne croît pas de manière significative mais le taux de lecture pour~1\% de confusion\footnote{Le taux de lecture pour~1\% de confusion correspond à la proportion de documents qui peuvent être traités avec un taux d'erreur inférieur à~1\%. Ces taux sont détaillés au parapgraphe~\ref{decision_roc_par}.} a augmenté de manière sensible. La liste des modèles de lettres et de bi-lettres utilisés pour cette expérience est recensée par la table~\ref{table_occurence_bilettre}.


    \begin{table}[ht]
    $$\begin{tabular}{|c|c|c|c|c|} \hline
    		&		base & 
    				\begin{minipage}{3cm} taux de reconnaissance \end{minipage} &
    				\begin{minipage}{4cm} taux de lecture pour 1\% de confusion \smallskip \end{minipage} &
    				seuil  \\ \hline
        sans				& 		app      &   88,8 \%                 & 60,0 \%                   & 0,976 \\
        bi-lettres	&			test     &   88,7 \%                 & 61,9 \%                   & 0,980 \\ \hline 
        avec 				&			app      &   90,0 \%                 & 65,3 \%                   & 0,972 \\
        bi-lettres	&			test     &   89,0 \%                 & 65,7 \%                   & 0,968 \\ \hline
    \end{tabular}$$
    \caption{Evaluation des performances des modèles sans bi-lettres et avec bi-lettre.}
    \label{table_evaluation_simple_bi_lettre}
		\end{table}














\subsection{Segmentation la plus probable}

\indexfr{Viterbi}
\indexfrr{segmentation}{probable}
\indexfr{alignement}
\indexfrr{séquence}{état}
\indexfrr{séquence}{modèle}

L'alignement Viterbi\seeannex{paragraphe_viterbi_principe}{Viterbi} (\citeindex{Levinson1983}, \citeindex{Rabiner1986}) est un algorithme permettant d'obtenir la séquence d'états la plus probable connaissant la séquence d'observations. Il peut être adapté à la recherche de la séquence de modèles de lettre la plus  probable. Appliqué au mot "attention", l'objectif est de déterminer quelle écriture est la plus vraisemblable parmi les dix possibles (table~\ref{hmm_reco_table_dix_attention}). Cette meilleure séquence, si elle inclut des modèles de groupes de lettres, pourrait permettre de détecter ces erreurs de segmentation. Elle ne peut en revanche pas détecter des erreurs de segmentation concernant un couple de lettres non modélisé.

Par conséquent, pour des séquences d'observations et de liaisons données $O=\vecteur{O_1}{O_T}$ et $L=\vecteur{L_1}{L_T}$, on cherche une séquence de lettres (et groupes de lettres) notée $\vecteur{H^*_1}{H^*_U}$ avec $U \infegal N$. Pour chaque modèle, $H^*_i$, $\delta^*_{H_i}$ désigne le nombre d'observations qui lui sont associées.


        $$
        \forall i \in \intervalle{1}{U},  \; d^*_{H^*_i} = \left\{
                        \begin{array}{l}
                        0 \text{ si } i = 1 \\
                        1 + \summy{k=1}{i-1} \delta^*_{H^*_i} \text{ sinon}
                        \end{array}
                        \right.
        $$
        
D'où~:        
        

        \begin{eqnarray}
        \pr{\vecteurno{O_1}{O_T},\vecteurno{L_1}{L_T},
        				 \vecteurno{H^*_1}{H^*_U},\vecteurno{\delta^*_{H^*_1}}{\delta^*_{H^*_U}} 
        								\sac G\pa{m}}  = \nonumber \\
             \pi_{H^*_1} \, \alpha_{1,\delta^*_{H^*_1}}^{H^*_1} \; \cro{
                        \prody{i=2}{U} a_{H^*_{i-1},H^*_i} \, \alpha_{d^*_{H^*_i},d^*_{H^*_i}+
                        \delta^*_{H^*_i} -1}^{H^*_i}
                        } \theta_{H^*_U}
                        \label{hmm_reco_eq_proba_path}
        \end{eqnarray}

La probabilité de toute autre séquence de modèles sera inférieure à (\ref{hmm_reco_eq_proba_path}). Il reste à trouver cette meilleure séquence $\vecteur{H^*_1}{H^*_U}$. On utilise pour cela les suites $\pa{p_{t,H_i}}_{t,i}$, $\pa{m_{t,H_i}}_{t,i}$, $\pa{s_{t,H_i}}_{t,i}$~:
        
        \begin{itemize}
        \item $p_{t,H_i}$ mémorise la probabilité de la meilleure séquence de lettres
                se terminant par la lettre $H_i$ à l'instant $t$.
        \item $s_{t,H_i}$ mémorise le nombre d'observations associées au modèle $H_i$
                qui a permis d'obtenir la meilleure probabilité $p_{t,H_i}$
        \item $m_{t,H_i}$ mémorise le modèle précédent $L_i$ à l'instant $t-s_{t,H_i}$ 
                qui a permis d'obtenir la meilleure probabilité $p_{t,H_i}$
        \end{itemize}
        
L'initialisation de ces suites est donnée par les formules qui suivent~:
        
        \begin{eqnarray}
        \begin{array}{rrcl}
        \forall i \in \intervalle{1}{N}, \, \forall t \in \intervalle{1}{T}, & p_{t,H_i} 	&=& 
        								\pi_{H_i} \, \alpha_{1,t}^{H_i} \\
        \forall i \in \intervalle{1}{N}, \, \forall t \in \intervalle{1}{T}, & s_{t,H_i}	&=& t  \\
        \forall i \in \intervalle{1}{N}, \, \forall t \in \intervalle{1}{T}, & m_{l,H_i} 	&=& -1
        \end{array}
        \label{hmm_reco_viterbi_lettre_1}
        \end{eqnarray}

Le calcul se poursuit en faisant croître $t$~:			

        
        \begin{eqnarray}
        \begin{array}{rrcl}
        \forall i \in \intervalle{1}{N}, \forall t \in \intervalle{2}{T}, & 
        			\pa { s_{t,H_i}, m_{t,H_i}} &=&  \pa{j^*, t^*} \in
        							\underset{ \begin{subarray}{c} 1 \infegal j \infegal N \\ 
        																									1 \infegal k \infegal t-1   \end{subarray} } 
			  							{ \arg \max } \;
											p_{t-k,H_j} \, a_{H_j,H_i} \, \alpha_{t-k+1,t}^{H_i} \\ \\
        \forall i \in \intervalle{1}{N}, \forall t \in \intervalle{2}{T}, & 
        			p_{t,H_i} &=&  	p_{t-m_{t,H_i},L_{s_{t,H_i}}} \, 
        											a_{H_{s_{t,H_i}},H_i} \, 
        											\alpha_{t-m_{t,H_i}+1,t}^{H_i}
        \end{array}
        \label{hmm_reco_viterbi_lettre_2}
        \end{eqnarray}
        
        
Et finalement~:

        \begin{eqnarray}
        \begin{array}{rcl}
        			\pa { m_{T+1}} &=&  j^* \in
        							\underset{ 1 \infegal j \infegal N } 
			  							{ \arg \max } \;
											p_{T,H_j} \, \theta_{H_j} \\ \\
        			p_{T+1} &=&  p_{T,H_{s_{T+1}}} \, \theta_{H_{s_{T+1}}} 
        \end{array}
        \label{hmm_reco_viterbi_lettre_3}
        \end{eqnarray}
				        

La probabilité de la meilleure séquence de lettres est donnée par $p_{T+1}$, les suites $\pa{s_{t,H_i}}_{t,i}$ et $\pa{m_{t,H_i}}_{t,i}$ permettent de retrouver cette séquence puisqu'elles conservent pour chaque maximum local les élements qui ont permis de l'obtenir. Les formules (\ref{hmm_reco_viterbi_lettre_1}), (\ref{hmm_reco_viterbi_lettre_2}), (\ref{hmm_reco_viterbi_lettre_3}) aboutissent à l'algorithme suivant~:







			\begin{xalgorithm}{meilleure séquence de lettres (1)} \label{hmm_reco_algo_sequence_lettre}
			\indexfrr{meilleur(e)}{séquence de lettres (1)}
			\indexfr{Viterbi}
			
			Cet algorithme permet de calculer les suites $\pa{p_{t,H_i}}_{t,i}$, $\pa{m_{t,H_i}}_{t,i}$,
			 $\pa{s_{t,H_i}}_{t,i}$.
			
			\begin{xalgostep}{initialisation 1}
			        Calcul de la suite $\pa{\alpha_{t,t'}^{H_i}}_{t,t',i}$ grâce à l'algorithme~\ref{hmm_algo_forward}.
			\end{xalgostep}
			        
			\begin{xalgostep}{initialisation 2}
			        \begin{xfor}{i}{1}{N}
								\begin{xfor}{t}{1}{T}
									$
									\begin{array}{lll}
				        	p_{t,H_i} &\longleftarrow& \pi_{H_i} \, \alpha_{1,t}^{H_i} \\
				        	m_{t,H_i} &\longleftarrow& i   \\
				        	s_{t,H_i} &\longleftarrow& -1  
				        	\end{array}
				        	$
				        \end{xfor}
			        \end{xfor}
			\end{xalgostep}
			
			\begin{xalgostep}{récurrence}
			        \begin{xfor}{i}{1}{N}
								\begin{xfor}{t}{2}{T}
								$
								\begin{array}{lll}
			        	p_{t,H_i} &\longleftarrow& 0   \\
			        	m_{t,H_i} &\longleftarrow& -1  \\
			        	s_{t,H_i} &\longleftarrow& -1  
			        	\end{array}
			        	$\\
				        \begin{xfor}{j}{1}{N}
									\begin{xfor}{k}{1}{t-1}
										$x \longleftarrow p_{t-k,H_j} \, a_{H_j,H_i} \, \alpha_{t-k+1,t}^{H_i}$ \\
										\begin{xif}{$x > p_{t,H_i}$}
											$
											\begin{array}{lll}
						        	p_{t,H_i} &\longleftarrow& x  \\
						        	m_{t,H_i} &\longleftarrow& j  \\
						        	s_{t,H_i} &\longleftarrow& k  
						        	\end{array}
						        	$
										\end{xif} 
					        \end{xfor}
				        \end{xfor}
				        \end{xfor}
			        \end{xfor}
			\end{xalgostep}
			
			\possiblecut

			\begin{xalgostep}{terminaison}
									$
									\begin{array}{lll}
				        	p_{T+1} &\longleftarrow& 0 	\\
				        	m_{T+1} &\longleftarrow& -1	
				        	\end{array}
				        	$ \\
					        \begin{xfor}{j}{1}{N}
										\begin{xfor}{k}{1}{t-1}
											$x \longleftarrow p_{T+1,H_j} \, \theta_{H_j} $ \\
											\begin{xif}{$x > p_{t,H_i}$}
												$
												\begin{array}{lll}
							        	p_{t,H_i} &\longleftarrow& x  \\
							        	m_{t,H_i} &\longleftarrow& j  
							        	\end{array}
							        	$
											\end{xif} 
						        \end{xfor}
					        \end{xfor}
			\end{xalgostep}
			
			\end{xalgorithm}


La meilleure séquence est finalement obtenue par l'algorithme qui suit.


			\begin{xalgorithm}{meilleure séquence de lettres (2)} \label{hmm_reco_algo_sequence_lettre_2}
			\indexfrr{meilleur(e)}{séquence de lettres (2)}
			\indexfr{Viterbi}
			
			A partir des suites $\pa{p_{t,H_i}}_{t,i}$,, $\pa{m_{t,H_i}}_{t,i}$, $\pa{s_{t,H_i}}_{t,i}$ calculées à partir de
			l'algortihme~\ref{hmm_reco_algo_sequence_lettre}, 
			cet algorithme permet d'obtenir la meilleure séquence de lettres 
			ainsi que les observations parmi la séquence $\vecteur{O_1}{O_T}$ qui leur sont associées.
			
			La séquence de modèles est notée $\vecteur{H^*_1}{H^*_U}$ et la séquence des nombres d'observations 
			associéss à chaque modèle est notée $\vecteur{\delta^*_1}{\delta^*_U}$
			
			\begin{xalgostep}{initialisation}
									$
									\begin{array}{lll}
									U 					&\longleftarrow& 1 							\\
									H^*_U 			&\longleftarrow& m_{T+1} 				\\
									\delta^*_U	&\longleftarrow& s_{T,H^*_U}		\\
									t						&\longleftarrow& T  						\\
									U						&\longleftarrow& U+1 
									\end{array}
									$
			\end{xalgostep}
			
			
			\begin{xalgostep}{récurrence}
									\begin{xwhile}{$H^*_{U-1} \neq -1$}
										$
										\begin{array}{lll}
										H^*_U 			&\longleftarrow& m_{t,H^*_{U-1}}		\\
										\delta^*_U	&\longleftarrow& s_{t,H^*_{U-1}}		\\
										t						&\longleftarrow& t - \delta^*_{U-1} \\
										U						&\longleftarrow& U+1 
										\end{array}
										$
									\end{xwhile} \\
									$U						\longleftarrow U-2$ 
			\end{xalgostep}
			
			La séquence obtenue est retournée.
			
			\begin{xalgostep}{terminaison}
									$i \longleftarrow 1$ et $j \longleftarrow U$ \\
									\begin{xwhile}{$i < j$}
										$
										\begin{array}{lll}
										H^*_i  			&\longleftrightarrow& H^*_j					\\
										\delta^*_i	&\longleftrightarrow&	\delta^*_j		\\
										i						&\longleftrightarrow& i + 1 \\
										j						&\longleftrightarrow& j - 1 
										\end{array}
										$
									\end{xwhile}
			\end{xalgostep}
			
			
			\end{xalgorithm}
			
			
			
			
			
			
			
			



		\begin{xtheorem}{meilleure séquence de lettres}\label{hmm_reco_th_sequence_lettre}
		\indexfrr{meilleur(e)}{séquence de lettres}
		\indexfr{Viterbi}
		Pour deux séquences d'observations et de liaisons $O=\vecteur{O_1}{O_T}$ et $L=\vecteur{L_1}{L_T}$, 
		les séquences de modèles et durées 
		$\vecteur{H^*_1}{H^*_U}$ et $\vecteur{\delta^*_1}{\delta^*_U}$ obtenues par les
		algorithmes~\ref{hmm_reco_algo_sequence_lettre} et~\ref{hmm_reco_algo_sequence_lettre_2} sont 
		celles qui maximisent (\ref{hmm_reco_eq_proba_path})~:
				$$
				\pr{\vecteurno{O_1}{O_T},\vecteurno{L_1}{L_T}
						\vecteurno{H^*_1}{H^*_U},\vecteurno{\delta^*_{H^*_1}}{\delta^*_{H^*_U}} \sac G\pa{m}}
				$$
			
		\end{xtheorem}












\begin{xdemo}{théorème}{\ref{hmm_reco_th_sequence_lettre}}

La démonstration est analogue à celle de l'algorithme~\ref{hmm_algo_viterbi_etat}. Pour résumer, celle-ci s'effectue par récurrence sur $t$, afin de montrer que pour tout $\pa{t,i}$, $p_{t,H_i}$ est la probabilité correspondant à la séquence de modèles la plus probable parmi toutes celles se terminant à l'instant $t$ par le modèle $H_i$. C'est de manière évidente vrai pour $t =1$ et ce quel que soit $i$, il suffit de le montrer pour $t+1$.

\end{xdemo}




La table~\ref{table_occurence_bilettre} illustre les résultats sur l'utilisation des modèles de bi-lettres. Toutefois, une estimation précise des erreurs de segmentation avec les modèles de bi-lettres est difficilement déductible des résultats présentés dans ce tableau. Les modèles de bi-lettres apprennent non seulement les cas mal segmentés mais aussi les cas bien segmentés. Pour contrecarrer ce penchant, il faudrait peut-être empêcher les modèles de bi-lettres de pouvoir émettre les mêmes classes d'observations que celles qu'émettent les modèles associées aux lettres qui le composent. Par exemple, soit $i$ un état quelconque du modèle "LL", le coefficient $c_{i,c}^{LL}$ associé à l'émission d'une observations de la classe $c$ pourrait être diminué lors de sa mise à jour si $\underset{j}{ \max} \;  {c_{j,c}^L}$ est non négligeable. Cette pénalisation \indexfr{pénalisation} n'est néanmoins pas aussi simple à mettre en \oe uvre puisque l'estimation des coefficients des modèles IOHMM est une optimisation sous contraintes.





		\begin{table}[tp]
    $$
    \fbox{$
        \begin{array}{lrrlrr}
        A   & 24328     & 83\%      & M     & 9630      & 98\% \\
        AI  & 872       & 44\%      & N     & 20077     & 70\% \\
        AN  & 7194      & 47\%      & O     & 8490      & 58\% \\
        B   & 2203      & 62\%      & OI    & 888       & 44\% \\
        BE  & 1556      & 44\%      & OM    & 252       & 64\% \\
        BI  & 51        & 61\%      & ON    & 1862      & 46\% \\
        BR  & 304       & 39\%      & OR    & 804       & 42\% \\
        BU  & 1         & 100\%     & OS    & 1157      & 56\% \\
        C   & 8355      & 67\%      & OT    & 65        & 72\% \\
        CE  & 1837      & 86\%      & OU    & 1487      & 72\% \\
        CH  & 1888      & 60\%      & P     & 3740      & 100\% \\
        D   & 5913      & 74\%      & Q     & 764       & 100\% \\
        DE  & 1975      & 78\%      & R     & 19782     & 50\% \\
        E   & 46530     & 59\%      & RE    & 3622      & 61\% \\
        ER  & 4545      & 64\%      & RI    & 6908      & 62\% \\
        ES  & 2322      & 58\%      & S     & 9555      & 42\% \\
        F   & 1928      & 100\%     & SA    & 391       & 68\% \\
        G   & 4384      & 100\%     & SE    & 3016      & 35\% \\
        H   & 6204      & 82\%      & SO    & 163       & 42\% \\
        I   & 22890     & 42\%      & T     & 8226      & 82\% \\
        IE  & 7603      & 34\%      & TE    & 2036      & 37\% \\
        IN  & 3708      & 49\%      & TT    & 1224      & 29\% \\
        IS  & 3656      & 58\%      & U     & 8812      & 82\% \\
        J   & 5083      & 60\%      & V     & 1775      & 63\% \\
        JE  & 2612      & 60\%      & VE    & 679       & 47\% \\
        JU  & 952       & 51\%      & VI    & 736       & 47\% \\
        K   & 247       & 100\%     & VU    & 1         & 100\% \\
        L   & 15702     & 69\%      & W     & 99        & 100\% \\
        LE  & 4484      & 52\%      & X     & 235       & 100\% \\
        LI  & 2633      & 53\%      & Y     & 1689      & 100\% \\
        LL  & 821       & 70\%      & Z     & 356       & 100\%
        \end{array}
    $}
    $$
    \caption{	Occurrences des lettres et couples de lettres de la base d'apprentissage, la troisième colonne est obtenue
    					grâce à un alignement Viterbi
             	qui détermine quelle écriture est la plus probable. Par exemple lorsque la séquence AI intervient 
             	dans un mot, dans 44 \% des cas, le modèle associé au couple AI est plus probable que le couple de
             	modèles associés aux lettres A et I. En revanche, dans le cas de
             	la lettre A, dans 83 \% des cas, elle est mieux modélisée par son modèle associé plutôt 
             	qu'incluse dans un modèle des couples AI, AN, SA.}
    \label{table_occurence_bilettre}
		\end{table}



L'algorithme permettant de trouver la meilleure séquence de modèles de lettres ne donne pas de résultats réellement satisfaisants lorsqu'elle est appliquée à la détection des erreurs de segmentation. Une meilleure détection nécessiterait de plus amples développements. Toutefois, le paragraphe~\ref{reco_sans_dico_par} montrera que l'algorithme de Viterbi permet d'obtenir des résultats intéressants.


\begin{xremark}{segmentations les plus probables}
Les algorithmes~\ref{hmm_reco_algo_sequence_lettre} et~\ref{hmm_reco_algo_sequence_lettre_2} permettent d'obtenir la meilleure séquence de modèles mais il possible d'obtenir la liste des séquences les plus probables en conservant à chaque itération $t$ et pour chaque modèle $i$ des informations non plus sur le meilleur chemin mais sur les $n$ meilleurs chemins. L'article \citeindex{Chen1994} présente une version optimisée permettant d'obtenir les $n$ meilleures séquences.
\end{xremark}



\subsection{Pour aller plus loin, génération de caractères manuscrits}
\label{reco_generation_word_sequence}

\indexfrr{génération}{caractères}

\indexfrr{méthode}{globale}
\indexfrr{méthode}{syllabique}


L'idée s'inspire de l'article \citeindex{ChoiH2003} qui propose la génération aléatoire de caractères manuscrits à partir de modèles écrits. Les modèles de Markov cachés permettent de générer aléatoirement des séquences probables d'états puis des séquences de vecteurs de probabilité de modèles et enfin des séquence de caractéristiques.

Pour peu que les séquences de caractéristiques permettent de fabriquer une image représentative du caractère -~c'est le cas des caractéristiques décrites aux paragraphes~\ref{reco_point_caracteristique_kohonen}, \ref{reco_feature_moment_zernike}, \ref{reco_feature_moment_ondelette_par}, \ref{reco_profil_polair}~-, il est possible pour un mot donné de générer quelques écritures les plus probables. Cela permet d'obtenir une représentation visuelle de ce que les modèles de reconnaissance ont appris. Cette méthode pourrait être utilisée pour mieux visualiser les différentes formes de lettres apprises par un modèle de bi-lettres.













\subsection{Apprentissage}
\indexfr{apprentissage}

\indexfr{graphe}

L'apprentissage des modèles de lettres et groupes de lettres utilisent les mêmes formules que celles de la table~\ref{hmm_reco_estimation_lettre} mais avec des suites $\pa{\alpha_t^{k,H_i}\pa{q}}$, $\pa{\beta_t^{k,H_i}\pa{q}}$, $\pa{\gamma_t^{k,H_i}}$, $\pa{\delta_t^{k,H_i}}$ différentes mais facilement déductibles de celles définies par les équations (\ref{hmm_reco_alpha_include}) à (\ref{hmm_reco_beta_include_end}). L'annexe~\ref{annexe_hmm_seq} propose un formalisme fondé sur les graphes permettant d'exprimer avec plus de clarté aussi bien le calcul des probabilités que la mise à jour des paramètres des modèles de Markov caché. 












%---------------------------------------------------------------------------------------------------------------
\section{Reconnaissance sans dictionnaire}
%---------------------------------------------------------------------------------------------------------------
\indexfrr{reconnaissance}{sans dictionnaire}
\indexfr{dictionnaire}
\indexfr{Viterbi}
\label{reco_sans_dico_par}

La reconnaissance sans dictionnaire utilise le même formalisme que celui développé au paragraphe~\ref{hmm_bi_lettre}. La reconnaissance sans dictionnaire de deux séquences d'observations et de liaisons $O=\vecteur{O_1}{O_T}$ et $L=\vecteur{L_1}{L_T}$ est la recherche de la meilleure séquence de lettres dans un modèle particulier, soit l'application des algorithmes~\ref{hmm_reco_algo_sequence_lettre} et~\ref{hmm_reco_algo_sequence_lettre_2} au modèle $G\pa{m}$ suivant~:

		\begin{itemize}
		\item La liste $\vecteur{H_1}{H_N}$ des modèles de lettres et groupes de lettres utilisés est l'alphabet 
					entier (étendu ou non).
		\item Les vecteurs $\Pi$, $\Theta$ et la matrice $A$ sont uniformes et tous leurs coefficients sont égaux à 1.
		\end{itemize}
		
        \begin{figure}[t]
        $$\frame{$\begin{array}[c]{c}\includegraphics[height=7cm, width=9cm]
        {\filext{../hmm_reco/image/viterbi}}\end{array}$}$$
        \caption{       Recherche de la séquence de lettres la plus probable~: chaque observation est 
                        associée à un état d'un modèle de lettre. La séquence d'états obtenue mène à une séquence
                        de modèles de lettres formant un mot très probable.
                }
        \label{hmm_figure_exemple_modele_lettre}
        \end{figure}


\indexfr{bi-grammes}
\indexfr{n-grammes}

Ce principe est illustré pour un alphabet de trois lettres par la figure~\ref{hmm_figure_exemple_modele_lettre}. 
Les performances d'un tel système peuvent être améliorées en estimant les coefficients $\Pi$, $\Theta$, $A$ à l'aide de bi-grammes\seeannex{annexe_ngrams}{n-grammes}. Inclure des n-grammes avec $n>2$ est plus délicat et nécessite une version différente des algorithmes~\ref{hmm_reco_algo_sequence_lettre} et~\ref{hmm_reco_algo_sequence_lettre_2} et plus coûteuse aussi. Par exemple, toujours sur la même base de 40000 prénoms, cet algorithme permet de trouver le bon prénom dans seulement 5~\% des cas. L'utilisation de bi-grammes permet d'augmenter ce pourcentage à 33~\%. Cette même expérience a montré que lorsque les reconnaissances avec dictionnaire et sans dictionnaire concordent, le taux d'erreur est nul. Cette remarque sera utilisée au paragraphe~\ref{decision_nn_un_modele} (page~\pageref{decision_nn_un_modele}).


















%---------------------------------------------------------------------------------------------------------------
\section{Sélection d'architecture}
%---------------------------------------------------------------------------------------------------------------
\label{reco_selection_architecture}

\indexfrr{architecture}{sélection}
\indexfrr{sélection}{architecture}

Les algorithmes proposés jusqu'à présent permettent d'estimer les paramètres des modèles de Markov cachés une fois que sa structure a été fixée. Chaque modèle de lettre est initialisé de manière intuitive, le nombre d'états est fonction de la complexité de la forme à représenter et de sa variabilité. Ce problème de sélection de modèles intervient dans de nombreux domaines et s'appuie sur un compromis entre taille de modèle et vraisemblance des observations. Deux critères couramment utilisés sont~: \textit{Akaike Information Criterieum} et \textit{Bayesian Information Criterieum} (AIC et BIC) (voir~\citeindex{Akaike1974}, \citeindex{Schwartz1978}, \citeindex{Saporta1990}). \indexfr{AIC} \indexfr{BIC} \indexfr{Akaike Information Criterieum} \indexfr{Baysian Information Criterieum} \indexfr{série temporelle} Par exemple, le modèle sélectionné AR\footnote{Auto-Regressive} pour modéliser une série temporelle $\pa{X_1, ..., X_N}$ est celui qui minimise un critère, ici BIC~:


		$$
		BIC\pa{d} = \underset{ \text{erreur moyenne de prédiction} } 
														{ \underbrace{ 
																		\frac{1}{N-d} \; 
																		\summy{t=d+1}{N} \; 
																		\pa{X_t - \summy{i=1}{d} \alpha_i \, X_{t-i}}^2 }} 
											+
								\underset{ \text{pénalisation} } { \underbrace{ d \ln \pa{N-d}}}
		$$

\indexfr{généralisation}

L'erreur de prédiction décroît uniformément lorsque $d$ augmente mais la faculté de généralisation du modèle diminue~: si la taille du modèle est trop importante, son erreur de prédiction est susceptible d'augmenter sur de nouvelles données.


Cette méthode ne peut malencontreusement pas s'adapter telle quelle aux modèles de Markov cachés. Les coefficients ne jouent pas tous le même rôle, certains sont des probabilités d'émissions, d'autres de transitions mais surtout, ces coefficients sont liés entre eux par des contraintes. Si les transitions sont utilisées à chaque séquence d'observations, en revanche, selon cette séquence, les probabilités d'émissions ne sont pas toutes utiles. De plus, un modèle de mot est un assemblage de modèles de lettre, comment déterminer les modèles de lettre qui contiennent trop ou pas assez de coefficients. Il existe également des modèles dont les nombres de coefficients sont différents et qui pourtant sont équivalents en terme de probabilité (voir \citeindex{Balasubramanian1993}, \citeindex{Kamp1985}). \indexfr{équivalence} L'article~\citeindex{Ziv1992} propose une méthode permettant de déterminer une chaîne de Markov non cachée optimale mais il faut pour cela estimer les modèles pour toutes les tailles. L'article \citeindex{Bicego2003} est celui qui s'approche le plus de la tâche à résoudre. Il s'applique sur des modèles de Markov cachés et dans le cadre de la reconnaissance de l'écriture. Il étend la méthode utilisant le critère BIC et enfin, propose un moyen d'optimiser les nombreuses réestimations de coefficients pour des tailles différentes de modèles.

On suppose que la base d'apprentissage est composée des $K$ séquences d'observations $\vecteur{O^1}{O^K}$, $\pr{\vecteurno{O^1}{O^K}}$ est donc la vraisemblance des modèles de reconnaissance, $N_k$ est le nombre de paramètres libres pour le modèle d'indice $k$ et $n$ représente la somme des longueurs des séquences d'observations. Le meilleur modèle maximise le critère suivant~:


			\begin{eqnarray}
			BIC\pa{k} &=&	\ln \pr{\vecteurno{O^1}{O^K}} - \frac{N_k}{2} \ln n
			\label{reco_selec_archi_bic}
			\end{eqnarray}
			
\label{modification_janvier_2004_li2004}			
			
Les travaux de \citeindex{Durand2003} montrent que le critère BIC est un des plus pertinents lorsqu'il s'agit de déterminer le nombre d'états de modèles de Markov cachés. Plutôt que d'essayer tous les nombres d'états possibles, l'article \citeindex{Bicego2003} propose de partir d'un nombre d'états surestimant le nombre d'états réels puis de faire décroître ce nombre d'états en supprimant à chaque étape l'état le moins probable\seeannex{hmm_ditribution_temporelle_etat}{état le moins probable}. L'article montre que cette méthode est bien meilleure que de repartir d'une initialisation aléatoire. La direction de recherche choisie consiste à faire évoluer petit à petit l'architecture des modèles de reconnaissance selon les méthodes développées dans l'annexe~\ref{annexe_hmm_select}. La taille des modèles va osciller en alternant des méthodes de suppression\seeannex{hmm_selec_decroissance_par}{décroissance de l'architecture} des coefficients et des méthodes de multiplication\seeannex{hmm_selec_croissance_par}{croissance de l'architecture} de ces mêmes coefficients. Cette succession d'étapes permet d'obtenir une série de modèles dont le meilleur est déterminé par le critère BIC (\ref{reco_selec_archi_bic}). L'article \citeindex{Li2004} propose quant à lui un critère basé sur l'entropie des coefficients et fait également décroître le nombre d'états jusqu'à obtenir un nombre optimal. Sa méthode n'est toutefois applicable que dans le cas d'émissions obéissant à une loi gaussienne\seeannex{hmm_loi_normale_emission_section}{émissions gaussiennes}.


\indexfrr{coefficients}{suppression}
\indexfrr{coefficients}{multiplication}
\indexfr{vraisemblance}

En ce qui concerne la reconnaissance de l'écriture manuscrite, la vraisemblance utilisée pour calculer le critère BIC (\ref{reco_selec_archi_bic}) utilise autant de modèles de Markov caché qu'il y a de lettres différentes, contrairement à l'article \citeindex{Bicego2003} ou à la thèse \citeindex{Durand2003} qui n'utilise qu'un seul modèle. De plus, ces modèles ne sont pas estimés avec le même nombre d'exemples puisque le modèle de la lettre $W$ française ne peut être estimé qu'avec une centaine de séquences alors que le modèle de la lettre $E$ bénéficie de plusieurs milliers de séquences. Le critère (\ref{reco_selec_archi_bic}) intègre donc des coefficients qui ne sont pas tous calculés avec le même nombre d'observations. Les estimations des modèles des lettres moins fréquentes K,Q,W,V,X,Y,Z (voir table~\ref{table_occurence_bilettre}) sont moins fiables et il est difficile de supprimer ou de multiplier un coefficient à choisir dans des modèles de lettres différents à moins de comparer.

Les expériences réalisées dans le cadre de ces travaux ont d'abord montré que les étapes de suppression de coefficients sont moins coûteuses et plus fiables que les étapes de multiplication des coefficients. Il est par conséquent préférable d'estimer un modèle au nombre de coefficients supérieur au nombre optimal. D'autre part, les modèles associés à des lettres fréquentes ont tendance à perdre des coefficients~: ils savent idientifier les écritures les plus courantes et rejettent de nombreuses formes aberrantes. Les modèles associés à des lettres moins fréquentes ont tendance à gagner des coefficients~: il ne se dégage pas des exemples qu'ils ont à apprendre une écriture courante, en contrepartie, le modèle apprend de nombreuses écritures dont certaines assez bruitées. Lorsque l'écriture devient mal écrite, ces modèles appris avec peu d'exemples et possédant plus de coefficients que les autres deviennent plus probables que ceux appris avec un grand nombre d'exemples (voir figures~\ref{reco_figure_seleca} et~\ref{reco_figure_selecb}). 


        \begin{figure}[t]
        $$\frame{$\begin{array}[c]{c}\includegraphics[height=4cm, width=10cm]
        {\filext{../reco/image/seleca}}\end{array}$}$$
        \caption{ Cette figure illustre de manière schématique les résultats de la reconnaissance 
        					pour deux jeux de modèles.
        					Le premier jeu contient des modèles de lettres dont les coefficients 
        					n'ont pas été sélectionnés. Chaque modèle
        					de lettre, appris avec beaucoup d'exemples ou non, inclut de nombreuses 
        					formes d'écriture symbolisées par 
        					le caractère $\epsilon$. Dans ce cas, la reconnaissance, en comparant les modèles 
        					"ACT" et "AZT" préfère "ACT" dans les deux cas. Pour la seconde expérience, les lettres 
        					"ACT" sont très fréquentes dans la langue française, les modèles ont donc été 
        					débarrassés des écritures bruitées alors que le modèle "Z", ayant été 
        					estimé avec peu d'exemples, inclut plus d'écritures bruitées. Dans le cas d'une écriture bruitée,
        					la reconnaissance utilisant des modèles sélectionnés choisira le modèle "AZT". Ce schéma est 
        					simplifié par rapport à ce qu'on observe dans les expériences mais il donne une bonne 
        					intuition des écueils auxquels aboutissent les méthodes de sélection d'architecture.
                }
        \label{reco_figure_seleca}
        \end{figure}


        \begin{figure}[t]
        $$\begin{tabular}{|c|c|} \hline
        \includegraphics[height=1cm, width=4cm]{\filext{../reco/image2/_e}} &
        \includegraphics[height=4.5cm, width=8cm]{\filext{../reco/image2/_k}} \\
        e & k \\ \hline
        \end{tabular}$$
        \caption{ Ces deux modèles de Markov cachés sont une représentation simplifiée de ceux obtenus 
        					pour les lettres~"E" et~"K". Ils illustrent l'idée décrite par la figure~\ref{reco_figure_seleca}.
        					Chaque classe de graphèmes est représentée par certains de ses éléments les plus probables.
        					Le modèle~"e" ne contient plus qu'une seule écriture prépondérante tandis que le modèle~"k" 
        					contient plusieurs écritures équiprobables dont certaines intègrent des classes de graphèmes
        					fort éloignées du dessin de la lettre~"k". Les autres lettres sont regroupées dans 
        					l'annexe~\ref{annexe_reco_illustration}.
                }
        \label{reco_figure_selecb}
        \end{figure}

\indexfr{Viterbi}

La recherche d'une méthode de sélection fiable s'articule autour de deux thèmes principaux qui sont une meilleure détermination des coefficients à supprimer et l'amélioration des bases d'apprentissage. Cette seconde option correspond à une préparation des bases d'apprentissage des modèles de façon à gommer les disparités entre lettres comme le suggère le paragraphe~\ref{reco_base_app}. Toutefois, la méthode utilisée pour des caractères doit être adaptée aux mots en tenant compte du fait que dupliquer l'image d'un mot contenant une lettre rare entraîne nécessairement la duplication de lettres fréquentes. La création de bases d'apprentissage plus fiables nécessite a priori de plus amples recherches. La première direction suscite quant à elle toujours l'intérêt des chercheurs comme en témoigne le récent article \citeindex{Li2004} traitant du problème de la sélection du nombre d'états d'un modèle de Markov caché dont les émissions sont gaussiennes\seeannex{hmm_loi_normale_emission_section}{émissions gaussiennes}. Comme dans l'article \citeindex{Bicego2003}, le nombre d'états décroît progressivement tout au long de l'apprentissage en choisissant de regrouper ensemble des paires d'états satisfaisant une condition dépendant du nombre de fois où un état est inclus dans la meilleure séquence sélectionnée par l'algorithme de Viterbi.


Une sélection pertinente de l'architecture d'un modèle reste pour le moment encore un problème non résolu de manière satisfaisante. Elle aurait pourtant des répercussions sur un possible apprentissage d'une segmentation graphème (voir paragraphes~\ref{image_prolongement_segmentation_grapheme}, \ref{hmm_bi_lettre}) ou sur la pertinence de la reconnaissance sans dictionnaire (voir paragraphe~\ref{reco_sans_dico_par}), ce problème étant relié à une meilleure prise de décision (voir paragraphe~\ref{decision_mot_bruit}). 









\indexfrr{directions de recherche}{sélection d'architecture}










%---------------------------------------------------------------------------------------------------------------
\section{Conclusion}
%---------------------------------------------------------------------------------------------------------------
\label{reco_conclusion}


Ce chapitre présente la partie de la reconnaissance de l'écriture qui s'étend de la description sous forme de caractéristiques de chaque graphème à l'obtention de la probabilité que l'image contienne tel ou tel mot. Le paragraphe~\ref{reco_selection_caracteristique} a montré qu'un même algorithme peut obtenir des performances médiocres ou bonnes selon le choix de la description des images. Cela tend à montrer que le choix des caractéristiques est de même importance que le choix de la modélisation. 

Cette partie développe une méthode de sélection des caractéristiques associées aux graphèmes ainsi qu'une étude de l'association positive d'une séquence de graphèmes et d'une séquence de liaisons. Outre le fait que ce chapitre détaille les mécanismes de la reconnaissance ainsi que l'apport des liaisons dans la reconnaissance, la modélisation markovienne de couples de lettres fréquemment mal segmentées est la contribution majeure présentée dans cette partie.

L'intérêt porté aux méthodes d'apprentissage et aux méthodes de sélection de modèles vient du fait qu'il est parfois nécessaire de comprendre comment le résultat de la reconnaissance a été obtenu. Les réseaux de neurones sont reconnus pour être des boîtes noires qu'il est impossible de déchiffrer. Dans le cas de la reconnaissance, il nous suffit de pouvoir retrouver l'association entre les graphèmes et les modèles de lettres, mais les piètres performances en reconnaissance sans dictionnaire montrent que ce décryptage n'est pas encore tout-à-fait possible bien que les modèles de groupes de lettres soient un pas dans cette direction.









\newpage


\firstpassagedo{
	\begin{thebibliography}{99}
	\input{reco_article.tex}
	\end{thebibliography}
}

\input{../../common/livre_table_end.tex}%
\input{../../common/livre_end.tex}%
